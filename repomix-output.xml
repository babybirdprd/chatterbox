This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/workflows/install_check.yml
.gitignore
candle/Cargo.toml
candle/examples/generate.rs
candle/examples/list_keys.rs
candle/examples/split_generate.rs
candle/examples/test_vocoder.rs
candle/examples/verify_shapes.rs
candle/examples/verify_weights.rs
candle/output.txt
candle/src/audio.rs
candle/src/campplus.rs
candle/src/chatterbox.rs
candle/src/gpt2.rs
candle/src/hifigan.rs
candle/src/lib.rs
candle/src/main.rs
candle/src/modules.rs
candle/src/s3gen.rs
candle/src/s3tokenizer.rs
candle/src/sampling.rs
candle/src/t3_model.rs
candle/src/voice_encoder.rs
demo.rs
example_for_mac.py
example_tts_turbo.py
example_tts.py
example_vc.py
gradio_tts_app.py
gradio_tts_turbo_app.py
gradio_vc_app.py
LICENSE
multilingual_app.py
pyproject.toml
README.md
s3tokenizer-v2-model/.gitattributes
s3tokenizer-v2-model/config.json
s3tokenizer-v2-model/README.md
src/chatterbox/__init__.py
src/chatterbox/models/s3gen/__init__.py
src/chatterbox/models/s3gen/configs.py
src/chatterbox/models/s3gen/const.py
src/chatterbox/models/s3gen/decoder.py
src/chatterbox/models/s3gen/f0_predictor.py
src/chatterbox/models/s3gen/flow_matching.py
src/chatterbox/models/s3gen/flow.py
src/chatterbox/models/s3gen/hifigan.py
src/chatterbox/models/s3gen/matcha/decoder.py
src/chatterbox/models/s3gen/matcha/flow_matching.py
src/chatterbox/models/s3gen/matcha/text_encoder.py
src/chatterbox/models/s3gen/matcha/transformer.py
src/chatterbox/models/s3gen/s3gen.py
src/chatterbox/models/s3gen/transformer/activation.py
src/chatterbox/models/s3gen/transformer/attention.py
src/chatterbox/models/s3gen/transformer/convolution.py
src/chatterbox/models/s3gen/transformer/embedding.py
src/chatterbox/models/s3gen/transformer/encoder_layer.py
src/chatterbox/models/s3gen/transformer/positionwise_feed_forward.py
src/chatterbox/models/s3gen/transformer/subsampling.py
src/chatterbox/models/s3gen/transformer/upsample_encoder.py
src/chatterbox/models/s3gen/utils/class_utils.py
src/chatterbox/models/s3gen/utils/intmeanflow.py
src/chatterbox/models/s3gen/utils/mask.py
src/chatterbox/models/s3gen/utils/mel.py
src/chatterbox/models/s3gen/xvector.py
src/chatterbox/models/s3tokenizer/__init__.py
src/chatterbox/models/s3tokenizer/s3tokenizer.py
src/chatterbox/models/t3/__init__.py
src/chatterbox/models/t3/inference/alignment_stream_analyzer.py
src/chatterbox/models/t3/inference/t3_hf_backend.py
src/chatterbox/models/t3/llama_configs.py
src/chatterbox/models/t3/modules/cond_enc.py
src/chatterbox/models/t3/modules/learned_pos_emb.py
src/chatterbox/models/t3/modules/perceiver.py
src/chatterbox/models/t3/modules/t3_config.py
src/chatterbox/models/t3/t3.py
src/chatterbox/models/tokenizers/__init__.py
src/chatterbox/models/tokenizers/tokenizer.py
src/chatterbox/models/utils.py
src/chatterbox/models/voice_encoder/__init__.py
src/chatterbox/models/voice_encoder/config.py
src/chatterbox/models/voice_encoder/melspec.py
src/chatterbox/models/voice_encoder/voice_encoder.py
src/chatterbox/mtl_tts.py
src/chatterbox/tts_turbo.py
src/chatterbox/tts.py
src/chatterbox/vc.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/install_check.yml">
name: Test Installation

on:
  push:
    branches: [ "master" ]
  pull_request:
    branches: [ "master" ]

jobs:
  build:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Test Standard Install
      run: |
        pip install -e .
</file>

<file path="candle/examples/generate.rs">
//! Example: Generate speech from text using Chatterbox TTS
//!
//! Usage:
//! ```bash
//! cargo run --example generate -- --text "Hello world" --ref-audio reference.wav
//! ```

use candle::{audio, chatterbox::ChatterboxTurboTTS, GenerateConfig};
use candle_core::Device;
use std::path::Path;

fn main() -> anyhow::Result<()> {
    // Parse command line args (simplified)
    let args: Vec<String> = std::env::args().collect();

    let text = args
        .iter()
        .position(|a| a == "--text")
        .and_then(|i| args.get(i + 1))
        .map(|s| s.as_str())
        .unwrap_or("Hello, this is a test of the Chatterbox TTS system.");

    let ref_audio = args
        .iter()
        .position(|a| a == "--ref-audio")
        .and_then(|i| args.get(i + 1))
        .map(|s| s.as_str())
        .unwrap_or("reference.wav");

    let output = args
        .iter()
        .position(|a| a == "--output")
        .and_then(|i| args.get(i + 1))
        .map(|s| s.as_str())
        .unwrap_or("output.wav");

    let use_cuda = args.iter().any(|a| a == "--cuda");

    // Select device
    let device = if use_cuda {
        println!("Using CUDA...");
        Device::new_cuda(0)?
    } else {
        println!("Using CPU...");
        Device::Cpu
    };

    // Check reference audio exists
    if !Path::new(ref_audio).exists() {
        anyhow::bail!("Reference audio not found: {}", ref_audio);
    }

    // Load model
    println!("Loading Chatterbox Turbo model...");
    let model = ChatterboxTurboTTS::from_pretrained(device)?;
    println!("Model loaded.");

    // Generate speech
    println!("Generating speech for: \"{}\"", text);
    let config = GenerateConfig::default();
    let (samples, sample_rate) = model.generate_speech(text, Path::new(ref_audio), config)?;

    // Save output
    audio::save_wav(output, &samples, sample_rate).map_err(|e| anyhow::anyhow!("{}", e))?;
    println!(
        "Audio saved to: {} ({} samples @ {}Hz)",
        output,
        samples.len(),
        sample_rate
    );

    Ok(())
}
</file>

<file path="candle/examples/list_keys.rs">
use candle_core::Device;
use candle_nn::VarBuilder;
use std::path::PathBuf;

fn main() -> anyhow::Result<()> {
    let args: Vec<String> = std::env::args().collect();
    if args.len() < 2 {
        println!("Usage: list_keys <file_path>");
        return Ok(());
    }
    let path = PathBuf::from(&args[1]);
    let vb = unsafe {
        VarBuilder::from_mmaped_safetensors(&[path], candle_core::DType::F32, &Device::Cpu)?
    };

    // VarBuilder doesn't provide a direct way to list all keys it *can* see,
    // but the underlying safetensors file does.
    let file = std::fs::File::open(&args[1])?;
    let mem = unsafe { memmap2::MmapOptions::new().map(&file)? };
    let safetensors = safetensors::SafeTensors::deserialize(&mem)?;

    println!("File: {}", args[1]);
    let mut names: Vec<_> = safetensors.names().into_iter().collect();
    names.sort();
    for name in names {
        let view = safetensors.tensor(&name)?;
        println!("{}: {:?}", name, view.shape());
    }

    Ok(())
}
</file>

<file path="candle/examples/verify_weights.rs">
use candle_core::{DType, Device};
use candle_nn::VarBuilder;

/// Weight loading verification test
/// Compares tensor shapes and norms between Python and Rust loading
fn main() -> anyhow::Result<()> {
    let device = Device::Cpu;
    let model_path = "C:/Users/Steve Business/.cache/huggingface/hub/models--ResembleAI--chatterbox-turbo/snapshots/749d1c1a46eb10492095d68fbcf55691ccf137cd/s3gen_meanflow.safetensors";

    if !std::path::Path::new(model_path).exists() {
        anyhow::bail!("Model file not found. Run Chatterbox to download first.");
    }

    println!("=== WEIGHT LOADING VERIFICATION TEST ===\n");

    // Load raw safetensors to get ground truth
    let file = std::fs::File::open(model_path)?;
    let mem = unsafe { memmap2::MmapOptions::new().map(&file)? };
    let safetensors = safetensors::SafeTensors::deserialize(&mem)?;

    // Test 1: CAMPPlus weight loading
    println!("--- TEST 1: CAMPPlus (speaker_encoder.*) ---");

    // Expected key samples from Python analysis
    let campplus_key_samples = vec![
        ("speaker_encoder.head.bn1.weight", vec![32]),
        ("speaker_encoder.head.conv1.weight", vec![32, 1, 3, 3]),
        (
            "speaker_encoder.xvector.tdnn.linear.weight",
            vec![128, 320, 5],
        ),
        (
            "speaker_encoder.xvector.block1.tdnnd1.cam_layer.linear1.weight",
            vec![64, 128, 1],
        ),
        (
            "speaker_encoder.xvector.transit1.linear.weight",
            vec![256, 512, 1],
        ),
        (
            "speaker_encoder.xvector.dense.linear.weight",
            vec![192, 1024, 1],
        ), // 512*2=1024 after stats pool
    ];

    for (key, expected_shape) in &campplus_key_samples {
        match safetensors.tensor(key) {
            Ok(view) => {
                let shape: Vec<usize> = view.shape().to_vec();
                let match_status = if &shape == expected_shape {
                    "✓"
                } else {
                    "✗"
                };
                println!(
                    "  {match_status} {key}: {:?} (expected {:?})",
                    shape, expected_shape
                );
            }
            Err(_) => {
                println!("  ✗ {key}: NOT FOUND");
            }
        }
    }

    // Test 2: HiFiGAN weight loading (mel2wav.*)
    println!("\n--- TEST 2: HiFiGAN (mel2wav.*) ---");

    let hifigan_key_samples = vec![
        (
            "mel2wav.conv_pre.parametrizations.weight.original0",
            vec![512, 1, 1],
        ),
        (
            "mel2wav.conv_pre.parametrizations.weight.original1",
            vec![512, 80, 7],
        ),
        (
            "mel2wav.f0_predictor.condnet.0.parametrizations.weight.original1",
            vec![512, 80, 3],
        ),
        ("mel2wav.f0_predictor.classifier.weight", vec![1, 512]),
        ("mel2wav.m_source.l_linear.weight", vec![1, 9]),
        (
            "mel2wav.ups.0.parametrizations.weight.original1",
            vec![512, 256, 16],
        ),
    ];

    for (key, expected_shape) in &hifigan_key_samples {
        match safetensors.tensor(key) {
            Ok(view) => {
                let shape: Vec<usize> = view.shape().to_vec();
                let match_status = if &shape == expected_shape {
                    "✓"
                } else {
                    "✗"
                };
                println!(
                    "  {match_status} {key}: {:?} (expected {:?})",
                    shape, expected_shape
                );
            }
            Err(_) => {
                println!("  ✗ {key}: NOT FOUND");
            }
        }
    }

    // Test 3: Flow encoder (flow.*)
    println!("\n--- TEST 3: Flow (flow.*) ---");

    let flow_key_samples = vec![
        ("flow.input_embedding.weight", vec![6561, 512]),
        ("flow.spk_embed_affine_layer.weight", vec![80, 192]), // Used for conditioning, output=80
        ("flow.encoder.embed.out.0.weight", vec![512, 512]),
        (
            "flow.encoder.encoders.0.feed_forward.w_1.weight",
            vec![2048, 512],
        ),
        (
            "flow.decoder.estimator.down_blocks.0.0.block1.block.0.weight",
            vec![256, 320, 3],
        ),
    ];

    for (key, expected_shape) in &flow_key_samples {
        match safetensors.tensor(key) {
            Ok(view) => {
                let shape: Vec<usize> = view.shape().to_vec();
                let match_status = if &shape == expected_shape {
                    "✓"
                } else {
                    "✗"
                };
                println!(
                    "  {match_status} {key}: {:?} (expected {:?})",
                    shape, expected_shape
                );
            }
            Err(_) => {
                println!("  ✗ {key}: NOT FOUND");
            }
        }
    }

    // Test 4: Try loading CAMPPlus via VarBuilder
    println!("\n--- TEST 4: VarBuilder Loading Test ---");

    let vb = unsafe { VarBuilder::from_mmaped_safetensors(&[model_path], DType::F32, &device)? };

    // Try loading with speaker_encoder prefix
    let vb_campplus = vb.pp("speaker_encoder");

    // Test a specific weight
    match vb_campplus.pp("head.bn1").get((32,), "weight") {
        Ok(tensor) => {
            println!(
                "  ✓ speaker_encoder.head.bn1.weight loaded: {:?}",
                tensor.dims()
            );
        }
        Err(e) => {
            println!("  ✗ speaker_encoder.head.bn1.weight failed: {}", e);
        }
    }

    match vb_campplus
        .pp("xvector.tdnn.linear")
        .get((128, 320, 5), "weight")
    {
        Ok(tensor) => {
            println!(
                "  ✓ speaker_encoder.xvector.tdnn.linear.weight loaded: {:?}",
                tensor.dims()
            );
        }
        Err(e) => {
            println!(
                "  ✗ speaker_encoder.xvector.tdnn.linear.weight failed: {}",
                e
            );
        }
    }

    // Test mel2wav prefix
    let vb_mel2wav = vb.pp("mel2wav");
    match vb_mel2wav
        .pp("f0_predictor.classifier")
        .get((1, 512), "weight")
    {
        Ok(tensor) => {
            println!(
                "  ✓ mel2wav.f0_predictor.classifier.weight loaded: {:?}",
                tensor.dims()
            );
        }
        Err(e) => {
            println!("  ✗ mel2wav.f0_predictor.classifier.weight failed: {}", e);
        }
    }

    println!("\n=== VERIFICATION COMPLETE ===");
    Ok(())
}
</file>

<file path="candle/output.txt">
Downloading model files...
[DEBUG] T3 Path: "C:\\Users\\Steve Business\\.cache\\huggingface\\hub\\models--ResembleAI--chatterbox-turbo\\snapshots\\749d1c1a46eb10492095d68fbcf55691ccf137cd\\t3_turbo_v1.safetensors"
[DEBUG] Keys in t3_turbo_v1.safetensors:
  cond_enc.spkr_enc.bias
  cond_enc.spkr_enc.weight
  speech_emb.weight
  speech_head.bias
  speech_head.weight
  text_emb.weight
  text_head.weight
  tfmr.h.0.attn.c_attn.bias
  tfmr.h.0.attn.c_attn.weight
  tfmr.h.0.attn.c_proj.bias
  tfmr.h.0.attn.c_proj.weight
  tfmr.h.0.ln_1.bias
  tfmr.h.0.ln_1.weight
  tfmr.h.0.ln_2.bias
  tfmr.h.0.ln_2.weight
  tfmr.h.0.mlp.c_fc.bias
  tfmr.h.0.mlp.c_fc.weight
  tfmr.h.0.mlp.c_proj.bias
  tfmr.h.0.mlp.c_proj.weight
  tfmr.h.1.attn.c_attn.bias
  tfmr.h.1.attn.c_attn.weight
  tfmr.h.1.attn.c_proj.bias
  tfmr.h.1.attn.c_proj.weight
  tfmr.h.1.ln_1.bias
  tfmr.h.1.ln_1.weight
  tfmr.h.1.ln_2.bias
  tfmr.h.1.ln_2.weight
  tfmr.h.1.mlp.c_fc.bias
  tfmr.h.1.mlp.c_fc.weight
  tfmr.h.1.mlp.c_proj.bias
  tfmr.h.1.mlp.c_proj.weight
  tfmr.h.10.attn.c_attn.bias
  tfmr.h.10.attn.c_attn.weight
  tfmr.h.10.attn.c_proj.bias
  tfmr.h.10.attn.c_proj.weight
  tfmr.h.10.ln_1.bias
  tfmr.h.10.ln_1.weight
  tfmr.h.10.ln_2.bias
  tfmr.h.10.ln_2.weight
  tfmr.h.10.mlp.c_fc.bias
  tfmr.h.10.mlp.c_fc.weight
  tfmr.h.10.mlp.c_proj.bias
  tfmr.h.10.mlp.c_proj.weight
  tfmr.h.11.attn.c_attn.bias
  tfmr.h.11.attn.c_attn.weight
  tfmr.h.11.attn.c_proj.bias
  tfmr.h.11.attn.c_proj.weight
  tfmr.h.11.ln_1.bias
  tfmr.h.11.ln_1.weight
  tfmr.h.11.ln_2.bias
  tfmr.h.11.ln_2.weight
  tfmr.h.11.mlp.c_fc.bias
  tfmr.h.11.mlp.c_fc.weight
  tfmr.h.11.mlp.c_proj.bias
  tfmr.h.11.mlp.c_proj.weight
  tfmr.h.12.attn.c_attn.bias
  tfmr.h.12.attn.c_attn.weight
  tfmr.h.12.attn.c_proj.bias
  tfmr.h.12.attn.c_proj.weight
  tfmr.h.12.ln_1.bias
  tfmr.h.12.ln_1.weight
  tfmr.h.12.ln_2.bias
  tfmr.h.12.ln_2.weight
  tfmr.h.12.mlp.c_fc.bias
  tfmr.h.12.mlp.c_fc.weight
  tfmr.h.12.mlp.c_proj.bias
  tfmr.h.12.mlp.c_proj.weight
  tfmr.h.13.attn.c_attn.bias
  tfmr.h.13.attn.c_attn.weight
  tfmr.h.13.attn.c_proj.bias
  tfmr.h.13.attn.c_proj.weight
  tfmr.h.13.ln_1.bias
  tfmr.h.13.ln_1.weight
  tfmr.h.13.ln_2.bias
  tfmr.h.13.ln_2.weight
  tfmr.h.13.mlp.c_fc.bias
  tfmr.h.13.mlp.c_fc.weight
  tfmr.h.13.mlp.c_proj.bias
  tfmr.h.13.mlp.c_proj.weight
  tfmr.h.14.attn.c_attn.bias
  tfmr.h.14.attn.c_attn.weight
  tfmr.h.14.attn.c_proj.bias
  tfmr.h.14.attn.c_proj.weight
  tfmr.h.14.ln_1.bias
  tfmr.h.14.ln_1.weight
  tfmr.h.14.ln_2.bias
  tfmr.h.14.ln_2.weight
  tfmr.h.14.mlp.c_fc.bias
  tfmr.h.14.mlp.c_fc.weight
  tfmr.h.14.mlp.c_proj.bias
  tfmr.h.14.mlp.c_proj.weight
  tfmr.h.15.attn.c_attn.bias
  tfmr.h.15.attn.c_attn.weight
  tfmr.h.15.attn.c_proj.bias
  tfmr.h.15.attn.c_proj.weight
  tfmr.h.15.ln_1.bias
  tfmr.h.15.ln_1.weight
  tfmr.h.15.ln_2.bias
  tfmr.h.15.ln_2.weight
  tfmr.h.15.mlp.c_fc.bias
  tfmr.h.15.mlp.c_fc.weight
  tfmr.h.15.mlp.c_proj.bias
  tfmr.h.15.mlp.c_proj.weight
  tfmr.h.16.attn.c_attn.bias
  tfmr.h.16.attn.c_attn.weight
  tfmr.h.16.attn.c_proj.bias
  tfmr.h.16.attn.c_proj.weight
  tfmr.h.16.ln_1.bias
  tfmr.h.16.ln_1.weight
  tfmr.h.16.ln_2.bias
  tfmr.h.16.ln_2.weight
  tfmr.h.16.mlp.c_fc.bias
  tfmr.h.16.mlp.c_fc.weight
  tfmr.h.16.mlp.c_proj.bias
  tfmr.h.16.mlp.c_proj.weight
  tfmr.h.17.attn.c_attn.bias
  tfmr.h.17.attn.c_attn.weight
  tfmr.h.17.attn.c_proj.bias
  tfmr.h.17.attn.c_proj.weight
  tfmr.h.17.ln_1.bias
  tfmr.h.17.ln_1.weight
  tfmr.h.17.ln_2.bias
  tfmr.h.17.ln_2.weight
  tfmr.h.17.mlp.c_fc.bias
  tfmr.h.17.mlp.c_fc.weight
  tfmr.h.17.mlp.c_proj.bias
  tfmr.h.17.mlp.c_proj.weight
  tfmr.h.18.attn.c_attn.bias
  tfmr.h.18.attn.c_attn.weight
  tfmr.h.18.attn.c_proj.bias
  tfmr.h.18.attn.c_proj.weight
  tfmr.h.18.ln_1.bias
  tfmr.h.18.ln_1.weight
  tfmr.h.18.ln_2.bias
  tfmr.h.18.ln_2.weight
  tfmr.h.18.mlp.c_fc.bias
  tfmr.h.18.mlp.c_fc.weight
  tfmr.h.18.mlp.c_proj.bias
  tfmr.h.18.mlp.c_proj.weight
  tfmr.h.19.attn.c_attn.bias
  tfmr.h.19.attn.c_attn.weight
  tfmr.h.19.attn.c_proj.bias
  tfmr.h.19.attn.c_proj.weight
  tfmr.h.19.ln_1.bias
  tfmr.h.19.ln_1.weight
  tfmr.h.19.ln_2.bias
  tfmr.h.19.ln_2.weight
  tfmr.h.19.mlp.c_fc.bias
  tfmr.h.19.mlp.c_fc.weight
  tfmr.h.19.mlp.c_proj.bias
  tfmr.h.19.mlp.c_proj.weight
  tfmr.h.2.attn.c_attn.bias
  tfmr.h.2.attn.c_attn.weight
  tfmr.h.2.attn.c_proj.bias
  tfmr.h.2.attn.c_proj.weight
  tfmr.h.2.ln_1.bias
  tfmr.h.2.ln_1.weight
  tfmr.h.2.ln_2.bias
  tfmr.h.2.ln_2.weight
  tfmr.h.2.mlp.c_fc.bias
  tfmr.h.2.mlp.c_fc.weight
  tfmr.h.2.mlp.c_proj.bias
  tfmr.h.2.mlp.c_proj.weight
  tfmr.h.20.attn.c_attn.bias
  tfmr.h.20.attn.c_attn.weight
  tfmr.h.20.attn.c_proj.bias
  tfmr.h.20.attn.c_proj.weight
  tfmr.h.20.ln_1.bias
  tfmr.h.20.ln_1.weight
  tfmr.h.20.ln_2.bias
  tfmr.h.20.ln_2.weight
  tfmr.h.20.mlp.c_fc.bias
  tfmr.h.20.mlp.c_fc.weight
  tfmr.h.20.mlp.c_proj.bias
  tfmr.h.20.mlp.c_proj.weight
  tfmr.h.21.attn.c_attn.bias
  tfmr.h.21.attn.c_attn.weight
  tfmr.h.21.attn.c_proj.bias
  tfmr.h.21.attn.c_proj.weight
  tfmr.h.21.ln_1.bias
  tfmr.h.21.ln_1.weight
  tfmr.h.21.ln_2.bias
  tfmr.h.21.ln_2.weight
  tfmr.h.21.mlp.c_fc.bias
  tfmr.h.21.mlp.c_fc.weight
  tfmr.h.21.mlp.c_proj.bias
  tfmr.h.21.mlp.c_proj.weight
  tfmr.h.22.attn.c_attn.bias
  tfmr.h.22.attn.c_attn.weight
  tfmr.h.22.attn.c_proj.bias
  tfmr.h.22.attn.c_proj.weight
  tfmr.h.22.ln_1.bias
  tfmr.h.22.ln_1.weight
  tfmr.h.22.ln_2.bias
  tfmr.h.22.ln_2.weight
  tfmr.h.22.mlp.c_fc.bias
  tfmr.h.22.mlp.c_fc.weight
  tfmr.h.22.mlp.c_proj.bias
  tfmr.h.22.mlp.c_proj.weight
  tfmr.h.23.attn.c_attn.bias
  tfmr.h.23.attn.c_attn.weight
  tfmr.h.23.attn.c_proj.bias
  tfmr.h.23.attn.c_proj.weight
  tfmr.h.23.ln_1.bias
  tfmr.h.23.ln_1.weight
  tfmr.h.23.ln_2.bias
  tfmr.h.23.ln_2.weight
  tfmr.h.23.mlp.c_fc.bias
  tfmr.h.23.mlp.c_fc.weight
  tfmr.h.23.mlp.c_proj.bias
  tfmr.h.23.mlp.c_proj.weight
  tfmr.h.3.attn.c_attn.bias
  tfmr.h.3.attn.c_attn.weight
  tfmr.h.3.attn.c_proj.bias
  tfmr.h.3.attn.c_proj.weight
  tfmr.h.3.ln_1.bias
  tfmr.h.3.ln_1.weight
  tfmr.h.3.ln_2.bias
  tfmr.h.3.ln_2.weight
  tfmr.h.3.mlp.c_fc.bias
  tfmr.h.3.mlp.c_fc.weight
  tfmr.h.3.mlp.c_proj.bias
  tfmr.h.3.mlp.c_proj.weight
  tfmr.h.4.attn.c_attn.bias
  tfmr.h.4.attn.c_attn.weight
  tfmr.h.4.attn.c_proj.bias
  tfmr.h.4.attn.c_proj.weight
  tfmr.h.4.ln_1.bias
  tfmr.h.4.ln_1.weight
  tfmr.h.4.ln_2.bias
  tfmr.h.4.ln_2.weight
  tfmr.h.4.mlp.c_fc.bias
  tfmr.h.4.mlp.c_fc.weight
  tfmr.h.4.mlp.c_proj.bias
  tfmr.h.4.mlp.c_proj.weight
  tfmr.h.5.attn.c_attn.bias
  tfmr.h.5.attn.c_attn.weight
  tfmr.h.5.attn.c_proj.bias
  tfmr.h.5.attn.c_proj.weight
  tfmr.h.5.ln_1.bias
  tfmr.h.5.ln_1.weight
  tfmr.h.5.ln_2.bias
  tfmr.h.5.ln_2.weight
  tfmr.h.5.mlp.c_fc.bias
  tfmr.h.5.mlp.c_fc.weight
  tfmr.h.5.mlp.c_proj.bias
  tfmr.h.5.mlp.c_proj.weight
  tfmr.h.6.attn.c_attn.bias
  tfmr.h.6.attn.c_attn.weight
  tfmr.h.6.attn.c_proj.bias
  tfmr.h.6.attn.c_proj.weight
  tfmr.h.6.ln_1.bias
  tfmr.h.6.ln_1.weight
  tfmr.h.6.ln_2.bias
  tfmr.h.6.ln_2.weight
  tfmr.h.6.mlp.c_fc.bias
  tfmr.h.6.mlp.c_fc.weight
  tfmr.h.6.mlp.c_proj.bias
  tfmr.h.6.mlp.c_proj.weight
  tfmr.h.7.attn.c_attn.bias
  tfmr.h.7.attn.c_attn.weight
  tfmr.h.7.attn.c_proj.bias
  tfmr.h.7.attn.c_proj.weight
  tfmr.h.7.ln_1.bias
  tfmr.h.7.ln_1.weight
  tfmr.h.7.ln_2.bias
  tfmr.h.7.ln_2.weight
  tfmr.h.7.mlp.c_fc.bias
  tfmr.h.7.mlp.c_fc.weight
  tfmr.h.7.mlp.c_proj.bias
  tfmr.h.7.mlp.c_proj.weight
  tfmr.h.8.attn.c_attn.bias
  tfmr.h.8.attn.c_attn.weight
  tfmr.h.8.attn.c_proj.bias
  tfmr.h.8.attn.c_proj.weight
  tfmr.h.8.ln_1.bias
  tfmr.h.8.ln_1.weight
  tfmr.h.8.ln_2.bias
  tfmr.h.8.ln_2.weight
  tfmr.h.8.mlp.c_fc.bias
  tfmr.h.8.mlp.c_fc.weight
  tfmr.h.8.mlp.c_proj.bias
  tfmr.h.8.mlp.c_proj.weight
  tfmr.h.9.attn.c_attn.bias
  tfmr.h.9.attn.c_attn.weight
  tfmr.h.9.attn.c_proj.bias
  tfmr.h.9.attn.c_proj.weight
  tfmr.h.9.ln_1.bias
  tfmr.h.9.ln_1.weight
  tfmr.h.9.ln_2.bias
  tfmr.h.9.ln_2.weight
  tfmr.h.9.mlp.c_fc.bias
  tfmr.h.9.mlp.c_fc.weight
  tfmr.h.9.mlp.c_proj.bias
  tfmr.h.9.mlp.c_proj.weight
  tfmr.ln_f.bias
  tfmr.ln_f.weight
  tfmr.wpe.weight
  tfmr.wte.weight

[Step 4/6] Extracting speaker embedding with VoiceEncoder...
[Step 5/6] Extracting synthesis embedding with CAMPPlus...

[Step 6/6] Generating tokens with T3...

[Step 7/6] Synthesizing audio...
  S3Gen Input Tokens Shape: [1, 327]
  Conditioning Tensor Shape: [1, 80, 654]
SUCCESS! Audio saved to: output.wav
</file>

<file path="demo.rs">
use anyhow::Result;
use half::f16;
use hf_hub::{api::sync::Api, Repo, RepoType};
use hound;
use ndarray::{concatenate, s, Array, Array1, Array2, Array4, ArrayD, Axis};
use ort::{
    execution_providers::CUDAExecutionProvider,
    inputs,
    session::{Session, SessionInputValue},
    value::Value,
};
use std::collections::HashMap;
use std::path::PathBuf;
use tokenizers::Tokenizer; // Added import

// --- CONSTANTS ---
const MODEL_REPO: &str = "ResembleAI/chatterbox-turbo-ONNX";
const SAMPLE_RATE: u32 = 24000;
const START_SPEECH_TOKEN: i64 = 6561;
const STOP_SPEECH_TOKEN: i64 = 6562;
const SILENCE_TOKEN: i64 = 4299;
const NUM_KV_HEADS: usize = 16;
const HEAD_DIM: usize = 64;
const MODEL_DTYPE: &str = "fp16";

pub fn run_inference() -> Result<()> {
    // Initialize ORT with CUDA
    ort::init()
        .with_name("chatterbox")
        .with_execution_providers([CUDAExecutionProvider::default().build()])
        .commit()?;

    println!("--- Chatterbox Turbo Rust Inference (ORT + CUDA) ---");
    println!("Target DType: {}", MODEL_DTYPE);

    // 1. Download/Locate Models
    let model_paths = download_models(MODEL_DTYPE)?;

    // 2. Load Tokenizer
    println!("Initializing API for tokenizer...");
    let api = Api::new()?;
    let repo = api.repo(Repo::new(MODEL_REPO.to_string(), RepoType::Model));
    println!("Fetching tokenizer.json...");
    let tokenizer_path = repo.get("tokenizer.json")?;
    println!("Tokenizer fetched at {:?}", tokenizer_path);
    let tokenizer = Tokenizer::from_file(tokenizer_path).map_err(|e| anyhow::anyhow!(e))?;

    // 3. Create Sessions
    let mut speech_encoder = Session::builder()?
        .with_execution_providers([CUDAExecutionProvider::default().build()])?
        .commit_from_file(&model_paths.speech_encoder)?;
    let mut embed_tokens = Session::builder()?
        .with_execution_providers([CUDAExecutionProvider::default().build()])?
        .commit_from_file(&model_paths.embed_tokens)?;
    let mut language_model = Session::builder()?
        .with_execution_providers([CUDAExecutionProvider::default().build()])?
        .commit_from_file(&model_paths.language_model)?;
    let mut cond_decoder = Session::builder()?
        .with_execution_providers([CUDAExecutionProvider::default().build()])?
        .commit_from_file(&model_paths.conditional_decoder)?;

    // --- INPUT DATA ---
    let text = "Oh, that's hilarious! [chuckle] Um anyway, how are you doing today?";
    let ref_audio_path = if std::path::Path::new("voice_input.wav").exists() {
        "voice_input.wav"
    } else {
        "reference.wav"
    };

    if !std::path::Path::new(ref_audio_path).exists() {
        anyhow::bail!("Input file '{}' not found.", ref_audio_path);
    }

    println!("Processing: '{}' using '{}'...", text, ref_audio_path);

    // 4. Prepare Audio Input
    let (audio_values, sr) = read_wav_as_tensor(ref_audio_path)?;
    if sr != SAMPLE_RATE {
        println!(
            "Warning: Input samplerate {} != {}. Result may be bad.",
            sr, SAMPLE_RATE
        );
    }

    // 5. Prepare Text Input
    let encoding = tokenizer
        .encode(text, true)
        .map_err(|e| anyhow::anyhow!(e))?;
    let input_ids_vec: Vec<i64> = encoding.get_ids().iter().map(|&x| x as i64).collect();
    let input_ids = Array2::from_shape_vec((1, input_ids_vec.len()), input_ids_vec)?;

    // 6. Run Speech Encoder
    // Explicitly create Values to avoid inputs! macro issues with Views
    // Convert audio to fp16 if using fp16 model
    let audio_with_batch = audio_values.insert_axis(Axis(0));
    let audio_val_ort = Value::from_array(audio_with_batch.into_dyn())?;

    let outputs = speech_encoder.run(inputs!["audio_values" => audio_val_ort])?;

    // Helper to extract fp16 tensor and convert to f32 ArrayD
    let extract_f32_tensor = |val: &Value, name: &str| -> Result<ArrayD<f32>> {
        // In fp16 mode, model outputs are f16, try that first
        if MODEL_DTYPE == "fp16" {
            if let Ok((s, d)) = val.try_extract_tensor::<f16>() {
                let f32_data: Vec<f32> = d.iter().map(|x| x.to_f32()).collect();
                return Ok(ArrayD::from_shape_vec(shape_to_vec(&s), f32_data)?);
            }
        }
        // Fallback to f32
        let (s, d) = val
            .try_extract_tensor::<f32>()
            .map_err(|e| anyhow::anyhow!("Failed to extract {} as f32: {}", name, e))?;
        Ok(ArrayD::from_shape_vec(shape_to_vec(&s), d.to_vec())?)
    };

    let cond_emb = extract_f32_tensor(&outputs["audio_features"], "audio_features")?;

    let (s, d) = outputs["audio_tokens"].try_extract_tensor::<i64>()?;
    let prompt_token = ArrayD::from_shape_vec(shape_to_vec(&s), d.to_vec())?;

    let speaker_embeddings =
        extract_f32_tensor(&outputs["speaker_embeddings"], "speaker_embeddings")?;
    let speaker_features = extract_f32_tensor(&outputs["speaker_features"], "speaker_features")?;

    // Helper to extract embeddings (works for both f16 and f32 models)
    let extract_embeds_tensor = |val: &Value| -> Result<ArrayD<f32>> {
        if MODEL_DTYPE == "fp16" {
            if let Ok((s, d)) = val.try_extract_tensor::<f16>() {
                let f32_data: Vec<f32> = d.iter().map(|x| x.to_f32()).collect();
                return Ok(ArrayD::from_shape_vec(shape_to_vec(&s), f32_data)?);
            }
        }
        let (s, d) = val.try_extract_tensor::<f32>()?;
        Ok(ArrayD::from_shape_vec(shape_to_vec(&s), d.to_vec())?)
    };

    // 7. Initial Embeddings
    let text_embeds = {
        let input_ids_ort = Value::from_array(input_ids.to_owned().into_dyn())?;
        let outputs = embed_tokens.run(inputs!["input_ids" => input_ids_ort])?;
        extract_embeds_tensor(&outputs["inputs_embeds"])?
    };

    // Concatenate cond_emb and inputs_embeds along axis 1
    let cond_emb_3d = cond_emb.view().into_dimensionality::<ndarray::Ix3>()?;
    let text_embeds_3d = text_embeds.view().into_dimensionality::<ndarray::Ix3>()?;
    let mut inputs_embeds = concatenate(Axis(1), &[cond_emb_3d.view(), text_embeds_3d.view()])?;

    // 8. Initialize Cache
    let batch_size = 1;
    let mut past_key_values: HashMap<String, Array4<f32>> = HashMap::new();
    for input in language_model.inputs.iter() {
        if input.name.contains("past_key_values") {
            let cache = Array4::<f32>::zeros((batch_size, NUM_KV_HEADS, 0, HEAD_DIM));
            past_key_values.insert(input.name.clone(), cache);
        }
    }

    let mut attention_mask = Array2::<i64>::ones((batch_size, inputs_embeds.shape()[1]));
    let mut position_ids = Array::from_iter(0..inputs_embeds.shape()[1] as i64)
        .into_shape_with_order((1, inputs_embeds.shape()[1]))?
        .mapv(|x| x as i64);

    let mut generate_tokens = Array2::<i64>::from_elem((1, 1), START_SPEECH_TOKEN);

    // Inspect and store expected input types for dynamic dtype handling
    let mut input_is_f16: HashMap<String, bool> = HashMap::new();
    for input in language_model.inputs.iter() {
        let type_str = format!("{:?}", input.input_type);
        let is_f16 = type_str.contains("Float16");
        input_is_f16.insert(input.name.clone(), is_f16);
    }

    println!("Starting generation loop...");
    let max_new_tokens = 1024;

    // Helper for tensor creation with dynamic dtype
    let make_tensor = |arr: &ArrayD<f32>, is_f16: bool| -> Result<Value> {
        if is_f16 {
            let arr_f16 = arr.mapv(|x| f16::from_f32(x));
            Ok(Value::from_array(arr_f16)?.into_dyn())
        } else {
            Ok(Value::from_array(arr.to_owned())?.into_dyn())
        }
    };

    for _i in 0..max_new_tokens {
        // Use Vec<(Cow, SessionInputValue)> to handle mixed types
        let mut dynamic_inputs: Vec<(std::borrow::Cow<'_, str>, SessionInputValue<'_>)> =
            Vec::new();

        // inputs_embeds - check model metadata for expected dtype
        let embeds_is_f16 = *input_is_f16.get("inputs_embeds").unwrap_or(&false);
        dynamic_inputs.push((
            "inputs_embeds".into(),
            make_tensor(&inputs_embeds.clone().into_dyn(), embeds_is_f16)?.into(),
        ));
        dynamic_inputs.push((
            "attention_mask".into(),
            Value::from_array(attention_mask.to_owned().into_dyn())?.into(),
        ));
        dynamic_inputs.push((
            "position_ids".into(),
            Value::from_array(position_ids.to_owned().into_dyn())?.into(),
        ));

        // past_key_values - check model metadata for expected dtype
        for (k, v) in &past_key_values {
            let is_f16 = *input_is_f16.get(k).unwrap_or(&false);
            dynamic_inputs.push((
                k.clone().into(),
                make_tensor(&v.clone().into_dyn(), is_f16)?.into(),
            ));
        }

        let outputs = language_model.run(dynamic_inputs)?;

        // Extract logits - always f32 (model outputs f32 even with fp16 internals)
        let (s, d) = outputs["logits"].try_extract_tensor::<f32>()?;
        let logits = ArrayD::from_shape_vec(shape_to_vec(&s), d.to_vec())?;

        let last_logits = logits.slice(s![.., -1, ..]).to_owned();

        // Apply repetition penalty to discourage repeating already-generated tokens
        const REPETITION_PENALTY: f32 = 1.2;
        let mut penalized_logits = last_logits.clone().into_dimensionality::<ndarray::Ix2>()?;
        for &token_id in generate_tokens.iter() {
            if token_id >= 0 && (token_id as usize) < penalized_logits.shape()[1] {
                let score = penalized_logits[[0, token_id as usize]];
                // Apply penalty: if score < 0, multiply by penalty; if score > 0, divide by penalty
                penalized_logits[[0, token_id as usize]] = if score < 0.0 {
                    score * REPETITION_PENALTY
                } else {
                    score / REPETITION_PENALTY
                };
            }
        }

        let next_token_id = penalized_logits
            .iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .map(|(idx, _)| idx as i64)
            .unwrap();

        let next_token = Array2::from_elem((1, 1), next_token_id);
        generate_tokens = concatenate(Axis(1), &[generate_tokens.view(), next_token.view()])?;

        if next_token_id == STOP_SPEECH_TOKEN {
            break;
        }

        // --- Prepare for Next Step ---
        let next_token_ort = Value::from_array(next_token.to_owned().into_dyn())?;
        let outputs_embed = embed_tokens.run(inputs!["input_ids" => next_token_ort])?;
        inputs_embeds = extract_embeds_tensor(&outputs_embed["inputs_embeds"])?
            .into_dimensionality::<ndarray::Ix3>()?;

        let ones = Array2::<i64>::ones((batch_size, 1));
        attention_mask = concatenate(Axis(1), &[attention_mask.view(), ones.view()])?;

        let binding = position_ids
            .slice(s![.., -1])
            .mapv(|x| x + 1)
            .insert_axis(Axis(1));
        position_ids = binding.to_owned();

        for (input_name, cache_tensor) in past_key_values.iter_mut() {
            let output_name = input_name.replace("past_key_values", "present");
            if let Some(val) = outputs.get(&output_name) {
                // Extract present state - f16 for fp16 models (matches past_key_values input dtype)
                let is_f16 = *input_is_f16.get(input_name).unwrap_or(&false);
                let (s_vec, data) = if is_f16 {
                    let (s, d) = val.try_extract_tensor::<f16>()?;
                    (
                        shape_to_vec(&s),
                        d.iter().map(|x| x.to_f32()).collect::<Vec<f32>>(),
                    )
                } else {
                    let (s, d) = val.try_extract_tensor::<f32>()?;
                    (shape_to_vec(&s), d.to_vec())
                };
                // We know cache is 4D
                if s_vec.len() == 4 {
                    let tensor =
                        Array4::from_shape_vec((s_vec[0], s_vec[1], s_vec[2], s_vec[3]), data)?;
                    *cache_tensor = tensor;
                }
            }
        }
    }

    println!("Generation Complete. Decoding audio...");
    let len = generate_tokens.shape()[1];
    let speech_tokens = if len > 2 {
        generate_tokens.slice(s![.., 1..len - 1]).to_owned()
    } else {
        generate_tokens.slice(s![.., 1..]).to_owned()
    };

    let silence = Array2::<i64>::from_elem((1, 3), SILENCE_TOKEN);
    let prompt_token_2d = prompt_token.into_dimensionality::<ndarray::Ix2>()?;
    let speech_tokens_2d = speech_tokens.view();

    let speech_input = concatenate(
        Axis(1),
        &[prompt_token_2d.view(), speech_tokens_2d, silence.view()],
    )?;

    // cond_decoder inputs
    let wav_output = cond_decoder.run(inputs![
        "speech_tokens" => Value::from_array(speech_input.to_owned().into_dyn())?,
        "speaker_embeddings" => Value::from_array(speaker_embeddings.to_owned().into_dyn())?,
        "speaker_features" => Value::from_array(speaker_features.to_owned().into_dyn())?
    ])?;

    let (s, d) = wav_output[0].try_extract_tensor::<f32>()?;
    let wav = ArrayD::from_shape_vec(shape_to_vec(&s), d.to_vec())?;

    // Use into_raw_vec_and_offset() as per suggestion
    let wav_vec = wav.into_raw_vec_and_offset().0;

    let spec = hound::WavSpec {
        channels: 1,
        sample_rate: SAMPLE_RATE,
        bits_per_sample: 32,
        sample_format: hound::SampleFormat::Float,
    };
    let mut writer = hound::WavWriter::create("output.wav", spec)?;
    for sample in wav_vec {
        writer.write_sample(sample)?;
    }

    println!("Audio saved to output.wav");
    Ok(())
}

// Helper to handle Shape (Vec<i64> or similar) to Vec<usize>
fn shape_to_vec(shape: &ort::tensor::Shape) -> Vec<usize> {
    // ort::tensor::Shape is often just a Vec<i64> type alias or struct wrapping it
    // If it allows iteration, we map to usize.
    // If it doesn't have iter(), we might need to check strict docs.
    // Error said `no method named 'as_slice'`.
    // Trying `.iter()` directly (if it derefs to slice? no).
    // ort 2.0 rc9 has `Shape` = `Vec<i64>`?
    // Error message: `&ort::tensor::Shape`.
    // If it is a Vec<i64>, `shape.iter()` works.
    // The previous error `s.as_slice()` suggests `s` is `&Shape`.
    // Let's assume generic iteration works, or `to_vec()`.
    // Actually, `Shape` might be a distinct type in 2.0.
    // Let's try `shape.iter()`
    // If that fails, `shape` might essentially be `&[i64]`.
    // Let's assume it implements `AsRef<[i64]>`
    let dims: &[i64] = shape.as_ref();
    dims.iter().map(|&x| x as usize).collect()
}

fn read_wav_as_tensor(path: &str) -> Result<(Array1<f32>, u32)> {
    let reader = hound::WavReader::open(path)?;
    let spec = reader.spec();
    let samples: Vec<f32> = reader
        .into_samples::<i16>()
        .map(|x| x.map(|s| s as f32 / 32768.0))
        .collect::<Result<Vec<_>, _>>()?;
    Ok((Array1::from_vec(samples), spec.sample_rate))
}

struct ModelPaths {
    speech_encoder: PathBuf,
    embed_tokens: PathBuf,
    language_model: PathBuf,
    conditional_decoder: PathBuf,
}

fn download_models(dtype: &str) -> Result<ModelPaths> {
    let api = Api::new()?;
    let repo = api.repo(Repo::new(MODEL_REPO.to_string(), RepoType::Model));

    println!("Checking/Downloading models (dtype={}) from HF...", dtype);
    let get_model = |name: &str| -> Result<PathBuf> {
        let suffix = match dtype {
            "fp32" => "".to_string(),
            "q8" => "_quantized".to_string(),
            val => format!("_{}", val),
        };
        let filename = format!("{}{}.onnx", name, suffix);
        println!("Fetching {}...", filename);
        let model_path = repo.get(&format!("onnx/{}", filename))?;

        let data_filename = format!("onnx/{}_data", filename);
        if let Ok(_) = repo.get(&data_filename) {
            println!("Fetched weights data: {}", data_filename);
        }
        Ok(model_path)
    };

    Ok(ModelPaths {
        conditional_decoder: get_model("conditional_decoder")?,
        speech_encoder: get_model("speech_encoder")?,
        embed_tokens: get_model("embed_tokens")?,
        language_model: get_model("language_model")?,
    })
}
</file>

<file path="example_for_mac.py">
import torch
import torchaudio as ta
from chatterbox.tts import ChatterboxTTS

# Detect device (Mac with M1/M2/M3/M4)
device = "mps" if torch.backends.mps.is_available() else "cpu"
map_location = torch.device(device)

torch_load_original = torch.load
def patched_torch_load(*args, **kwargs):
    if 'map_location' not in kwargs:
        kwargs['map_location'] = map_location
    return torch_load_original(*args, **kwargs)

torch.load = patched_torch_load

model = ChatterboxTTS.from_pretrained(device=device)
text = "Today is the day. I want to move like a titan at dawn, sweat like a god forging lightning. No more excuses. From now on, my mornings will be temples of discipline. I am going to work out like the gods… every damn day."

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = "YOUR_FILE.wav"
wav = model.generate(
    text, 
    audio_prompt_path=AUDIO_PROMPT_PATH,
    exaggeration=2.0,
    cfg_weight=0.5
    )
ta.save("test-2.wav", wav, model.sr)
</file>

<file path="example_tts_turbo.py">
import torchaudio as ta
import torch
from chatterbox.tts_turbo import ChatterboxTurboTTS

# Load the Turbo model
model = ChatterboxTurboTTS.from_pretrained(device="cuda")

# Generate with Paralinguistic Tags
text = "Oh, that's hilarious! [chuckle] Um anyway, we do have a new model in store. It's the SkyNet T-800 series and it's got basically everything. Including AI integration with ChatGPT and all that jazz. Would you like me to get some prices for you?"

# Generate audio (requires a reference clip for voice cloning)
# wav = model.generate(text, audio_prompt_path="your_10s_ref_clip.wav")
wav = model.generate(text)
ta.save("test-turbo.wav", wav, model.sr)
</file>

<file path="gradio_tts_turbo_app.py">
import random
import numpy as np
import torch
import gradio as gr
from chatterbox.tts_turbo import ChatterboxTurboTTS

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

EVENT_TAGS = [
    "[clear throat]", "[sigh]", "[shush]", "[cough]", "[groan]",
    "[sniff]", "[gasp]", "[chuckle]", "[laugh]"
]

# --- REFINED CSS ---
# 1. tag-container: Forces the row to wrap items instead of scrolling. Removes borders/backgrounds.
# 2. tag-btn: Sets the specific look (indigo theme) and stops them from stretching.
CUSTOM_CSS = """
.tag-container {
    display: flex !important;
    flex-wrap: wrap !important; /* This fixes the one-per-line issue */
    gap: 8px !important;
    margin-top: 5px !important;
    margin-bottom: 10px !important;
    border: none !important;
    background: transparent !important;
}

.tag-btn {
    min-width: fit-content !important;
    width: auto !important;
    height: 32px !important;
    font-size: 13px !important;
    background: #eef2ff !important;
    border: 1px solid #c7d2fe !important;
    color: #3730a3 !important;
    border-radius: 6px !important;
    padding: 0 10px !important;
    margin: 0 !important;
    box-shadow: none !important;
}

.tag-btn:hover {
    background: #c7d2fe !important;
    transform: translateY(-1px);
}
"""

INSERT_TAG_JS = """
(tag_val, current_text) => {
    const textarea = document.querySelector('#main_textbox textarea');
    if (!textarea) return current_text + " " + tag_val; 

    const start = textarea.selectionStart;
    const end = textarea.selectionEnd;

    let prefix = " ";
    let suffix = " ";

    if (start === 0) prefix = "";
    else if (current_text[start - 1] === ' ') prefix = "";

    if (end < current_text.length && current_text[end] === ' ') suffix = "";

    return current_text.slice(0, start) + prefix + tag_val + suffix + current_text.slice(end);
}
"""


def set_seed(seed: int):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    np.random.seed(seed)


def load_model():
    print(f"Loading Chatterbox-Turbo on {DEVICE}...")
    model = ChatterboxTurboTTS.from_pretrained(DEVICE)
    return model


def generate(
        model,
        text,
        audio_prompt_path,
        temperature,
        seed_num,
        min_p,
        top_p,
        top_k,
        repetition_penalty,
        norm_loudness
):
    if model is None:
        model = ChatterboxTurboTTS.from_pretrained(DEVICE)

    if seed_num != 0:
        set_seed(int(seed_num))

    wav = model.generate(
        text,
        audio_prompt_path=audio_prompt_path,
        temperature=temperature,
        min_p=min_p,
        top_p=top_p,
        top_k=int(top_k),
        repetition_penalty=repetition_penalty,
        norm_loudness=norm_loudness,
    )
    return (model.sr, wav.squeeze(0).numpy())


with gr.Blocks(title="Chatterbox Turbo", css=CUSTOM_CSS) as demo:
    gr.Markdown("# ⚡ Chatterbox Turbo")

    model_state = gr.State(None)

    with gr.Row():
        with gr.Column():
            text = gr.Textbox(
                value="Oh, that's hilarious! [chuckle] Um anyway, we do have a new model in store. It's the SkyNet T-800 series and it's got basically everything. Including AI integration with ChatGPT and um all that jazz. Would you like me to get some prices for you?",
                label="Text to synthesize (max chars 300)",
                max_lines=5,
                elem_id="main_textbox"
            )

            # --- Event Tags ---
            # Switched back to Row, but applied specific CSS to force wrapping
            with gr.Row(elem_classes=["tag-container"]):
                for tag in EVENT_TAGS:
                    # elem_classes targets the button specifically
                    btn = gr.Button(tag, elem_classes=["tag-btn"])

                    btn.click(
                        fn=None,
                        inputs=[btn, text],
                        outputs=text,
                        js=INSERT_TAG_JS
                    )

            ref_wav = gr.Audio(
                sources=["upload", "microphone"],
                type="filepath",
                label="Reference Audio File",
                value="https://storage.googleapis.com/chatterbox-demo-samples/prompts/female_random_podcast.wav"
            )

            run_btn = gr.Button("Generate ⚡", variant="primary")

        with gr.Column():
            audio_output = gr.Audio(label="Output Audio")

            with gr.Accordion("Advanced Options", open=False):
                seed_num = gr.Number(value=0, label="Random seed (0 for random)")
                temp = gr.Slider(0.05, 2.0, step=.05, label="Temperature", value=0.8)
                top_p = gr.Slider(0.00, 1.00, step=0.01, label="Top P", value=0.95)
                top_k = gr.Slider(0, 1000, step=10, label="Top K", value=1000)
                repetition_penalty = gr.Slider(1.00, 2.00, step=0.05, label="Repetition Penalty", value=1.2)
                min_p = gr.Slider(0.00, 1.00, step=0.01, label="Min P (Set to 0 to disable)", value=0.00)
                norm_loudness = gr.Checkbox(value=True, label="Normalize Loudness (-27 LUFS)")

    demo.load(fn=load_model, inputs=[], outputs=model_state)

    run_btn.click(
        fn=generate,
        inputs=[
            model_state,
            text,
            ref_wav,
            temp,
            seed_num,
            min_p,
            top_p,
            top_k,
            repetition_penalty,
            norm_loudness,
        ],
        outputs=audio_output,
    )

if __name__ == "__main__":
    demo.queue(
        max_size=50,
        default_concurrency_limit=1,
    ).launch(share=True)
</file>

<file path="gradio_vc_app.py">
import torch
import gradio as gr
from chatterbox.vc import ChatterboxVC


DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


model = ChatterboxVC.from_pretrained(DEVICE)
def generate(audio, target_voice_path):
    wav = model.generate(
        audio, target_voice_path=target_voice_path,
    )
    return model.sr, wav.squeeze(0).numpy()


demo = gr.Interface(
    generate,
    [
        gr.Audio(sources=["upload", "microphone"], type="filepath", label="Input audio file"),
        gr.Audio(sources=["upload", "microphone"], type="filepath", label="Target voice audio file (if none, the default voice is used)", value=None),
    ],
    "audio",
)

if __name__ == "__main__":
    demo.launch()
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Resemble AI

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="s3tokenizer-v2-model/.gitattributes">
*.7z filter=lfs diff=lfs merge=lfs -text
*.arrow filter=lfs diff=lfs merge=lfs -text
*.bin filter=lfs diff=lfs merge=lfs -text
*.bz2 filter=lfs diff=lfs merge=lfs -text
*.ckpt filter=lfs diff=lfs merge=lfs -text
*.ftz filter=lfs diff=lfs merge=lfs -text
*.gz filter=lfs diff=lfs merge=lfs -text
*.h5 filter=lfs diff=lfs merge=lfs -text
*.joblib filter=lfs diff=lfs merge=lfs -text
*.lfs.* filter=lfs diff=lfs merge=lfs -text
*.mlmodel filter=lfs diff=lfs merge=lfs -text
*.model filter=lfs diff=lfs merge=lfs -text
*.msgpack filter=lfs diff=lfs merge=lfs -text
*.npy filter=lfs diff=lfs merge=lfs -text
*.npz filter=lfs diff=lfs merge=lfs -text
*.onnx filter=lfs diff=lfs merge=lfs -text
*.ot filter=lfs diff=lfs merge=lfs -text
*.parquet filter=lfs diff=lfs merge=lfs -text
*.pb filter=lfs diff=lfs merge=lfs -text
*.pickle filter=lfs diff=lfs merge=lfs -text
*.pkl filter=lfs diff=lfs merge=lfs -text
*.pt filter=lfs diff=lfs merge=lfs -text
*.pth filter=lfs diff=lfs merge=lfs -text
*.rar filter=lfs diff=lfs merge=lfs -text
*.safetensors filter=lfs diff=lfs merge=lfs -text
saved_model/**/* filter=lfs diff=lfs merge=lfs -text
*.tar.* filter=lfs diff=lfs merge=lfs -text
*.tar filter=lfs diff=lfs merge=lfs -text
*.tflite filter=lfs diff=lfs merge=lfs -text
*.tgz filter=lfs diff=lfs merge=lfs -text
*.wasm filter=lfs diff=lfs merge=lfs -text
*.xz filter=lfs diff=lfs merge=lfs -text
*.zip filter=lfs diff=lfs merge=lfs -text
*.zst filter=lfs diff=lfs merge=lfs -text
*tfevents* filter=lfs diff=lfs merge=lfs -text
</file>

<file path="s3tokenizer-v2-model/config.json">
{
  "architectures": [
    "S3TokenizerModel"
  ],
  "dtype": "float32",
  "hidden_size": 1280,
  "hop_length": 160,
  "model_type": "s3tokenizer",
  "n_audio_ctx": 1500,
  "n_audio_head": 20,
  "n_audio_layer": 6,
  "n_audio_state": 1280,
  "n_fft": 400,
  "n_mels": 128,
  "sampling_rate": 16000,
  "token_rate": 25,
  "transformers_version": "5.0.0.dev0",
  "use_sdpa": false,
  "vocab_size": 6561
}
</file>

<file path="s3tokenizer-v2-model/README.md">
---
license: apache-2.0
---
</file>

<file path="src/chatterbox/models/s3gen/__init__.py">
from .s3gen import S3Token2Wav as S3Gen
from .const import S3GEN_SR
</file>

<file path="src/chatterbox/models/s3gen/configs.py">
from ..utils import AttrDict

CFM_PARAMS = AttrDict({
    "sigma_min": 1e-06,
    "solver": "euler",
    "t_scheduler": "cosine",
    "training_cfg_rate": 0.2,
    "inference_cfg_rate": 0.7,
    "reg_loss_type": "l1"
})
</file>

<file path="src/chatterbox/models/s3gen/f0_predictor.py">
# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Kai Hu)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import torch
import torch.nn as nn
from torch.nn.utils.parametrizations import weight_norm


class ConvRNNF0Predictor(nn.Module):
    def __init__(self,
                 num_class: int = 1,
                 in_channels: int = 80,
                 cond_channels: int = 512
                 ):
        super().__init__()

        self.num_class = num_class
        self.condnet = nn.Sequential(
            weight_norm(
                nn.Conv1d(in_channels, cond_channels, kernel_size=3, padding=1)
            ),
            nn.ELU(),
            weight_norm(
                nn.Conv1d(cond_channels, cond_channels, kernel_size=3, padding=1)
            ),
            nn.ELU(),
            weight_norm(
                nn.Conv1d(cond_channels, cond_channels, kernel_size=3, padding=1)
            ),
            nn.ELU(),
            weight_norm(
                nn.Conv1d(cond_channels, cond_channels, kernel_size=3, padding=1)
            ),
            nn.ELU(),
            weight_norm(
                nn.Conv1d(cond_channels, cond_channels, kernel_size=3, padding=1)
            ),
            nn.ELU(),
        )
        self.classifier = nn.Linear(in_features=cond_channels, out_features=self.num_class)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.condnet(x)
        x = x.transpose(1, 2)
        return torch.abs(self.classifier(x).squeeze(-1))
</file>

<file path="src/chatterbox/models/s3gen/hifigan.py">
# jrm: adapted from CosyVoice/cosyvoice/hifigan/generator.py
#      most modules should be reusable, but I found their SineGen changed a git.

# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Kai Hu)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""HIFI-GAN"""

from typing import Dict, Optional, List
import numpy as np
from scipy.signal import get_window
import torch
import torch.nn.functional as F
from torch.nn import Conv1d
from torch.nn import ConvTranspose1d
from torch.nn.utils import remove_weight_norm
from torch.nn.utils.parametrizations import weight_norm
from torch.distributions.uniform import Uniform
from torch import nn, sin, pow
from torch.nn import Parameter


class Snake(nn.Module):
    '''
    Implementation of a sine-based periodic activation function
    Shape:
        - Input: (B, C, T)
        - Output: (B, C, T), same shape as the input
    Parameters:
        - alpha - trainable parameter
    References:
        - This activation function is from this paper by Liu Ziyin, Tilman Hartwig, Masahito Ueda:
        https://arxiv.org/abs/2006.08195
    Examples:
        >>> a1 = snake(256)
        >>> x = torch.randn(256)
        >>> x = a1(x)
    '''
    def __init__(self, in_features, alpha=1.0, alpha_trainable=True, alpha_logscale=False):
        '''
        Initialization.
        INPUT:
            - in_features: shape of the input
            - alpha: trainable parameter
            alpha is initialized to 1 by default, higher values = higher-frequency.
            alpha will be trained along with the rest of your model.
        '''
        super(Snake, self).__init__()
        self.in_features = in_features

        # initialize alpha
        self.alpha_logscale = alpha_logscale
        if self.alpha_logscale: # log scale alphas initialized to zeros
            self.alpha = Parameter(torch.zeros(in_features) * alpha)
        else: # linear scale alphas initialized to ones
            self.alpha = Parameter(torch.ones(in_features) * alpha)

        self.alpha.requires_grad = alpha_trainable

        self.no_div_by_zero = 0.000000001

    def forward(self, x):
        '''
        Forward pass of the function.
        Applies the function to the input elementwise.
        Snake ∶= x + 1/a * sin^2 (xa)
        '''
        alpha = self.alpha.unsqueeze(0).unsqueeze(-1) # line up with x to [B, C, T]
        if self.alpha_logscale:
            alpha = torch.exp(alpha)
        x = x + (1.0 / (alpha + self.no_div_by_zero)) * pow(sin(x * alpha), 2)

        return x



def get_padding(kernel_size, dilation=1):
    return int((kernel_size * dilation - dilation) / 2)

def init_weights(m, mean=0.0, std=0.01):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        m.weight.data.normal_(mean, std)


"""hifigan based generator implementation.

This code is modified from https://github.com/jik876/hifi-gan
 ,https://github.com/kan-bayashi/ParallelWaveGAN and
 https://github.com/NVIDIA/BigVGAN

"""


class ResBlock(torch.nn.Module):
    """Residual block module in HiFiGAN/BigVGAN."""
    def __init__(
        self,
        channels: int = 512,
        kernel_size: int = 3,
        dilations: List[int] = [1, 3, 5],
    ):
        super(ResBlock, self).__init__()
        self.convs1 = nn.ModuleList()
        self.convs2 = nn.ModuleList()

        for dilation in dilations:
            self.convs1.append(
                weight_norm(
                    Conv1d(
                        channels,
                        channels,
                        kernel_size,
                        1,
                        dilation=dilation,
                        padding=get_padding(kernel_size, dilation)
                    )
                )
            )
            self.convs2.append(
                weight_norm(
                    Conv1d(
                        channels,
                        channels,
                        kernel_size,
                        1,
                        dilation=1,
                        padding=get_padding(kernel_size, 1)
                    )
                )
            )
        self.convs1.apply(init_weights)
        self.convs2.apply(init_weights)
        self.activations1 = nn.ModuleList([
            Snake(channels, alpha_logscale=False)
            for _ in range(len(self.convs1))
        ])
        self.activations2 = nn.ModuleList([
            Snake(channels, alpha_logscale=False)
            for _ in range(len(self.convs2))
        ])

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for idx in range(len(self.convs1)):
            xt = self.activations1[idx](x)
            xt = self.convs1[idx](xt)
            xt = self.activations2[idx](xt)
            xt = self.convs2[idx](xt)
            x = xt + x
        return x

    def remove_weight_norm(self):
        for idx in range(len(self.convs1)):
            remove_weight_norm(self.convs1[idx])
            remove_weight_norm(self.convs2[idx])


class SineGen(torch.nn.Module):
    """ Definition of sine generator
    SineGen(samp_rate, harmonic_num = 0,
            sine_amp = 0.1, noise_std = 0.003,
            voiced_threshold = 0,
            flag_for_pulse=False)
    samp_rate: sampling rate in Hz
    harmonic_num: number of harmonic overtones (default 0)
    sine_amp: amplitude of sine-wavefrom (default 0.1)
    noise_std: std of Gaussian noise (default 0.003)
    voiced_thoreshold: F0 threshold for U/V classification (default 0)
    flag_for_pulse: this SinGen is used inside PulseGen (default False)
    Note: when flag_for_pulse is True, the first time step of a voiced
        segment is always sin(np.pi) or cos(0)
    """

    def __init__(self, samp_rate, harmonic_num=0,
                 sine_amp=0.1, noise_std=0.003,
                 voiced_threshold=0):
        super(SineGen, self).__init__()
        self.sine_amp = sine_amp
        self.noise_std = noise_std
        self.harmonic_num = harmonic_num
        self.sampling_rate = samp_rate
        self.voiced_threshold = voiced_threshold

    def _f02uv(self, f0):
        # generate uv signal
        uv = (f0 > self.voiced_threshold).type(torch.float32)
        return uv

    @torch.no_grad()
    def forward(self, f0):
        """
        :param f0: [B, 1, sample_len], Hz
        :return: [B, 1, sample_len]
        """

        F_mat = torch.zeros((f0.size(0), self.harmonic_num + 1, f0.size(-1))).to(f0.device)
        for i in range(self.harmonic_num + 1):
            F_mat[:, i: i + 1, :] = f0 * (i + 1) / self.sampling_rate

        theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)
        u_dist = Uniform(low=-np.pi, high=np.pi)
        phase_vec = u_dist.sample(sample_shape=(f0.size(0), self.harmonic_num + 1, 1)).to(F_mat.device)
        phase_vec[:, 0, :] = 0

        # generate sine waveforms
        sine_waves = self.sine_amp * torch.sin(theta_mat + phase_vec)

        # generate uv signal
        uv = self._f02uv(f0)

        # noise: for unvoiced should be similar to sine_amp
        #        std = self.sine_amp/3 -> max value ~ self.sine_amp
        # .       for voiced regions is self.noise_std
        noise_amp = uv * self.noise_std + (1 - uv) * self.sine_amp / 3
        noise = noise_amp * torch.randn_like(sine_waves)

        # first: set the unvoiced part to 0 by uv
        # then: additive noise
        sine_waves = sine_waves * uv + noise
        return sine_waves, uv, noise


class SourceModuleHnNSF(torch.nn.Module):
    """ SourceModule for hn-nsf
    SourceModule(sampling_rate, harmonic_num=0, sine_amp=0.1,
                 add_noise_std=0.003, voiced_threshod=0)
    sampling_rate: sampling_rate in Hz
    harmonic_num: number of harmonic above F0 (default: 0)
    sine_amp: amplitude of sine source signal (default: 0.1)
    add_noise_std: std of additive Gaussian noise (default: 0.003)
        note that amplitude of noise in unvoiced is decided
        by sine_amp
    voiced_threshold: threhold to set U/V given F0 (default: 0)
    Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)
    F0_sampled (batchsize, length, 1)
    Sine_source (batchsize, length, 1)
    noise_source (batchsize, length 1)
    uv (batchsize, length, 1)
    """

    def __init__(self, sampling_rate, upsample_scale, harmonic_num=0, sine_amp=0.1,
                 add_noise_std=0.003, voiced_threshod=0):
        super(SourceModuleHnNSF, self).__init__()

        self.sine_amp = sine_amp
        self.noise_std = add_noise_std

        # to produce sine waveforms
        self.l_sin_gen = SineGen(sampling_rate, harmonic_num,
                                 sine_amp, add_noise_std, voiced_threshod)

        # to merge source harmonics into a single excitation
        self.l_linear = torch.nn.Linear(harmonic_num + 1, 1)
        self.l_tanh = torch.nn.Tanh()

    def forward(self, x):
        """
        Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)
        F0_sampled (batchsize, length, 1)
        Sine_source (batchsize, length, 1)
        noise_source (batchsize, length 1)
        """
        # source for harmonic branch
        with torch.no_grad():
            sine_wavs, uv, _ = self.l_sin_gen(x.transpose(1, 2))
            sine_wavs = sine_wavs.transpose(1, 2)
            uv = uv.transpose(1, 2)
        sine_merge = self.l_tanh(self.l_linear(sine_wavs))

        # source for noise branch, in the same shape as uv
        noise = torch.randn_like(uv) * self.sine_amp / 3
        return sine_merge, noise, uv


class HiFTGenerator(nn.Module):
    """
    HiFTNet Generator: Neural Source Filter + ISTFTNet
    https://arxiv.org/abs/2309.09493
    """
    def __init__(
            self,
            in_channels: int = 80,
            base_channels: int = 512,
            nb_harmonics: int = 8,
            sampling_rate: int = 22050,
            nsf_alpha: float = 0.1,
            nsf_sigma: float = 0.003,
            nsf_voiced_threshold: float = 10,
            upsample_rates: List[int] = [8, 8],
            upsample_kernel_sizes: List[int] = [16, 16],
            istft_params: Dict[str, int] = {"n_fft": 16, "hop_len": 4},
            resblock_kernel_sizes: List[int] = [3, 7, 11],
            resblock_dilation_sizes: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]],
            source_resblock_kernel_sizes: List[int] = [7, 11],
            source_resblock_dilation_sizes: List[List[int]] = [[1, 3, 5], [1, 3, 5]],
            lrelu_slope: float = 0.1,
            audio_limit: float = 0.99,
            f0_predictor: torch.nn.Module = None,
    ):
        super(HiFTGenerator, self).__init__()

        self.out_channels = 1
        self.nb_harmonics = nb_harmonics
        self.sampling_rate = sampling_rate
        self.istft_params = istft_params
        self.lrelu_slope = lrelu_slope
        self.audio_limit = audio_limit

        self.num_kernels = len(resblock_kernel_sizes)
        self.num_upsamples = len(upsample_rates)
        self.m_source = SourceModuleHnNSF(
            sampling_rate=sampling_rate,
            upsample_scale=np.prod(upsample_rates) * istft_params["hop_len"],
            harmonic_num=nb_harmonics,
            sine_amp=nsf_alpha,
            add_noise_std=nsf_sigma,
            voiced_threshod=nsf_voiced_threshold)
        self.f0_upsamp = torch.nn.Upsample(scale_factor=np.prod(upsample_rates) * istft_params["hop_len"])

        self.conv_pre = weight_norm(
            Conv1d(in_channels, base_channels, 7, 1, padding=3)
        )

        # Up
        self.ups = nn.ModuleList()
        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):
            self.ups.append(
                weight_norm(
                    ConvTranspose1d(
                        base_channels // (2**i),
                        base_channels // (2**(i + 1)),
                        k,
                        u,
                        padding=(k - u) // 2,
                    )
                )
            )

        # Down
        self.source_downs = nn.ModuleList()
        self.source_resblocks = nn.ModuleList()
        downsample_rates = [1] + upsample_rates[::-1][:-1]
        downsample_cum_rates = np.cumprod(downsample_rates)
        for i, (u, k, d) in enumerate(zip(downsample_cum_rates[::-1], source_resblock_kernel_sizes, source_resblock_dilation_sizes)):
            if u == 1:
                self.source_downs.append(
                    Conv1d(istft_params["n_fft"] + 2, base_channels // (2 ** (i + 1)), 1, 1)
                )
            else:
                self.source_downs.append(
                    Conv1d(istft_params["n_fft"] + 2, base_channels // (2 ** (i + 1)), u * 2, u, padding=(u // 2))
                )

            self.source_resblocks.append(
                ResBlock(base_channels // (2 ** (i + 1)), k, d)
            )

        self.resblocks = nn.ModuleList()
        for i in range(len(self.ups)):
            ch = base_channels // (2**(i + 1))
            for _, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):
                self.resblocks.append(ResBlock(ch, k, d))

        self.conv_post = weight_norm(Conv1d(ch, istft_params["n_fft"] + 2, 7, 1, padding=3))
        self.ups.apply(init_weights)
        self.conv_post.apply(init_weights)
        self.reflection_pad = nn.ReflectionPad1d((1, 0))
        self.stft_window = torch.from_numpy(get_window("hann", istft_params["n_fft"], fftbins=True).astype(np.float32))
        self.f0_predictor = f0_predictor

    def remove_weight_norm(self):
        print('Removing weight norm...')
        for l in self.ups:
            remove_weight_norm(l)
        for l in self.resblocks:
            l.remove_weight_norm()
        remove_weight_norm(self.conv_pre)
        remove_weight_norm(self.conv_post)
        self.m_source.remove_weight_norm()
        for l in self.source_downs:
            remove_weight_norm(l)
        for l in self.source_resblocks:
            l.remove_weight_norm()

    def _stft(self, x):
        spec = torch.stft(
            x,
            self.istft_params["n_fft"], self.istft_params["hop_len"], self.istft_params["n_fft"], window=self.stft_window.to(x.device),
            return_complex=True)
        spec = torch.view_as_real(spec)  # [B, F, TT, 2]
        return spec[..., 0], spec[..., 1]

    def _istft(self, magnitude, phase):
        magnitude = torch.clip(magnitude, max=1e2)
        real = magnitude * torch.cos(phase)
        img = magnitude * torch.sin(phase)
        inverse_transform = torch.istft(torch.complex(real, img), self.istft_params["n_fft"], self.istft_params["hop_len"],
                                        self.istft_params["n_fft"], window=self.stft_window.to(magnitude.device))
        return inverse_transform

    def decode(self, x: torch.Tensor, s: torch.Tensor = torch.zeros(1, 1, 0)) -> torch.Tensor:
        s_stft_real, s_stft_imag = self._stft(s.squeeze(1))
        s_stft = torch.cat([s_stft_real, s_stft_imag], dim=1)

        x = self.conv_pre(x)
        for i in range(self.num_upsamples):
            x = F.leaky_relu(x, self.lrelu_slope)
            x = self.ups[i](x)

            if i == self.num_upsamples - 1:
                x = self.reflection_pad(x)

            # fusion
            si = self.source_downs[i](s_stft)
            si = self.source_resblocks[i](si)
            x = x + si

            xs = None
            for j in range(self.num_kernels):
                if xs is None:
                    xs = self.resblocks[i * self.num_kernels + j](x)
                else:
                    xs += self.resblocks[i * self.num_kernels + j](x)
            x = xs / self.num_kernels

        x = F.leaky_relu(x)
        x = self.conv_post(x)
        magnitude = torch.exp(x[:, :self.istft_params["n_fft"] // 2 + 1, :])
        phase = torch.sin(x[:, self.istft_params["n_fft"] // 2 + 1:, :])  # actually, sin is redundancy

        x = self._istft(magnitude, phase)
        x = torch.clamp(x, -self.audio_limit, self.audio_limit)
        return x

    def forward(
            self,
            batch: dict,
            device: torch.device,
    ) -> Dict[str, Optional[torch.Tensor]]:
        speech_feat = batch['speech_feat'].transpose(1, 2).to(device)
        # mel->f0
        f0 = self.f0_predictor(speech_feat)
        # f0->source
        s = self.f0_upsamp(f0[:, None]).transpose(1, 2)  # bs,n,t
        s, _, _ = self.m_source(s)
        s = s.transpose(1, 2)
        # mel+source->speech
        generated_speech = self.decode(x=speech_feat, s=s)
        return generated_speech, f0

    @torch.inference_mode()
    def inference(self, speech_feat: torch.Tensor, cache_source: torch.Tensor = torch.zeros(1, 1, 0)) -> torch.Tensor:
        # mel->f0
        f0 = self.f0_predictor(speech_feat)
        # f0->source
        s = self.f0_upsamp(f0[:, None]).transpose(1, 2)  # bs,n,t
        s, _, _ = self.m_source(s)
        s = s.transpose(1, 2)
        # use cache_source to avoid glitch
        if cache_source.shape[2] != 0:
            s[:, :, :cache_source.shape[2]] = cache_source
        generated_speech = self.decode(x=speech_feat, s=s)
        return generated_speech, s
</file>

<file path="src/chatterbox/models/s3gen/matcha/decoder.py">
import math
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from conformer import ConformerBlock
from diffusers.models.activations import get_activation
from einops import pack, rearrange, repeat

from .transformer import BasicTransformerBlock


class SinusoidalPosEmb(torch.nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        assert self.dim % 2 == 0, "SinusoidalPosEmb requires dim to be even"

    def forward(self, x, scale=1000):
        if x.ndim < 1:
            x = x.unsqueeze(0)
        device = x.device
        half_dim = self.dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=device).float() * -emb)
        emb = scale * x.unsqueeze(1) * emb.unsqueeze(0)
        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
        return emb


class Block1D(torch.nn.Module):
    def __init__(self, dim, dim_out, groups=8):
        super().__init__()
        self.block = torch.nn.Sequential(
            torch.nn.Conv1d(dim, dim_out, 3, padding=1),
            torch.nn.GroupNorm(groups, dim_out),
            nn.Mish(),
        )

    def forward(self, x, mask):
        output = self.block(x * mask)
        return output * mask


class ResnetBlock1D(torch.nn.Module):
    def __init__(self, dim, dim_out, time_emb_dim, groups=8):
        super().__init__()
        self.mlp = torch.nn.Sequential(nn.Mish(), torch.nn.Linear(time_emb_dim, dim_out))

        self.block1 = Block1D(dim, dim_out, groups=groups)
        self.block2 = Block1D(dim_out, dim_out, groups=groups)

        self.res_conv = torch.nn.Conv1d(dim, dim_out, 1)

    def forward(self, x, mask, time_emb):
        h = self.block1(x, mask)
        h += self.mlp(time_emb).unsqueeze(-1)
        h = self.block2(h, mask)
        output = h + self.res_conv(x * mask)
        return output


class Downsample1D(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.conv = torch.nn.Conv1d(dim, dim, 3, 2, 1)

    def forward(self, x):
        return self.conv(x)


class TimestepEmbedding(nn.Module):
    def __init__(
        self,
        in_channels: int,
        time_embed_dim: int,
        act_fn: str = "silu",
        out_dim: int = None,
        post_act_fn: Optional[str] = None,
        cond_proj_dim=None,
    ):
        super().__init__()

        self.linear_1 = nn.Linear(in_channels, time_embed_dim)

        if cond_proj_dim is not None:
            self.cond_proj = nn.Linear(cond_proj_dim, in_channels, bias=False)
        else:
            self.cond_proj = None

        self.act = get_activation(act_fn)

        if out_dim is not None:
            time_embed_dim_out = out_dim
        else:
            time_embed_dim_out = time_embed_dim
        self.linear_2 = nn.Linear(time_embed_dim, time_embed_dim_out)

        if post_act_fn is None:
            self.post_act = None
        else:
            self.post_act = get_activation(post_act_fn)

    def forward(self, sample, condition=None):
        if condition is not None:
            sample = sample + self.cond_proj(condition)
        sample = self.linear_1(sample)

        if self.act is not None:
            sample = self.act(sample)

        sample = self.linear_2(sample)

        if self.post_act is not None:
            sample = self.post_act(sample)
        return sample


class Upsample1D(nn.Module):
    """A 1D upsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        use_conv_transpose (`bool`, default `False`):
            option to use a convolution transpose.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
    """

    def __init__(self, channels, use_conv=False, use_conv_transpose=True, out_channels=None, name="conv"):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels or channels
        self.use_conv = use_conv
        self.use_conv_transpose = use_conv_transpose
        self.name = name

        self.conv = None
        if use_conv_transpose:
            self.conv = nn.ConvTranspose1d(channels, self.out_channels, 4, 2, 1)
        elif use_conv:
            self.conv = nn.Conv1d(self.channels, self.out_channels, 3, padding=1)

    def forward(self, inputs):
        assert inputs.shape[1] == self.channels
        if self.use_conv_transpose:
            return self.conv(inputs)

        outputs = F.interpolate(inputs, scale_factor=2.0, mode="nearest")

        if self.use_conv:
            outputs = self.conv(outputs)

        return outputs


class ConformerWrapper(ConformerBlock):
    def __init__(  # pylint: disable=useless-super-delegation
        self,
        *,
        dim,
        dim_head=64,
        heads=8,
        ff_mult=4,
        conv_expansion_factor=2,
        conv_kernel_size=31,
        attn_dropout=0,
        ff_dropout=0,
        conv_dropout=0,
        conv_causal=False,
    ):
        super().__init__(
            dim=dim,
            dim_head=dim_head,
            heads=heads,
            ff_mult=ff_mult,
            conv_expansion_factor=conv_expansion_factor,
            conv_kernel_size=conv_kernel_size,
            attn_dropout=attn_dropout,
            ff_dropout=ff_dropout,
            conv_dropout=conv_dropout,
            conv_causal=conv_causal,
        )

    def forward(
        self,
        hidden_states,
        attention_mask,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        timestep=None,
    ):
        return super().forward(x=hidden_states, mask=attention_mask.bool())


class Decoder(nn.Module):
    def __init__(
        self,
        in_channels,
        out_channels,
        channels=(256, 256),
        dropout=0.05,
        attention_head_dim=64,
        n_blocks=1,
        num_mid_blocks=2,
        num_heads=4,
        act_fn="snake",
        down_block_type="transformer",
        mid_block_type="transformer",
        up_block_type="transformer",
    ):
        super().__init__()
        channels = tuple(channels)
        self.in_channels = in_channels
        self.out_channels = out_channels

        self.time_embeddings = SinusoidalPosEmb(in_channels)
        time_embed_dim = channels[0] * 4
        self.time_mlp = TimestepEmbedding(
            in_channels=in_channels,
            time_embed_dim=time_embed_dim,
            act_fn="silu",
        )

        self.down_blocks = nn.ModuleList([])
        self.mid_blocks = nn.ModuleList([])
        self.up_blocks = nn.ModuleList([])

        output_channel = in_channels
        for i in range(len(channels)):  # pylint: disable=consider-using-enumerate
            input_channel = output_channel
            output_channel = channels[i]
            is_last = i == len(channels) - 1
            resnet = ResnetBlock1D(dim=input_channel, dim_out=output_channel, time_emb_dim=time_embed_dim)
            transformer_blocks = nn.ModuleList(
                [
                    self.get_block(
                        down_block_type,
                        output_channel,
                        attention_head_dim,
                        num_heads,
                        dropout,
                        act_fn,
                    )
                    for _ in range(n_blocks)
                ]
            )
            downsample = (
                Downsample1D(output_channel) if not is_last else nn.Conv1d(output_channel, output_channel, 3, padding=1)
            )

            self.down_blocks.append(nn.ModuleList([resnet, transformer_blocks, downsample]))

        for i in range(num_mid_blocks):
            input_channel = channels[-1]
            out_channels = channels[-1]

            resnet = ResnetBlock1D(dim=input_channel, dim_out=output_channel, time_emb_dim=time_embed_dim)

            transformer_blocks = nn.ModuleList(
                [
                    self.get_block(
                        mid_block_type,
                        output_channel,
                        attention_head_dim,
                        num_heads,
                        dropout,
                        act_fn,
                    )
                    for _ in range(n_blocks)
                ]
            )

            self.mid_blocks.append(nn.ModuleList([resnet, transformer_blocks]))

        channels = channels[::-1] + (channels[0],)
        for i in range(len(channels) - 1):
            input_channel = channels[i]
            output_channel = channels[i + 1]
            is_last = i == len(channels) - 2

            resnet = ResnetBlock1D(
                dim=2 * input_channel,
                dim_out=output_channel,
                time_emb_dim=time_embed_dim,
            )
            transformer_blocks = nn.ModuleList(
                [
                    self.get_block(
                        up_block_type,
                        output_channel,
                        attention_head_dim,
                        num_heads,
                        dropout,
                        act_fn,
                    )
                    for _ in range(n_blocks)
                ]
            )
            upsample = (
                Upsample1D(output_channel, use_conv_transpose=True)
                if not is_last
                else nn.Conv1d(output_channel, output_channel, 3, padding=1)
            )

            self.up_blocks.append(nn.ModuleList([resnet, transformer_blocks, upsample]))

        self.final_block = Block1D(channels[-1], channels[-1])
        self.final_proj = nn.Conv1d(channels[-1], self.out_channels, 1)

        self.initialize_weights()
        # nn.init.normal_(self.final_proj.weight)

    @staticmethod
    def get_block(block_type, dim, attention_head_dim, num_heads, dropout, act_fn):
        if block_type == "conformer":
            block = ConformerWrapper(
                dim=dim,
                dim_head=attention_head_dim,
                heads=num_heads,
                ff_mult=1,
                conv_expansion_factor=2,
                ff_dropout=dropout,
                attn_dropout=dropout,
                conv_dropout=dropout,
                conv_kernel_size=31,
            )
        elif block_type == "transformer":
            block = BasicTransformerBlock(
                dim=dim,
                num_attention_heads=num_heads,
                attention_head_dim=attention_head_dim,
                dropout=dropout,
                activation_fn=act_fn,
            )
        else:
            raise ValueError(f"Unknown block type {block_type}")

        return block

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, nonlinearity="relu")

                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

            elif isinstance(m, nn.GroupNorm):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, nonlinearity="relu")

                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x, mask, mu, t, spks=None, cond=None):
        """Forward pass of the UNet1DConditional model.

        Args:
            x (torch.Tensor): shape (batch_size, in_channels, time)
            mask (_type_): shape (batch_size, 1, time)
            t (_type_): shape (batch_size)
            spks (_type_, optional): shape: (batch_size, condition_channels). Defaults to None.
            cond (_type_, optional): placeholder for future use. Defaults to None.

        Raises:
            ValueError: _description_
            ValueError: _description_

        Returns:
            _type_: _description_
        """

        t = self.time_embeddings(t)
        t = self.time_mlp(t)

        x = pack([x, mu], "b * t")[0]

        if spks is not None:
            spks = repeat(spks, "b c -> b c t", t=x.shape[-1])
            x = pack([x, spks], "b * t")[0]

        hiddens = []
        masks = [mask]
        for resnet, transformer_blocks, downsample in self.down_blocks:
            mask_down = masks[-1]
            x = resnet(x, mask_down, t)
            x = rearrange(x, "b c t -> b t c")
            mask_down = rearrange(mask_down, "b 1 t -> b t")
            for transformer_block in transformer_blocks:
                x = transformer_block(
                    hidden_states=x,
                    attention_mask=mask_down,
                    timestep=t,
                )
            x = rearrange(x, "b t c -> b c t")
            mask_down = rearrange(mask_down, "b t -> b 1 t")
            hiddens.append(x)  # Save hidden states for skip connections
            x = downsample(x * mask_down)
            masks.append(mask_down[:, :, ::2])

        masks = masks[:-1]
        mask_mid = masks[-1]

        for resnet, transformer_blocks in self.mid_blocks:
            x = resnet(x, mask_mid, t)
            x = rearrange(x, "b c t -> b t c")
            mask_mid = rearrange(mask_mid, "b 1 t -> b t")
            for transformer_block in transformer_blocks:
                x = transformer_block(
                    hidden_states=x,
                    attention_mask=mask_mid,
                    timestep=t,
                )
            x = rearrange(x, "b t c -> b c t")
            mask_mid = rearrange(mask_mid, "b t -> b 1 t")

        for resnet, transformer_blocks, upsample in self.up_blocks:
            mask_up = masks.pop()
            x = resnet(pack([x, hiddens.pop()], "b * t")[0], mask_up, t)
            x = rearrange(x, "b c t -> b t c")
            mask_up = rearrange(mask_up, "b 1 t -> b t")
            for transformer_block in transformer_blocks:
                x = transformer_block(
                    hidden_states=x,
                    attention_mask=mask_up,
                    timestep=t,
                )
            x = rearrange(x, "b t c -> b c t")
            mask_up = rearrange(mask_up, "b t -> b 1 t")
            x = upsample(x * mask_up)

        x = self.final_block(x, mask_up)
        output = self.final_proj(x * mask_up)

        return output * mask
</file>

<file path="src/chatterbox/models/s3gen/matcha/flow_matching.py">
from abc import ABC

import torch
import torch.nn.functional as F

from .decoder import Decoder


class BASECFM(torch.nn.Module, ABC):
    def __init__(
        self,
        n_feats,
        cfm_params,
        n_spks=1,
        spk_emb_dim=128,
    ):
        super().__init__()
        self.n_feats = n_feats
        self.n_spks = n_spks
        self.spk_emb_dim = spk_emb_dim
        self.solver = cfm_params.solver
        if hasattr(cfm_params, "sigma_min"):
            self.sigma_min = cfm_params.sigma_min
        else:
            self.sigma_min = 1e-4

        self.estimator = None

    @torch.inference_mode()
    def forward(self, mu, mask, n_timesteps, temperature=1.0, spks=None, cond=None):
        """Forward diffusion

        Args:
            mu (torch.Tensor): output of encoder
                shape: (batch_size, n_feats, mel_timesteps)
            mask (torch.Tensor): output_mask
                shape: (batch_size, 1, mel_timesteps)
            n_timesteps (int): number of diffusion steps
            temperature (float, optional): temperature for scaling noise. Defaults to 1.0.
            spks (torch.Tensor, optional): speaker ids. Defaults to None.
                shape: (batch_size, spk_emb_dim)
            cond: Not used but kept for future purposes

        Returns:
            sample: generated mel-spectrogram
                shape: (batch_size, n_feats, mel_timesteps)
        """
        z = torch.randn_like(mu) * temperature
        t_span = torch.linspace(0, 1, n_timesteps + 1, device=mu.device)
        return self.solve_euler(z, t_span=t_span, mu=mu, mask=mask, spks=spks, cond=cond)

    def solve_euler(self, x, t_span, mu, mask, spks, cond):
        """
        Fixed euler solver for ODEs.
        Args:
            x (torch.Tensor): random noise
            t_span (torch.Tensor): n_timesteps interpolated
                shape: (n_timesteps + 1,)
            mu (torch.Tensor): output of encoder
                shape: (batch_size, n_feats, mel_timesteps)
            mask (torch.Tensor): output_mask
                shape: (batch_size, 1, mel_timesteps)
            spks (torch.Tensor, optional): speaker ids. Defaults to None.
                shape: (batch_size, spk_emb_dim)
            cond: Not used but kept for future purposes
        """
        t, _, dt = t_span[0], t_span[-1], t_span[1] - t_span[0]

        # I am storing this because I can later plot it by putting a debugger here and saving it to a file
        # Or in future might add like a return_all_steps flag
        sol = []

        for step in range(1, len(t_span)):
            dphi_dt = self.estimator(x, mask, mu, t, spks, cond)

            x = x + dt * dphi_dt
            t = t + dt
            sol.append(x)
            if step < len(t_span) - 1:
                dt = t_span[step + 1] - t

        return sol[-1]

    def compute_loss(self, x1, mask, mu, spks=None, cond=None):
        """Computes diffusion loss

        Args:
            x1 (torch.Tensor): Target
                shape: (batch_size, n_feats, mel_timesteps)
            mask (torch.Tensor): target mask
                shape: (batch_size, 1, mel_timesteps)
            mu (torch.Tensor): output of encoder
                shape: (batch_size, n_feats, mel_timesteps)
            spks (torch.Tensor, optional): speaker embedding. Defaults to None.
                shape: (batch_size, spk_emb_dim)

        Returns:
            loss: conditional flow matching loss
            y: conditional flow
                shape: (batch_size, n_feats, mel_timesteps)
        """
        b, _, t = mu.shape

        # random timestep
        t = torch.rand([b, 1, 1], device=mu.device, dtype=mu.dtype)
        # sample noise p(x_0)
        z = torch.randn_like(x1)

        y = (1 - (1 - self.sigma_min) * t) * z + t * x1
        u = x1 - (1 - self.sigma_min) * z

        loss = F.mse_loss(self.estimator(y, mask, mu, t.squeeze(), spks), u, reduction="sum") / (
            torch.sum(mask) * u.shape[1]
        )
        return loss, y


class CFM(BASECFM):
    def __init__(self, in_channels, out_channel, cfm_params, decoder_params, n_spks=1, spk_emb_dim=64):
        super().__init__(
            n_feats=in_channels,
            cfm_params=cfm_params,
            n_spks=n_spks,
            spk_emb_dim=spk_emb_dim,
        )

        in_channels = in_channels + (spk_emb_dim if n_spks > 1 else 0)
        # Just change the architecture of the estimator here
        self.estimator = Decoder(in_channels=in_channels, out_channels=out_channel, **decoder_params)
</file>

<file path="src/chatterbox/models/s3gen/matcha/text_encoder.py">
""" from https://github.com/jaywalnut310/glow-tts """

import math

import torch
import torch.nn as nn
from einops import rearrange


def sequence_mask(length, max_length=None):
    if max_length is None:
        max_length = length.max()
    x = torch.arange(max_length, dtype=length.dtype, device=length.device)
    return x.unsqueeze(0) < length.unsqueeze(1)



class LayerNorm(nn.Module):
    def __init__(self, channels, eps=1e-4):
        super().__init__()
        self.channels = channels
        self.eps = eps

        self.gamma = torch.nn.Parameter(torch.ones(channels))
        self.beta = torch.nn.Parameter(torch.zeros(channels))

    def forward(self, x):
        n_dims = len(x.shape)
        mean = torch.mean(x, 1, keepdim=True)
        variance = torch.mean((x - mean) ** 2, 1, keepdim=True)

        x = (x - mean) * torch.rsqrt(variance + self.eps)

        shape = [1, -1] + [1] * (n_dims - 2)
        x = x * self.gamma.view(*shape) + self.beta.view(*shape)
        return x


class ConvReluNorm(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, kernel_size, n_layers, p_dropout):
        super().__init__()
        self.in_channels = in_channels
        self.hidden_channels = hidden_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.n_layers = n_layers
        self.p_dropout = p_dropout

        self.conv_layers = torch.nn.ModuleList()
        self.norm_layers = torch.nn.ModuleList()
        self.conv_layers.append(torch.nn.Conv1d(in_channels, hidden_channels, kernel_size, padding=kernel_size // 2))
        self.norm_layers.append(LayerNorm(hidden_channels))
        self.relu_drop = torch.nn.Sequential(torch.nn.ReLU(), torch.nn.Dropout(p_dropout))
        for _ in range(n_layers - 1):
            self.conv_layers.append(
                torch.nn.Conv1d(hidden_channels, hidden_channels, kernel_size, padding=kernel_size // 2)
            )
            self.norm_layers.append(LayerNorm(hidden_channels))
        self.proj = torch.nn.Conv1d(hidden_channels, out_channels, 1)
        self.proj.weight.data.zero_()
        self.proj.bias.data.zero_()

    def forward(self, x, x_mask):
        x_org = x
        for i in range(self.n_layers):
            x = self.conv_layers[i](x * x_mask)
            x = self.norm_layers[i](x)
            x = self.relu_drop(x)
        x = x_org + self.proj(x)
        return x * x_mask


class DurationPredictor(nn.Module):
    def __init__(self, in_channels, filter_channels, kernel_size, p_dropout):
        super().__init__()
        self.in_channels = in_channels
        self.filter_channels = filter_channels
        self.p_dropout = p_dropout

        self.drop = torch.nn.Dropout(p_dropout)
        self.conv_1 = torch.nn.Conv1d(in_channels, filter_channels, kernel_size, padding=kernel_size // 2)
        self.norm_1 = LayerNorm(filter_channels)
        self.conv_2 = torch.nn.Conv1d(filter_channels, filter_channels, kernel_size, padding=kernel_size // 2)
        self.norm_2 = LayerNorm(filter_channels)
        self.proj = torch.nn.Conv1d(filter_channels, 1, 1)

    def forward(self, x, x_mask):
        x = self.conv_1(x * x_mask)
        x = torch.relu(x)
        x = self.norm_1(x)
        x = self.drop(x)
        x = self.conv_2(x * x_mask)
        x = torch.relu(x)
        x = self.norm_2(x)
        x = self.drop(x)
        x = self.proj(x * x_mask)
        return x * x_mask


class RotaryPositionalEmbeddings(nn.Module):
    """
    ## RoPE module

    Rotary encoding transforms pairs of features by rotating in the 2D plane.
    That is, it organizes the $d$ features as $\frac{d}{2}$ pairs.
    Each pair can be considered a coordinate in a 2D plane, and the encoding will rotate it
    by an angle depending on the position of the token.
    """

    def __init__(self, d: int, base: int = 10_000):
        r"""
        * `d` is the number of features $d$
        * `base` is the constant used for calculating $\Theta$
        """
        super().__init__()

        self.base = base
        self.d = int(d)
        self.cos_cached = None
        self.sin_cached = None

    def _build_cache(self, x: torch.Tensor):
        r"""
        Cache $\cos$ and $\sin$ values
        """
        # Return if cache is already built
        if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:
            return

        # Get sequence length
        seq_len = x.shape[0]

        # $\Theta = {\theta_i = 10000^{-\frac{2(i-1)}{d}}, i \in [1, 2, ..., \frac{d}{2}]}$
        theta = 1.0 / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device)

        # Create position indexes `[0, 1, ..., seq_len - 1]`
        seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device)

        # Calculate the product of position index and $\theta_i$
        idx_theta = torch.einsum("n,d->nd", seq_idx, theta)

        # Concatenate so that for row $m$ we have
        # $[m \theta_0, m \theta_1, ..., m \theta_{\frac{d}{2}}, m \theta_0, m \theta_1, ..., m \theta_{\frac{d}{2}}]$
        idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)

        # Cache them
        self.cos_cached = idx_theta2.cos()[:, None, None, :]
        self.sin_cached = idx_theta2.sin()[:, None, None, :]

    def _neg_half(self, x: torch.Tensor):
        # $\frac{d}{2}$
        d_2 = self.d // 2

        # Calculate $[-x^{(\frac{d}{2} + 1)}, -x^{(\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., x^{(\frac{d}{2})}]$
        return torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1)

    def forward(self, x: torch.Tensor):
        """
        * `x` is the Tensor at the head of a key or a query with shape `[seq_len, batch_size, n_heads, d]`
        """
        # Cache $\cos$ and $\sin$ values
        x = rearrange(x, "b h t d -> t b h d")

        self._build_cache(x)

        # Split the features, we can choose to apply rotary embeddings only to a partial set of features.
        x_rope, x_pass = x[..., : self.d], x[..., self.d :]

        # Calculate
        # $[-x^{(\frac{d}{2} + 1)}, -x^{(\frac{d}{2} + 2)}, ..., -x^{(d)}, x^{(1)}, x^{(2)}, ..., x^{(\frac{d}{2})}]$
        neg_half_x = self._neg_half(x_rope)

        x_rope = (x_rope * self.cos_cached[: x.shape[0]]) + (neg_half_x * self.sin_cached[: x.shape[0]])

        return rearrange(torch.cat((x_rope, x_pass), dim=-1), "t b h d -> b h t d")


class MultiHeadAttention(nn.Module):
    def __init__(
        self,
        channels,
        out_channels,
        n_heads,
        heads_share=True,
        p_dropout=0.0,
        proximal_bias=False,
        proximal_init=False,
    ):
        super().__init__()
        assert channels % n_heads == 0

        self.channels = channels
        self.out_channels = out_channels
        self.n_heads = n_heads
        self.heads_share = heads_share
        self.proximal_bias = proximal_bias
        self.p_dropout = p_dropout
        self.attn = None

        self.k_channels = channels // n_heads
        self.conv_q = torch.nn.Conv1d(channels, channels, 1)
        self.conv_k = torch.nn.Conv1d(channels, channels, 1)
        self.conv_v = torch.nn.Conv1d(channels, channels, 1)

        # from https://nn.labml.ai/transformers/rope/index.html
        self.query_rotary_pe = RotaryPositionalEmbeddings(self.k_channels * 0.5)
        self.key_rotary_pe = RotaryPositionalEmbeddings(self.k_channels * 0.5)

        self.conv_o = torch.nn.Conv1d(channels, out_channels, 1)
        self.drop = torch.nn.Dropout(p_dropout)

        torch.nn.init.xavier_uniform_(self.conv_q.weight)
        torch.nn.init.xavier_uniform_(self.conv_k.weight)
        if proximal_init:
            self.conv_k.weight.data.copy_(self.conv_q.weight.data)
            self.conv_k.bias.data.copy_(self.conv_q.bias.data)
        torch.nn.init.xavier_uniform_(self.conv_v.weight)

    def forward(self, x, c, attn_mask=None):
        q = self.conv_q(x)
        k = self.conv_k(c)
        v = self.conv_v(c)

        x, self.attn = self.attention(q, k, v, mask=attn_mask)

        x = self.conv_o(x)
        return x

    def attention(self, query, key, value, mask=None):
        b, d, t_s, t_t = (*key.size(), query.size(2))
        query = rearrange(query, "b (h c) t-> b h t c", h=self.n_heads)
        key = rearrange(key, "b (h c) t-> b h t c", h=self.n_heads)
        value = rearrange(value, "b (h c) t-> b h t c", h=self.n_heads)

        query = self.query_rotary_pe(query)
        key = self.key_rotary_pe(key)

        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.k_channels)

        if self.proximal_bias:
            assert t_s == t_t, "Proximal bias is only available for self-attention."
            scores = scores + self._attention_bias_proximal(t_s).to(device=scores.device, dtype=scores.dtype)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e4)
        p_attn = torch.nn.functional.softmax(scores, dim=-1)
        p_attn = self.drop(p_attn)
        output = torch.matmul(p_attn, value)
        output = output.transpose(2, 3).contiguous().view(b, d, t_t)
        return output, p_attn

    @staticmethod
    def _attention_bias_proximal(length):
        r = torch.arange(length, dtype=torch.float32)
        diff = torch.unsqueeze(r, 0) - torch.unsqueeze(r, 1)
        return torch.unsqueeze(torch.unsqueeze(-torch.log1p(torch.abs(diff)), 0), 0)


class FFN(nn.Module):
    def __init__(self, in_channels, out_channels, filter_channels, kernel_size, p_dropout=0.0):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.filter_channels = filter_channels
        self.kernel_size = kernel_size
        self.p_dropout = p_dropout

        self.conv_1 = torch.nn.Conv1d(in_channels, filter_channels, kernel_size, padding=kernel_size // 2)
        self.conv_2 = torch.nn.Conv1d(filter_channels, out_channels, kernel_size, padding=kernel_size // 2)
        self.drop = torch.nn.Dropout(p_dropout)

    def forward(self, x, x_mask):
        x = self.conv_1(x * x_mask)
        x = torch.relu(x)
        x = self.drop(x)
        x = self.conv_2(x * x_mask)
        return x * x_mask


class Encoder(nn.Module):
    def __init__(
        self,
        hidden_channels,
        filter_channels,
        n_heads,
        n_layers,
        kernel_size=1,
        p_dropout=0.0,
        **kwargs,
    ):
        super().__init__()
        self.hidden_channels = hidden_channels
        self.filter_channels = filter_channels
        self.n_heads = n_heads
        self.n_layers = n_layers
        self.kernel_size = kernel_size
        self.p_dropout = p_dropout

        self.drop = torch.nn.Dropout(p_dropout)
        self.attn_layers = torch.nn.ModuleList()
        self.norm_layers_1 = torch.nn.ModuleList()
        self.ffn_layers = torch.nn.ModuleList()
        self.norm_layers_2 = torch.nn.ModuleList()
        for _ in range(self.n_layers):
            self.attn_layers.append(MultiHeadAttention(hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout))
            self.norm_layers_1.append(LayerNorm(hidden_channels))
            self.ffn_layers.append(
                FFN(
                    hidden_channels,
                    hidden_channels,
                    filter_channels,
                    kernel_size,
                    p_dropout=p_dropout,
                )
            )
            self.norm_layers_2.append(LayerNorm(hidden_channels))

    def forward(self, x, x_mask):
        attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)
        for i in range(self.n_layers):
            x = x * x_mask
            y = self.attn_layers[i](x, x, attn_mask)
            y = self.drop(y)
            x = self.norm_layers_1[i](x + y)
            y = self.ffn_layers[i](x, x_mask)
            y = self.drop(y)
            x = self.norm_layers_2[i](x + y)
        x = x * x_mask
        return x


class TextEncoder(nn.Module):
    def __init__(
        self,
        encoder_type,
        encoder_params,
        duration_predictor_params,
        n_vocab,
        n_spks=1,
        spk_emb_dim=128,
    ):
        super().__init__()
        self.encoder_type = encoder_type
        self.n_vocab = n_vocab
        self.n_feats = encoder_params.n_feats
        self.n_channels = encoder_params.n_channels
        self.spk_emb_dim = spk_emb_dim
        self.n_spks = n_spks

        self.emb = torch.nn.Embedding(n_vocab, self.n_channels)
        torch.nn.init.normal_(self.emb.weight, 0.0, self.n_channels**-0.5)

        if encoder_params.prenet:
            self.prenet = ConvReluNorm(
                self.n_channels,
                self.n_channels,
                self.n_channels,
                kernel_size=5,
                n_layers=3,
                p_dropout=0.5,
            )
        else:
            self.prenet = lambda x, x_mask: x

        self.encoder = Encoder(
            encoder_params.n_channels + (spk_emb_dim if n_spks > 1 else 0),
            encoder_params.filter_channels,
            encoder_params.n_heads,
            encoder_params.n_layers,
            encoder_params.kernel_size,
            encoder_params.p_dropout,
        )

        self.proj_m = torch.nn.Conv1d(self.n_channels + (spk_emb_dim if n_spks > 1 else 0), self.n_feats, 1)
        self.proj_w = DurationPredictor(
            self.n_channels + (spk_emb_dim if n_spks > 1 else 0),
            duration_predictor_params.filter_channels_dp,
            duration_predictor_params.kernel_size,
            duration_predictor_params.p_dropout,
        )

    def forward(self, x, x_lengths, spks=None):
        """Run forward pass to the transformer based encoder and duration predictor

        Args:
            x (torch.Tensor): text input
                shape: (batch_size, max_text_length)
            x_lengths (torch.Tensor): text input lengths
                shape: (batch_size,)
            spks (torch.Tensor, optional): speaker ids. Defaults to None.
                shape: (batch_size,)

        Returns:
            mu (torch.Tensor): average output of the encoder
                shape: (batch_size, n_feats, max_text_length)
            logw (torch.Tensor): log duration predicted by the duration predictor
                shape: (batch_size, 1, max_text_length)
            x_mask (torch.Tensor): mask for the text input
                shape: (batch_size, 1, max_text_length)
        """
        x = self.emb(x) * math.sqrt(self.n_channels)
        x = torch.transpose(x, 1, -1)
        x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)

        x = self.prenet(x, x_mask)
        if self.n_spks > 1:
            x = torch.cat([x, spks.unsqueeze(-1).repeat(1, 1, x.shape[-1])], dim=1)
        x = self.encoder(x, x_mask)
        mu = self.proj_m(x) * x_mask

        x_dp = torch.detach(x)
        logw = self.proj_w(x_dp, x_mask)

        return mu, logw, x_mask
</file>

<file path="src/chatterbox/models/s3gen/matcha/transformer.py">
from typing import Any, Dict, Optional

import torch
import torch.nn as nn
from diffusers.models.attention import (
    GEGLU,
    GELU,
    AdaLayerNorm,
    AdaLayerNormZero,
    ApproximateGELU,
)
from diffusers.models.attention_processor import Attention
from diffusers.models.lora import LoRACompatibleLinear
from diffusers.utils.torch_utils import maybe_allow_in_graph


class SnakeBeta(nn.Module):
    """
    A modified Snake function which uses separate parameters for the magnitude of the periodic components
    Shape:
        - Input: (B, C, T)
        - Output: (B, C, T), same shape as the input
    Parameters:
        - alpha - trainable parameter that controls frequency
        - beta - trainable parameter that controls magnitude
    References:
        - This activation function is a modified version based on this paper by Liu Ziyin, Tilman Hartwig, Masahito Ueda:
        https://arxiv.org/abs/2006.08195
    Examples:
        >>> a1 = snakebeta(256)
        >>> x = torch.randn(256)
        >>> x = a1(x)
    """

    def __init__(self, in_features, out_features, alpha=1.0, alpha_trainable=True, alpha_logscale=True):
        """
        Initialization.
        INPUT:
            - in_features: shape of the input
            - alpha - trainable parameter that controls frequency
            - beta - trainable parameter that controls magnitude
            alpha is initialized to 1 by default, higher values = higher-frequency.
            beta is initialized to 1 by default, higher values = higher-magnitude.
            alpha will be trained along with the rest of your model.
        """
        super().__init__()
        self.in_features = out_features if isinstance(out_features, list) else [out_features]
        self.proj = LoRACompatibleLinear(in_features, out_features)

        # initialize alpha
        self.alpha_logscale = alpha_logscale
        if self.alpha_logscale:  # log scale alphas initialized to zeros
            self.alpha = nn.Parameter(torch.zeros(self.in_features) * alpha)
            self.beta = nn.Parameter(torch.zeros(self.in_features) * alpha)
        else:  # linear scale alphas initialized to ones
            self.alpha = nn.Parameter(torch.ones(self.in_features) * alpha)
            self.beta = nn.Parameter(torch.ones(self.in_features) * alpha)

        self.alpha.requires_grad = alpha_trainable
        self.beta.requires_grad = alpha_trainable

        self.no_div_by_zero = 0.000000001

    def forward(self, x):
        """
        Forward pass of the function.
        Applies the function to the input elementwise.
        SnakeBeta ∶= x + 1/b * sin^2 (xa)
        """
        x = self.proj(x)
        if self.alpha_logscale:
            alpha = torch.exp(self.alpha)
            beta = torch.exp(self.beta)
        else:
            alpha = self.alpha
            beta = self.beta

        x = x + (1.0 / (beta + self.no_div_by_zero)) * torch.pow(torch.sin(x * alpha), 2)

        return x


class FeedForward(nn.Module):
    r"""
    A feed-forward layer.

    Parameters:
        dim (`int`): The number of channels in the input.
        dim_out (`int`, *optional*): The number of channels in the output. If not given, defaults to `dim`.
        mult (`int`, *optional*, defaults to 4): The multiplier to use for the hidden dimension.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        activation_fn (`str`, *optional*, defaults to `"geglu"`): Activation function to be used in feed-forward.
        final_dropout (`bool` *optional*, defaults to False): Apply a final dropout.
    """

    def __init__(
        self,
        dim: int,
        dim_out: Optional[int] = None,
        mult: int = 4,
        dropout: float = 0.0,
        activation_fn: str = "geglu",
        final_dropout: bool = False,
    ):
        super().__init__()
        inner_dim = int(dim * mult)
        dim_out = dim_out if dim_out is not None else dim

        if activation_fn == "gelu":
            act_fn = GELU(dim, inner_dim)
        if activation_fn == "gelu-approximate":
            act_fn = GELU(dim, inner_dim, approximate="tanh")
        elif activation_fn == "geglu":
            act_fn = GEGLU(dim, inner_dim)
        elif activation_fn == "geglu-approximate":
            act_fn = ApproximateGELU(dim, inner_dim)
        elif activation_fn == "snakebeta":
            act_fn = SnakeBeta(dim, inner_dim)

        self.net = nn.ModuleList([])
        # project in
        self.net.append(act_fn)
        # project dropout
        self.net.append(nn.Dropout(dropout))
        # project out
        self.net.append(LoRACompatibleLinear(inner_dim, dim_out))
        # FF as used in Vision Transformer, MLP-Mixer, etc. have a final dropout
        if final_dropout:
            self.net.append(nn.Dropout(dropout))

    def forward(self, hidden_states):
        for module in self.net:
            hidden_states = module(hidden_states)
        return hidden_states


@maybe_allow_in_graph
class BasicTransformerBlock(nn.Module):
    r"""
    A basic Transformer block.

    Parameters:
        dim (`int`): The number of channels in the input and output.
        num_attention_heads (`int`): The number of heads to use for multi-head attention.
        attention_head_dim (`int`): The number of channels in each head.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        cross_attention_dim (`int`, *optional*): The size of the encoder_hidden_states vector for cross attention.
        only_cross_attention (`bool`, *optional*):
            Whether to use only cross-attention layers. In this case two cross attention layers are used.
        double_self_attention (`bool`, *optional*):
            Whether to use two self-attention layers. In this case no cross attention layers are used.
        activation_fn (`str`, *optional*, defaults to `"geglu"`): Activation function to be used in feed-forward.
        num_embeds_ada_norm (:
            obj: `int`, *optional*): The number of diffusion steps used during training. See `Transformer2DModel`.
        attention_bias (:
            obj: `bool`, *optional*, defaults to `False`): Configure if the attentions should contain a bias parameter.
    """

    def __init__(
        self,
        dim: int,
        num_attention_heads: int,
        attention_head_dim: int,
        dropout=0.0,
        cross_attention_dim: Optional[int] = None,
        activation_fn: str = "geglu",
        num_embeds_ada_norm: Optional[int] = None,
        attention_bias: bool = False,
        only_cross_attention: bool = False,
        double_self_attention: bool = False,
        upcast_attention: bool = False,
        norm_elementwise_affine: bool = True,
        norm_type: str = "layer_norm",
        final_dropout: bool = False,
    ):
        super().__init__()
        self.only_cross_attention = only_cross_attention

        self.use_ada_layer_norm_zero = (num_embeds_ada_norm is not None) and norm_type == "ada_norm_zero"
        self.use_ada_layer_norm = (num_embeds_ada_norm is not None) and norm_type == "ada_norm"

        if norm_type in ("ada_norm", "ada_norm_zero") and num_embeds_ada_norm is None:
            raise ValueError(
                f"`norm_type` is set to {norm_type}, but `num_embeds_ada_norm` is not defined. Please make sure to"
                f" define `num_embeds_ada_norm` if setting `norm_type` to {norm_type}."
            )

        # Define 3 blocks. Each block has its own normalization layer.
        # 1. Self-Attn
        if self.use_ada_layer_norm:
            self.norm1 = AdaLayerNorm(dim, num_embeds_ada_norm)
        elif self.use_ada_layer_norm_zero:
            self.norm1 = AdaLayerNormZero(dim, num_embeds_ada_norm)
        else:
            self.norm1 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        self.attn1 = Attention(
            query_dim=dim,
            heads=num_attention_heads,
            dim_head=attention_head_dim,
            dropout=dropout,
            bias=attention_bias,
            cross_attention_dim=cross_attention_dim if only_cross_attention else None,
            upcast_attention=upcast_attention,
        )

        # 2. Cross-Attn
        if cross_attention_dim is not None or double_self_attention:
            # We currently only use AdaLayerNormZero for self attention where there will only be one attention block.
            # I.e. the number of returned modulation chunks from AdaLayerZero would not make sense if returned during
            # the second cross attention block.
            self.norm2 = (
                AdaLayerNorm(dim, num_embeds_ada_norm)
                if self.use_ada_layer_norm
                else nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
            )
            self.attn2 = Attention(
                query_dim=dim,
                cross_attention_dim=cross_attention_dim if not double_self_attention else None,
                heads=num_attention_heads,
                dim_head=attention_head_dim,
                dropout=dropout,
                bias=attention_bias,
                upcast_attention=upcast_attention,
                # scale_qk=False, # uncomment this to not to use flash attention
            )  # is self-attn if encoder_hidden_states is none
        else:
            self.norm2 = None
            self.attn2 = None

        # 3. Feed-forward
        self.norm3 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        self.ff = FeedForward(dim, dropout=dropout, activation_fn=activation_fn, final_dropout=final_dropout)

        # let chunk size default to None
        self._chunk_size = None
        self._chunk_dim = 0

    def set_chunk_feed_forward(self, chunk_size: Optional[int], dim: int):
        # Sets chunk feed-forward
        self._chunk_size = chunk_size
        self._chunk_dim = dim

    def forward(
        self,
        hidden_states: torch.FloatTensor,
        attention_mask: Optional[torch.FloatTensor] = None,
        encoder_hidden_states: Optional[torch.FloatTensor] = None,
        encoder_attention_mask: Optional[torch.FloatTensor] = None,
        timestep: Optional[torch.LongTensor] = None,
        cross_attention_kwargs: Dict[str, Any] = None,
        class_labels: Optional[torch.LongTensor] = None,
    ):
        # Notice that normalization is always applied before the real computation in the following blocks.
        # 1. Self-Attention
        if self.use_ada_layer_norm:
            norm_hidden_states = self.norm1(hidden_states, timestep)
        elif self.use_ada_layer_norm_zero:
            norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.norm1(
                hidden_states, timestep, class_labels, hidden_dtype=hidden_states.dtype
            )
        else:
            norm_hidden_states = self.norm1(hidden_states)

        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}

        attn_output = self.attn1(
            norm_hidden_states,
            encoder_hidden_states=encoder_hidden_states if self.only_cross_attention else None,
            attention_mask=encoder_attention_mask if self.only_cross_attention else attention_mask,
            **cross_attention_kwargs,
        )
        if self.use_ada_layer_norm_zero:
            attn_output = gate_msa.unsqueeze(1) * attn_output
        hidden_states = attn_output + hidden_states

        # 2. Cross-Attention
        if self.attn2 is not None:
            norm_hidden_states = (
                self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)
            )

            attn_output = self.attn2(
                norm_hidden_states,
                encoder_hidden_states=encoder_hidden_states,
                attention_mask=encoder_attention_mask,
                **cross_attention_kwargs,
            )
            hidden_states = attn_output + hidden_states

        # 3. Feed-forward
        norm_hidden_states = self.norm3(hidden_states)

        if self.use_ada_layer_norm_zero:
            norm_hidden_states = norm_hidden_states * (1 + scale_mlp[:, None]) + shift_mlp[:, None]

        if self._chunk_size is not None:
            # "feed_forward_chunk_size" can be used to save memory
            if norm_hidden_states.shape[self._chunk_dim] % self._chunk_size != 0:
                raise ValueError(
                    f"`hidden_states` dimension to be chunked: {norm_hidden_states.shape[self._chunk_dim]} has to be divisible by chunk size: {self._chunk_size}. Make sure to set an appropriate `chunk_size` when calling `unet.enable_forward_chunking`."
                )

            num_chunks = norm_hidden_states.shape[self._chunk_dim] // self._chunk_size
            ff_output = torch.cat(
                [self.ff(hid_slice) for hid_slice in norm_hidden_states.chunk(num_chunks, dim=self._chunk_dim)],
                dim=self._chunk_dim,
            )
        else:
            ff_output = self.ff(norm_hidden_states)

        if self.use_ada_layer_norm_zero:
            ff_output = gate_mlp.unsqueeze(1) * ff_output

        hidden_states = ff_output + hidden_states

        return hidden_states
</file>

<file path="src/chatterbox/models/s3gen/transformer/activation.py">
# Copyright (c) 2020 Johns Hopkins University (Shinji Watanabe)
#               2020 Northwestern Polytechnical University (Pengcheng Guo)
#               2020 Mobvoi Inc (Binbin Zhang)
#               2024 Alibaba Inc (Xiang Lyu)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Swish() activation function for Conformer."""

import torch
from torch import nn, sin, pow
from torch.nn import Parameter


class Swish(torch.nn.Module):
    """Construct an Swish object."""

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Return Swish activation function."""
        return x * torch.sigmoid(x)


# Implementation adapted from https://github.com/EdwardDixon/snake under the MIT license.
#   LICENSE is in incl_licenses directory.
class Snake(nn.Module):
    '''
    Implementation of a sine-based periodic activation function
    Shape:
        - Input: (B, C, T)
        - Output: (B, C, T), same shape as the input
    Parameters:
        - alpha - trainable parameter
    References:
        - This activation function is from this paper by Liu Ziyin, Tilman Hartwig, Masahito Ueda:
        https://arxiv.org/abs/2006.08195
    Examples:
        >>> a1 = snake(256)
        >>> x = torch.randn(256)
        >>> x = a1(x)
    '''
    def __init__(self, in_features, alpha=1.0, alpha_trainable=True, alpha_logscale=False):
        '''
        Initialization.
        INPUT:
            - in_features: shape of the input
            - alpha: trainable parameter
            alpha is initialized to 1 by default, higher values = higher-frequency.
            alpha will be trained along with the rest of your model.
        '''
        super(Snake, self).__init__()
        self.in_features = in_features

        # initialize alpha
        self.alpha_logscale = alpha_logscale
        if self.alpha_logscale:  # log scale alphas initialized to zeros
            self.alpha = Parameter(torch.zeros(in_features) * alpha)
        else:  # linear scale alphas initialized to ones
            self.alpha = Parameter(torch.ones(in_features) * alpha)

        self.alpha.requires_grad = alpha_trainable

        self.no_div_by_zero = 0.000000001

    def forward(self, x):
        '''
        Forward pass of the function.
        Applies the function to the input elementwise.
        Snake ∶= x + 1/a * sin^2 (xa)
        '''
        alpha = self.alpha.unsqueeze(0).unsqueeze(-1)  # line up with x to [B, C, T]
        if self.alpha_logscale:
            alpha = torch.exp(alpha)
        x = x + (1.0 / (alpha + self.no_div_by_zero)) * pow(sin(x * alpha), 2)

        return x
</file>

<file path="src/chatterbox/models/s3gen/transformer/attention.py">
# Copyright (c) 2019 Shigeki Karita
#               2020 Mobvoi Inc (Binbin Zhang)
#               2022 Xingchen Song (sxc19@mails.tsinghua.edu.cn)
#               2024 Alibaba Inc (Xiang Lyu)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Multi-Head Attention layer definition."""

import math
from typing import Tuple

import torch
from torch import nn


class MultiHeadedAttention(nn.Module):
    """Multi-Head Attention layer.

    Args:
        n_head (int): The number of heads.
        n_feat (int): The number of features.
        dropout_rate (float): Dropout rate.

    """

    def __init__(self,
                 n_head: int,
                 n_feat: int,
                 dropout_rate: float,
                 key_bias: bool = True):
        """Construct an MultiHeadedAttention object."""
        super().__init__()
        assert n_feat % n_head == 0
        # We assume d_v always equals d_k
        self.d_k = n_feat // n_head
        self.h = n_head
        self.linear_q = nn.Linear(n_feat, n_feat)
        self.linear_k = nn.Linear(n_feat, n_feat, bias=key_bias)
        self.linear_v = nn.Linear(n_feat, n_feat)
        self.linear_out = nn.Linear(n_feat, n_feat)
        self.dropout = nn.Dropout(p=dropout_rate)

    def forward_qkv(
        self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Transform query, key and value.

        Args:
            query (torch.Tensor): Query tensor (#batch, time1, size).
            key (torch.Tensor): Key tensor (#batch, time2, size).
            value (torch.Tensor): Value tensor (#batch, time2, size).

        Returns:
            torch.Tensor: Transformed query tensor, size
                (#batch, n_head, time1, d_k).
            torch.Tensor: Transformed key tensor, size
                (#batch, n_head, time2, d_k).
            torch.Tensor: Transformed value tensor, size
                (#batch, n_head, time2, d_k).

        """
        n_batch = query.size(0)
        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)
        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)
        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)
        q = q.transpose(1, 2)  # (batch, head, time1, d_k)
        k = k.transpose(1, 2)  # (batch, head, time2, d_k)
        v = v.transpose(1, 2)  # (batch, head, time2, d_k)

        return q, k, v

    def forward_attention(
        self,
        value: torch.Tensor,
        scores: torch.Tensor,
        mask: torch.Tensor = torch.ones((0, 0, 0), dtype=torch.bool)
    ) -> torch.Tensor:
        """Compute attention context vector.

        Args:
            value (torch.Tensor): Transformed value, size
                (#batch, n_head, time2, d_k).
            scores (torch.Tensor): Attention score, size
                (#batch, n_head, time1, time2).
            mask (torch.Tensor): Mask, size (#batch, 1, time2) or
                (#batch, time1, time2), (0, 0, 0) means fake mask.

        Returns:
            torch.Tensor: Transformed value (#batch, time1, d_model)
                weighted by the attention score (#batch, time1, time2).

        """
        n_batch = value.size(0)
        # NOTE(xcsong): When will `if mask.size(2) > 0` be True?
        #   1. onnx(16/4) [WHY? Because we feed real cache & real mask for the
        #           1st chunk to ease the onnx export.]
        #   2. pytorch training
        if mask.size(2) > 0:  # time2 > 0
            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, *, time2)
            # For last chunk, time2 might be larger than scores.size(-1)
            mask = mask[:, :, :, :scores.size(-1)]  # (batch, 1, *, time2)
            scores = scores.masked_fill(mask, -float('inf'))
            attn = torch.softmax(scores, dim=-1).masked_fill(
                mask, 0.0)  # (batch, head, time1, time2)
        # NOTE(xcsong): When will `if mask.size(2) > 0` be False?
        #   1. onnx(16/-1, -1/-1, 16/0)
        #   2. jit (16/-1, -1/-1, 16/0, 16/4)
        else:
            attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)

        p_attn = self.dropout(attn)
        x = torch.matmul(p_attn, value)  # (batch, head, time1, d_k)
        x = (x.transpose(1, 2).contiguous().view(n_batch, -1,
                                                 self.h * self.d_k)
             )  # (batch, time1, d_model)

        return self.linear_out(x)  # (batch, time1, d_model)

    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        mask: torch.Tensor = torch.ones((0, 0, 0), dtype=torch.bool),
        pos_emb: torch.Tensor = torch.empty(0),
        cache: torch.Tensor = torch.zeros((0, 0, 0, 0))
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Compute scaled dot product attention.

        Args:
            query (torch.Tensor): Query tensor (#batch, time1, size).
            key (torch.Tensor): Key tensor (#batch, time2, size).
            value (torch.Tensor): Value tensor (#batch, time2, size).
            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or
                (#batch, time1, time2).
                1.When applying cross attention between decoder and encoder,
                the batch padding mask for input is in (#batch, 1, T) shape.
                2.When applying self attention of encoder,
                the mask is in (#batch, T, T)  shape.
                3.When applying self attention of decoder,
                the mask is in (#batch, L, L)  shape.
                4.If the different position in decoder see different block
                of the encoder, such as Mocha, the passed in mask could be
                in (#batch, L, T) shape. But there is no such case in current
                CosyVoice.
            cache (torch.Tensor): Cache tensor (1, head, cache_t, d_k * 2),
                where `cache_t == chunk_size * num_decoding_left_chunks`
                and `head * d_k == size`


        Returns:
            torch.Tensor: Output tensor (#batch, time1, d_model).
            torch.Tensor: Cache tensor (1, head, cache_t + time1, d_k * 2)
                where `cache_t == chunk_size * num_decoding_left_chunks`
                and `head * d_k == size`

        """
        q, k, v = self.forward_qkv(query, key, value)

        # NOTE(xcsong):
        #   when export onnx model, for 1st chunk, we feed
        #       cache(1, head, 0, d_k * 2) (16/-1, -1/-1, 16/0 mode)
        #       or cache(1, head, real_cache_t, d_k * 2) (16/4 mode).
        #       In all modes, `if cache.size(0) > 0` will alwayse be `True`
        #       and we will always do splitting and
        #       concatnation(this will simplify onnx export). Note that
        #       it's OK to concat & split zero-shaped tensors(see code below).
        #   when export jit  model, for 1st chunk, we always feed
        #       cache(0, 0, 0, 0) since jit supports dynamic if-branch.
        # >>> a = torch.ones((1, 2, 0, 4))
        # >>> b = torch.ones((1, 2, 3, 4))
        # >>> c = torch.cat((a, b), dim=2)
        # >>> torch.equal(b, c)        # True
        # >>> d = torch.split(a, 2, dim=-1)
        # >>> torch.equal(d[0], d[1])  # True
        if cache.size(0) > 0:
            key_cache, value_cache = torch.split(cache,
                                                 cache.size(-1) // 2,
                                                 dim=-1)
            k = torch.cat([key_cache, k], dim=2)
            v = torch.cat([value_cache, v], dim=2)
        # NOTE(xcsong): We do cache slicing in encoder.forward_chunk, since it's
        #   non-trivial to calculate `next_cache_start` here.
        new_cache = torch.cat((k, v), dim=-1)

        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        return self.forward_attention(v, scores, mask), new_cache


class RelPositionMultiHeadedAttention(MultiHeadedAttention):
    """Multi-Head Attention layer with relative position encoding.
    Paper: https://arxiv.org/abs/1901.02860
    Args:
        n_head (int): The number of heads.
        n_feat (int): The number of features.
        dropout_rate (float): Dropout rate.
    """

    def __init__(self,
                 n_head: int,
                 n_feat: int,
                 dropout_rate: float,
                 key_bias: bool = True):
        """Construct an RelPositionMultiHeadedAttention object."""
        super().__init__(n_head, n_feat, dropout_rate, key_bias)
        # linear transformation for positional encoding
        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)
        # these two learnable bias are used in matrix c and matrix d
        # as described in https://arxiv.org/abs/1901.02860 Section 3.3
        self.pos_bias_u = nn.Parameter(torch.Tensor(self.h, self.d_k))
        self.pos_bias_v = nn.Parameter(torch.Tensor(self.h, self.d_k))
        torch.nn.init.xavier_uniform_(self.pos_bias_u)
        torch.nn.init.xavier_uniform_(self.pos_bias_v)

    def rel_shift(self, x: torch.Tensor) -> torch.Tensor:
        """Compute relative positional encoding.

        Args:
            x (torch.Tensor): Input tensor (batch, head, time1, 2*time1-1).
            time1 means the length of query vector.

        Returns:
            torch.Tensor: Output tensor.

        """
        zero_pad = torch.zeros((x.size()[0], x.size()[1], x.size()[2], 1),
                               device=x.device,
                               dtype=x.dtype)
        x_padded = torch.cat([zero_pad, x], dim=-1)

        x_padded = x_padded.view(x.size()[0],
                                 x.size()[1],
                                 x.size(3) + 1, x.size(2))
        x = x_padded[:, :, 1:].view_as(x)[
            :, :, :, : x.size(-1) // 2 + 1
        ]  # only keep the positions from 0 to time2
        return x

    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        mask: torch.Tensor = torch.ones((0, 0, 0), dtype=torch.bool),
        pos_emb: torch.Tensor = torch.empty(0),
        cache: torch.Tensor = torch.zeros((0, 0, 0, 0))
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Compute 'Scaled Dot Product Attention' with rel. positional encoding.
        Args:
            query (torch.Tensor): Query tensor (#batch, time1, size).
            key (torch.Tensor): Key tensor (#batch, time2, size).
            value (torch.Tensor): Value tensor (#batch, time2, size).
            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or
                (#batch, time1, time2), (0, 0, 0) means fake mask.
            pos_emb (torch.Tensor): Positional embedding tensor
                (#batch, time2, size).
            cache (torch.Tensor): Cache tensor (1, head, cache_t, d_k * 2),
                where `cache_t == chunk_size * num_decoding_left_chunks`
                and `head * d_k == size`
        Returns:
            torch.Tensor: Output tensor (#batch, time1, d_model).
            torch.Tensor: Cache tensor (1, head, cache_t + time1, d_k * 2)
                where `cache_t == chunk_size * num_decoding_left_chunks`
                and `head * d_k == size`
        """
        q, k, v = self.forward_qkv(query, key, value)
        q = q.transpose(1, 2)  # (batch, time1, head, d_k)

        # NOTE(xcsong):
        #   when export onnx model, for 1st chunk, we feed
        #       cache(1, head, 0, d_k * 2) (16/-1, -1/-1, 16/0 mode)
        #       or cache(1, head, real_cache_t, d_k * 2) (16/4 mode).
        #       In all modes, `if cache.size(0) > 0` will alwayse be `True`
        #       and we will always do splitting and
        #       concatnation(this will simplify onnx export). Note that
        #       it's OK to concat & split zero-shaped tensors(see code below).
        #   when export jit  model, for 1st chunk, we always feed
        #       cache(0, 0, 0, 0) since jit supports dynamic if-branch.
        # >>> a = torch.ones((1, 2, 0, 4))
        # >>> b = torch.ones((1, 2, 3, 4))
        # >>> c = torch.cat((a, b), dim=2)
        # >>> torch.equal(b, c)        # True
        # >>> d = torch.split(a, 2, dim=-1)
        # >>> torch.equal(d[0], d[1])  # True
        if cache.size(0) > 0:
            key_cache, value_cache = torch.split(cache,
                                                 cache.size(-1) // 2,
                                                 dim=-1)
            k = torch.cat([key_cache, k], dim=2)
            v = torch.cat([value_cache, v], dim=2)
        # NOTE(xcsong): We do cache slicing in encoder.forward_chunk, since it's
        #   non-trivial to calculate `next_cache_start` here.
        new_cache = torch.cat((k, v), dim=-1)

        n_batch_pos = pos_emb.size(0)
        p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)
        p = p.transpose(1, 2)  # (batch, head, time1, d_k)

        # (batch, head, time1, d_k)
        q_with_bias_u = (q + self.pos_bias_u.to(q.device)).transpose(1, 2)
        # (batch, head, time1, d_k)
        q_with_bias_v = (q + self.pos_bias_v.to(q.device)).transpose(1, 2)

        # compute attention score
        # first compute matrix a and matrix c
        # as described in https://arxiv.org/abs/1901.02860 Section 3.3
        # (batch, head, time1, time2)
        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))

        # compute matrix b and matrix d
        # (batch, head, time1, time2)
        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))
        # NOTE(Xiang Lyu): Keep rel_shift since espnet rel_pos_emb is used
        if matrix_ac.shape != matrix_bd.shape:
            matrix_bd = self.rel_shift(matrix_bd)

        scores = (matrix_ac + matrix_bd) / math.sqrt(
            self.d_k)  # (batch, head, time1, time2)

        return self.forward_attention(v, scores, mask), new_cache
</file>

<file path="src/chatterbox/models/s3gen/transformer/convolution.py">
# Copyright (c) 2020 Mobvoi Inc. (authors: Binbin Zhang, Di Wu)
#               2024 Alibaba Inc (Xiang Lyu)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Modified from ESPnet(https://github.com/espnet/espnet)
"""ConvolutionModule definition."""

from typing import Tuple

import torch
from torch import nn


class ConvolutionModule(nn.Module):
    """ConvolutionModule in Conformer model."""

    def __init__(self,
                 channels: int,
                 kernel_size: int = 15,
                 activation: nn.Module = nn.ReLU(),
                 norm: str = "batch_norm",
                 causal: bool = False,
                 bias: bool = True):
        """Construct an ConvolutionModule object.
        Args:
            channels (int): The number of channels of conv layers.
            kernel_size (int): Kernel size of conv layers.
            causal (int): Whether use causal convolution or not
        """
        super().__init__()

        self.pointwise_conv1 = nn.Conv1d(
            channels,
            2 * channels,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=bias,
        )
        # self.lorder is used to distinguish if it's a causal convolution,
        # if self.lorder > 0: it's a causal convolution, the input will be
        #    padded with self.lorder frames on the left in forward.
        # else: it's a symmetrical convolution
        if causal:
            padding = 0
            self.lorder = kernel_size - 1
        else:
            # kernel_size should be an odd number for none causal convolution
            assert (kernel_size - 1) % 2 == 0
            padding = (kernel_size - 1) // 2
            self.lorder = 0
        self.depthwise_conv = nn.Conv1d(
            channels,
            channels,
            kernel_size,
            stride=1,
            padding=padding,
            groups=channels,
            bias=bias,
        )

        assert norm in ['batch_norm', 'layer_norm']
        if norm == "batch_norm":
            self.use_layer_norm = False
            self.norm = nn.BatchNorm1d(channels)
        else:
            self.use_layer_norm = True
            self.norm = nn.LayerNorm(channels)

        self.pointwise_conv2 = nn.Conv1d(
            channels,
            channels,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=bias,
        )
        self.activation = activation

    def forward(
        self,
        x: torch.Tensor,
        mask_pad: torch.Tensor = torch.ones((0, 0, 0), dtype=torch.bool),
        cache: torch.Tensor = torch.zeros((0, 0, 0)),
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Compute convolution module.
        Args:
            x (torch.Tensor): Input tensor (#batch, time, channels).
            mask_pad (torch.Tensor): used for batch padding (#batch, 1, time),
                (0, 0, 0) means fake mask.
            cache (torch.Tensor): left context cache, it is only
                used in causal convolution (#batch, channels, cache_t),
                (0, 0, 0) meas fake cache.
        Returns:
            torch.Tensor: Output tensor (#batch, time, channels).
        """
        # exchange the temporal dimension and the feature dimension
        x = x.transpose(1, 2)  # (#batch, channels, time)

        # mask batch padding
        if mask_pad.size(2) > 0:  # time > 0
            x.masked_fill_(~mask_pad, 0.0)

        if self.lorder > 0:
            if cache.size(2) == 0:  # cache_t == 0
                x = nn.functional.pad(x, (self.lorder, 0), 'constant', 0.0)
            else:
                assert cache.size(0) == x.size(0)  # equal batch
                assert cache.size(1) == x.size(1)  # equal channel
                x = torch.cat((cache, x), dim=2)
            assert (x.size(2) > self.lorder)
            new_cache = x[:, :, -self.lorder:]
        else:
            # It's better we just return None if no cache is required,
            # However, for JIT export, here we just fake one tensor instead of
            # None.
            new_cache = torch.zeros((0, 0, 0), dtype=x.dtype, device=x.device)

        # GLU mechanism
        x = self.pointwise_conv1(x)  # (batch, 2*channel, dim)
        x = nn.functional.glu(x, dim=1)  # (batch, channel, dim)

        # 1D Depthwise Conv
        x = self.depthwise_conv(x)
        if self.use_layer_norm:
            x = x.transpose(1, 2)
        x = self.activation(self.norm(x))
        if self.use_layer_norm:
            x = x.transpose(1, 2)
        x = self.pointwise_conv2(x)
        # mask batch padding
        if mask_pad.size(2) > 0:  # time > 0
            x.masked_fill_(~mask_pad, 0.0)

        return x.transpose(1, 2), new_cache
</file>

<file path="src/chatterbox/models/s3gen/transformer/embedding.py">
# Copyright (c) 2020 Mobvoi Inc. (authors: Binbin Zhang, Di Wu)
#               2024 Alibaba Inc (Xiang Lyu)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Modified from ESPnet(https://github.com/espnet/espnet)
"""Positonal Encoding Module."""

import math
from typing import Tuple, Union

import torch
import torch.nn.functional as F
import numpy as np


class PositionalEncoding(torch.nn.Module):
    """Positional encoding.

    :param int d_model: embedding dim
    :param float dropout_rate: dropout rate
    :param int max_len: maximum input length

    PE(pos, 2i)   = sin(pos/(10000^(2i/dmodel)))
    PE(pos, 2i+1) = cos(pos/(10000^(2i/dmodel)))
    """

    def __init__(self,
                 d_model: int,
                 dropout_rate: float,
                 max_len: int = 5000,
                 reverse: bool = False):
        """Construct an PositionalEncoding object."""
        super().__init__()
        self.d_model = d_model
        self.xscale = math.sqrt(self.d_model)
        self.dropout = torch.nn.Dropout(p=dropout_rate)
        self.max_len = max_len

        self.pe = torch.zeros(self.max_len, self.d_model)
        position = torch.arange(0, self.max_len,
                                dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, self.d_model, 2, dtype=torch.float32) *
            -(math.log(10000.0) / self.d_model))
        self.pe[:, 0::2] = torch.sin(position * div_term)
        self.pe[:, 1::2] = torch.cos(position * div_term)
        self.pe = self.pe.unsqueeze(0)

    def forward(self,
                x: torch.Tensor,
                offset: Union[int, torch.Tensor] = 0) \
            -> Tuple[torch.Tensor, torch.Tensor]:
        """Add positional encoding.

        Args:
            x (torch.Tensor): Input. Its shape is (batch, time, ...)
            offset (int, torch.tensor): position offset

        Returns:
            torch.Tensor: Encoded tensor. Its shape is (batch, time, ...)
            torch.Tensor: for compatibility to RelPositionalEncoding
        """

        self.pe = self.pe.to(x.device)
        pos_emb = self.position_encoding(offset, x.size(1), False)
        x = x * self.xscale + pos_emb
        return self.dropout(x), self.dropout(pos_emb)

    def position_encoding(self,
                          offset: Union[int, torch.Tensor],
                          size: int,
                          apply_dropout: bool = True) -> torch.Tensor:
        """ For getting encoding in a streaming fashion

        Attention!!!!!
        we apply dropout only once at the whole utterance level in a none
        streaming way, but will call this function several times with
        increasing input size in a streaming scenario, so the dropout will
        be applied several times.

        Args:
            offset (int or torch.tensor): start offset
            size (int): required size of position encoding

        Returns:
            torch.Tensor: Corresponding encoding
        """
        # How to subscript a Union type:
        #   https://github.com/pytorch/pytorch/issues/69434
        if isinstance(offset, int):
            assert offset + size <= self.max_len
            pos_emb = self.pe[:, offset:offset + size]
        elif isinstance(offset, torch.Tensor) and offset.dim() == 0:  # scalar
            assert offset + size <= self.max_len
            pos_emb = self.pe[:, offset:offset + size]
        else:  # for batched streaming decoding on GPU
            assert torch.max(offset) + size <= self.max_len
            index = offset.unsqueeze(1) + \
                torch.arange(0, size).to(offset.device)  # B X T
            flag = index > 0
            # remove negative offset
            index = index * flag
            pos_emb = F.embedding(index, self.pe[0])  # B X T X d_model

        if apply_dropout:
            pos_emb = self.dropout(pos_emb)
        return pos_emb


class RelPositionalEncoding(PositionalEncoding):
    """Relative positional encoding module.
    See : Appendix B in https://arxiv.org/abs/1901.02860
    Args:
        d_model (int): Embedding dimension.
        dropout_rate (float): Dropout rate.
        max_len (int): Maximum input length.
    """

    def __init__(self, d_model: int, dropout_rate: float, max_len: int = 5000):
        """Initialize class."""
        super().__init__(d_model, dropout_rate, max_len, reverse=True)

    def forward(self,
                x: torch.Tensor,
                offset: Union[int, torch.Tensor] = 0) \
            -> Tuple[torch.Tensor, torch.Tensor]:
        """Compute positional encoding.
        Args:
            x (torch.Tensor): Input tensor (batch, time, `*`).
        Returns:
            torch.Tensor: Encoded tensor (batch, time, `*`).
            torch.Tensor: Positional embedding tensor (1, time, `*`).
        """
        self.pe = self.pe.to(x.device)
        x = x * self.xscale
        pos_emb = self.position_encoding(offset, x.size(1), False)
        return self.dropout(x), self.dropout(pos_emb)


class WhisperPositionalEncoding(PositionalEncoding):
    """ Sinusoids position encoding used in openai-whisper.encoder
    """

    def __init__(self, d_model: int, dropout_rate: float, max_len: int = 1500):
        super().__init__(d_model, dropout_rate, max_len)
        self.xscale = 1.0
        log_timescale_increment = np.log(10000) / (d_model // 2 - 1)
        inv_timescales = torch.exp(-log_timescale_increment *
                                   torch.arange(d_model // 2))
        scaled_time = torch.arange(max_len)[:, np.newaxis] * \
            inv_timescales[np.newaxis, :]
        pe = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)
        delattr(self, "pe")
        self.register_buffer("pe", pe.unsqueeze(0))


class LearnablePositionalEncoding(PositionalEncoding):
    """ Learnable position encoding used in openai-whisper.decoder
    """

    def __init__(self, d_model: int, dropout_rate: float, max_len: int = 448):
        super().__init__(d_model, dropout_rate, max_len)
        # NOTE(xcsong): overwrite self.pe & self.xscale
        self.pe = torch.nn.Parameter(torch.empty(1, max_len, d_model))
        self.xscale = 1.0


class NoPositionalEncoding(torch.nn.Module):
    """ No position encoding
    """

    def __init__(self, d_model: int, dropout_rate: float):
        super().__init__()
        self.d_model = d_model
        self.dropout = torch.nn.Dropout(p=dropout_rate)

    def forward(self,
                x: torch.Tensor,
                offset: Union[int, torch.Tensor] = 0) \
            -> Tuple[torch.Tensor, torch.Tensor]:
        """ Just return zero vector for interface compatibility
        """
        pos_emb = torch.zeros(1, x.size(1), self.d_model).to(x.device)
        return self.dropout(x), pos_emb

    def position_encoding(self, offset: Union[int, torch.Tensor],
                          size: int) -> torch.Tensor:
        return torch.zeros(1, size, self.d_model)


class EspnetRelPositionalEncoding(torch.nn.Module):
    """Relative positional encoding module (new implementation).

    Details can be found in https://github.com/espnet/espnet/pull/2816.

    See : Appendix B in https://arxiv.org/abs/1901.02860

    Args:
        d_model (int): Embedding dimension.
        dropout_rate (float): Dropout rate.
        max_len (int): Maximum input length.

    """

    def __init__(self, d_model: int, dropout_rate: float, max_len: int = 5000):
        """Construct an PositionalEncoding object."""
        super(EspnetRelPositionalEncoding, self).__init__()
        self.d_model = d_model
        self.xscale = math.sqrt(self.d_model)
        self.dropout = torch.nn.Dropout(p=dropout_rate)
        self.pe = None
        self.extend_pe(torch.tensor(0.0).expand(1, max_len))

    def extend_pe(self, x: torch.Tensor):
        """Reset the positional encodings."""
        if self.pe is not None:
            # self.pe contains both positive and negative parts
            # the length of self.pe is 2 * input_len - 1
            if self.pe.size(1) >= x.size(1) * 2 - 1:
                if self.pe.dtype != x.dtype or self.pe.device != x.device:
                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)
                return
        # Suppose `i` means to the position of query vecotr and `j` means the
        # position of key vector. We use position relative positions when keys
        # are to the left (i>j) and negative relative positions otherwise (i<j).
        pe_positive = torch.zeros(x.size(1), self.d_model)
        pe_negative = torch.zeros(x.size(1), self.d_model)
        position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, self.d_model, 2, dtype=torch.float32)
            * -(math.log(10000.0) / self.d_model)
        )
        pe_positive[:, 0::2] = torch.sin(position * div_term)
        pe_positive[:, 1::2] = torch.cos(position * div_term)
        pe_negative[:, 0::2] = torch.sin(-1 * position * div_term)
        pe_negative[:, 1::2] = torch.cos(-1 * position * div_term)

        # Reserve the order of positive indices and concat both positive and
        # negative indices. This is used to support the shifting trick
        # as in https://arxiv.org/abs/1901.02860
        pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)
        pe_negative = pe_negative[1:].unsqueeze(0)
        pe = torch.cat([pe_positive, pe_negative], dim=1)
        self.pe = pe.to(device=x.device, dtype=x.dtype)

    def forward(self, x: torch.Tensor, offset: Union[int, torch.Tensor] = 0) \
            -> Tuple[torch.Tensor, torch.Tensor]:
        """Add positional encoding.

        Args:
            x (torch.Tensor): Input tensor (batch, time, `*`).

        Returns:
            torch.Tensor: Encoded tensor (batch, time, `*`).

        """
        self.extend_pe(x)
        x = x * self.xscale
        pos_emb = self.position_encoding(size=x.size(1), offset=offset)
        return self.dropout(x), self.dropout(pos_emb)

    def position_encoding(self,
                          offset: Union[int, torch.Tensor],
                          size: int) -> torch.Tensor:
        """ For getting encoding in a streaming fashion

        Attention!!!!!
        we apply dropout only once at the whole utterance level in a none
        streaming way, but will call this function several times with
        increasing input size in a streaming scenario, so the dropout will
        be applied several times.

        Args:
            offset (int or torch.tensor): start offset
            size (int): required size of position encoding

        Returns:
            torch.Tensor: Corresponding encoding
        """
        pos_emb = self.pe[
            :,
            self.pe.size(1) // 2 - size + 1: self.pe.size(1) // 2 + size,
        ]
        return pos_emb
</file>

<file path="src/chatterbox/models/s3gen/transformer/encoder_layer.py">
# Copyright (c) 2021 Mobvoi Inc (Binbin Zhang, Di Wu)
#               2022 Xingchen Song (sxc19@mails.tsinghua.edu.cn)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Modified from ESPnet(https://github.com/espnet/espnet)
"""Encoder self-attention layer definition."""

from typing import Optional, Tuple

import torch
from torch import nn


class TransformerEncoderLayer(nn.Module):
    """Encoder layer module.

    Args:
        size (int): Input dimension.
        self_attn (torch.nn.Module): Self-attention module instance.
            `MultiHeadedAttention` or `RelPositionMultiHeadedAttention`
            instance can be used as the argument.
        feed_forward (torch.nn.Module): Feed-forward module instance.
            `PositionwiseFeedForward`, instance can be used as the argument.
        dropout_rate (float): Dropout rate.
        normalize_before (bool):
            True: use layer_norm before each sub-block.
            False: to use layer_norm after each sub-block.
    """

    def __init__(
        self,
        size: int,
        self_attn: torch.nn.Module,
        feed_forward: torch.nn.Module,
        dropout_rate: float,
        normalize_before: bool = True,
    ):
        """Construct an EncoderLayer object."""
        super().__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.norm1 = nn.LayerNorm(size, eps=1e-12)
        self.norm2 = nn.LayerNorm(size, eps=1e-12)
        self.dropout = nn.Dropout(dropout_rate)
        self.size = size
        self.normalize_before = normalize_before

    def forward(
        self,
        x: torch.Tensor,
        mask: torch.Tensor,
        pos_emb: torch.Tensor,
        mask_pad: torch.Tensor = torch.ones((0, 0, 0), dtype=torch.bool),
        att_cache: torch.Tensor = torch.zeros((0, 0, 0, 0)),
        cnn_cache: torch.Tensor = torch.zeros((0, 0, 0, 0)),
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """Compute encoded features.

        Args:
            x (torch.Tensor): (#batch, time, size)
            mask (torch.Tensor): Mask tensor for the input (#batch, time，time),
                (0, 0, 0) means fake mask.
            pos_emb (torch.Tensor): just for interface compatibility
                to ConformerEncoderLayer
            mask_pad (torch.Tensor): does not used in transformer layer,
                just for unified api with conformer.
            att_cache (torch.Tensor): Cache tensor of the KEY & VALUE
                (#batch=1, head, cache_t1, d_k * 2), head * d_k == size.
            cnn_cache (torch.Tensor): Convolution cache in conformer layer
                (#batch=1, size, cache_t2), not used here, it's for interface
                compatibility to ConformerEncoderLayer.
        Returns:
            torch.Tensor: Output tensor (#batch, time, size).
            torch.Tensor: Mask tensor (#batch, time, time).
            torch.Tensor: att_cache tensor,
                (#batch=1, head, cache_t1 + time, d_k * 2).
            torch.Tensor: cnn_cahce tensor (#batch=1, size, cache_t2).

        """
        residual = x
        if self.normalize_before:
            x = self.norm1(x)
        x_att, new_att_cache = self.self_attn(x, x, x, mask, pos_emb=pos_emb, cache=att_cache)
        x = residual + self.dropout(x_att)
        if not self.normalize_before:
            x = self.norm1(x)

        residual = x
        if self.normalize_before:
            x = self.norm2(x)
        x = residual + self.dropout(self.feed_forward(x))
        if not self.normalize_before:
            x = self.norm2(x)

        fake_cnn_cache = torch.zeros((0, 0, 0), dtype=x.dtype, device=x.device)
        return x, mask, new_att_cache, fake_cnn_cache


class ConformerEncoderLayer(nn.Module):
    """Encoder layer module.
    Args:
        size (int): Input dimension.
        self_attn (torch.nn.Module): Self-attention module instance.
            `MultiHeadedAttention` or `RelPositionMultiHeadedAttention`
            instance can be used as the argument.
        feed_forward (torch.nn.Module): Feed-forward module instance.
            `PositionwiseFeedForward` instance can be used as the argument.
        feed_forward_macaron (torch.nn.Module): Additional feed-forward module
             instance.
            `PositionwiseFeedForward` instance can be used as the argument.
        conv_module (torch.nn.Module): Convolution module instance.
            `ConvlutionModule` instance can be used as the argument.
        dropout_rate (float): Dropout rate.
        normalize_before (bool):
            True: use layer_norm before each sub-block.
            False: use layer_norm after each sub-block.
    """

    def __init__(
        self,
        size: int,
        self_attn: torch.nn.Module,
        feed_forward: Optional[nn.Module] = None,
        feed_forward_macaron: Optional[nn.Module] = None,
        conv_module: Optional[nn.Module] = None,
        dropout_rate: float = 0.1,
        normalize_before: bool = True,
    ):
        """Construct an EncoderLayer object."""
        super().__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.feed_forward_macaron = feed_forward_macaron
        self.conv_module = conv_module
        self.norm_ff = nn.LayerNorm(size, eps=1e-12)  # for the FNN module
        self.norm_mha = nn.LayerNorm(size, eps=1e-12)  # for the MHA module
        if feed_forward_macaron is not None:
            self.norm_ff_macaron = nn.LayerNorm(size, eps=1e-12)
            self.ff_scale = 0.5
        else:
            self.ff_scale = 1.0
        if self.conv_module is not None:
            self.norm_conv = nn.LayerNorm(size, eps=1e-12)  # for the CNN module
            self.norm_final = nn.LayerNorm(
                size, eps=1e-12)  # for the final output of the block
        self.dropout = nn.Dropout(dropout_rate)
        self.size = size
        self.normalize_before = normalize_before

    def forward(
        self,
        x: torch.Tensor,
        mask: torch.Tensor,
        pos_emb: torch.Tensor,
        mask_pad: torch.Tensor = torch.ones((0, 0, 0), dtype=torch.bool),
        att_cache: torch.Tensor = torch.zeros((0, 0, 0, 0)),
        cnn_cache: torch.Tensor = torch.zeros((0, 0, 0, 0)),
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """Compute encoded features.

        Args:
            x (torch.Tensor): (#batch, time, size)
            mask (torch.Tensor): Mask tensor for the input (#batch, time，time),
                (0, 0, 0) means fake mask.
            pos_emb (torch.Tensor): positional encoding, must not be None
                for ConformerEncoderLayer.
            mask_pad (torch.Tensor): batch padding mask used for conv module.
                (#batch, 1，time), (0, 0, 0) means fake mask.
            att_cache (torch.Tensor): Cache tensor of the KEY & VALUE
                (#batch=1, head, cache_t1, d_k * 2), head * d_k == size.
            cnn_cache (torch.Tensor): Convolution cache in conformer layer
                (#batch=1, size, cache_t2)
        Returns:
            torch.Tensor: Output tensor (#batch, time, size).
            torch.Tensor: Mask tensor (#batch, time, time).
            torch.Tensor: att_cache tensor,
                (#batch=1, head, cache_t1 + time, d_k * 2).
            torch.Tensor: cnn_cahce tensor (#batch, size, cache_t2).
        """

        # whether to use macaron style
        if self.feed_forward_macaron is not None:
            residual = x
            if self.normalize_before:
                x = self.norm_ff_macaron(x)
            x = residual + self.ff_scale * self.dropout(
                self.feed_forward_macaron(x))
            if not self.normalize_before:
                x = self.norm_ff_macaron(x)

        # multi-headed self-attention module
        residual = x
        if self.normalize_before:
            x = self.norm_mha(x)
        x_att, new_att_cache = self.self_attn(x, x, x, mask, pos_emb,
                                              att_cache)
        x = residual + self.dropout(x_att)
        if not self.normalize_before:
            x = self.norm_mha(x)

        # convolution module
        # Fake new cnn cache here, and then change it in conv_module
        new_cnn_cache = torch.zeros((0, 0, 0), dtype=x.dtype, device=x.device)
        if self.conv_module is not None:
            residual = x
            if self.normalize_before:
                x = self.norm_conv(x)
            x, new_cnn_cache = self.conv_module(x, mask_pad, cnn_cache)
            x = residual + self.dropout(x)

            if not self.normalize_before:
                x = self.norm_conv(x)

        # feed forward module
        residual = x
        if self.normalize_before:
            x = self.norm_ff(x)

        x = residual + self.ff_scale * self.dropout(self.feed_forward(x))
        if not self.normalize_before:
            x = self.norm_ff(x)

        if self.conv_module is not None:
            x = self.norm_final(x)

        return x, mask, new_att_cache, new_cnn_cache
</file>

<file path="src/chatterbox/models/s3gen/transformer/positionwise_feed_forward.py">
# Copyright (c) 2019 Shigeki Karita
#               2020 Mobvoi Inc (Binbin Zhang)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Positionwise feed forward layer definition."""

import torch


class PositionwiseFeedForward(torch.nn.Module):
    """Positionwise feed forward layer.

    FeedForward are appied on each position of the sequence.
    The output dim is same with the input dim.

    Args:
        idim (int): Input dimenstion.
        hidden_units (int): The number of hidden units.
        dropout_rate (float): Dropout rate.
        activation (torch.nn.Module): Activation function
    """

    def __init__(
            self,
            idim: int,
            hidden_units: int,
            dropout_rate: float,
            activation: torch.nn.Module = torch.nn.ReLU(),
    ):
        """Construct a PositionwiseFeedForward object."""
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = torch.nn.Linear(idim, hidden_units)
        self.activation = activation
        self.dropout = torch.nn.Dropout(dropout_rate)
        self.w_2 = torch.nn.Linear(hidden_units, idim)

    def forward(self, xs: torch.Tensor) -> torch.Tensor:
        """Forward function.

        Args:
            xs: input tensor (B, L, D)
        Returns:
            output tensor, (B, L, D)
        """
        return self.w_2(self.dropout(self.activation(self.w_1(xs))))


class MoEFFNLayer(torch.nn.Module):
    """
    Mixture of expert with Positionwise feed forward layer
    See also figure 1 in https://arxiv.org/pdf/2305.15663.pdf
    The output dim is same with the input dim.

    Modified from https://github.com/Lightning-AI/lit-gpt/pull/823
                  https://github.com/mistralai/mistral-src/blob/b46d6/moe_one_file_ref.py#L203-L219
    Args:
        n_expert: number of expert.
        n_expert_per_token: The actual number of experts used for each frame
        idim (int): Input dimenstion.
        hidden_units (int): The number of hidden units.
        dropout_rate (float): Dropout rate.
        activation (torch.nn.Module): Activation function
    """

    def __init__(
            self,
            n_expert: int,
            n_expert_per_token: int,
            idim: int,
            hidden_units: int,
            dropout_rate: float,
            activation: torch.nn.Module = torch.nn.ReLU(),
    ):
        super(MoEFFNLayer, self).__init__()
        self.gate = torch.nn.Linear(idim, n_expert, bias=False)
        self.experts = torch.nn.ModuleList(
            PositionwiseFeedForward(idim, hidden_units, dropout_rate,
                                    activation) for _ in range(n_expert))
        self.n_expert_per_token = n_expert_per_token

    def forward(self, xs: torch.Tensor) -> torch.Tensor:
        """Foward function.
        Args:
            xs: input tensor (B, L, D)
        Returns:
            output tensor, (B, L, D)

        """
        B, L, D = xs.size(
        )  # batch size, sequence length, embedding dimension (idim)
        xs = xs.view(-1, D)  # (B*L, D)
        router = self.gate(xs)  # (B*L, n_expert)
        logits, indices = torch.topk(
            router, self.n_expert_per_token
        )  # probs:(B*L, n_expert), indices: (B*L, n_expert)
        weights = torch.nn.functional.softmax(
            logits, dim=1,
            dtype=torch.float).to(dtype=xs.dtype)  # (B*L, n_expert_per_token)
        output = torch.zeros_like(xs)  # (B*L, D)
        for i, expert in enumerate(self.experts):
            mask = indices == i
            batch_idx, ith_expert = torch.where(mask)
            output[batch_idx] += weights[batch_idx, ith_expert, None] * expert(
                xs[batch_idx])
        return output.view(B, L, D)
</file>

<file path="src/chatterbox/models/s3gen/transformer/subsampling.py">
# Copyright (c) 2021 Mobvoi Inc (Binbin Zhang, Di Wu)
#               2024 Alibaba Inc (Xiang Lyu)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Modified from ESPnet(https://github.com/espnet/espnet)
"""Subsampling layer definition."""

from typing import Tuple, Union

import torch


class BaseSubsampling(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.right_context = 0
        self.subsampling_rate = 1

    def position_encoding(self, offset: Union[int, torch.Tensor],
                          size: int) -> torch.Tensor:
        return self.pos_enc.position_encoding(offset, size)


class EmbedinigNoSubsampling(BaseSubsampling):
    """Embedding input without subsampling
    """

    def __init__(self, idim: int, odim: int, dropout_rate: float,
                 pos_enc_class: torch.nn.Module):
        super().__init__()
        self.embed = torch.nn.Embedding(idim, odim)
        self.pos_enc = pos_enc_class

    def forward(
        self,
        x: torch.Tensor,
        x_mask: torch.Tensor,
        offset: Union[int, torch.Tensor] = 0
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Input x.

        Args:
            x (torch.Tensor): Input tensor (#batch, time, idim).
            x_mask (torch.Tensor): Input mask (#batch, 1, time).

        Returns:
            torch.Tensor: linear input tensor (#batch, time', odim),
                where time' = time .
            torch.Tensor: linear input mask (#batch, 1, time'),
                where time' = time .

        """
        x = self.embed(x)
        x, pos_emb = self.pos_enc(x, offset)
        return x, pos_emb, x_mask


class LinearNoSubsampling(BaseSubsampling):
    """Linear transform the input without subsampling

    Args:
        idim (int): Input dimension.
        odim (int): Output dimension.
        dropout_rate (float): Dropout rate.

    """

    def __init__(self, idim: int, odim: int, dropout_rate: float,
                 pos_enc_class: torch.nn.Module):
        """Construct an linear object."""
        super().__init__()
        self.out = torch.nn.Sequential(
            torch.nn.Linear(idim, odim),
            torch.nn.LayerNorm(odim, eps=1e-5),
            torch.nn.Dropout(dropout_rate),
        )
        self.pos_enc = pos_enc_class
        self.right_context = 0
        self.subsampling_rate = 1

    def forward(
        self,
        x: torch.Tensor,
        x_mask: torch.Tensor,
        offset: Union[int, torch.Tensor] = 0
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Input x.

        Args:
            x (torch.Tensor): Input tensor (#batch, time, idim).
            x_mask (torch.Tensor): Input mask (#batch, 1, time).

        Returns:
            torch.Tensor: linear input tensor (#batch, time', odim),
                where time' = time .
            torch.Tensor: linear input mask (#batch, 1, time'),
                where time' = time .

        """
        x = self.out(x)
        x, pos_emb = self.pos_enc(x, offset)
        return x, pos_emb, x_mask


class Conv1dSubsampling2(BaseSubsampling):
    """Convolutional 1D subsampling (to 1/2 length).
       It is designed for Whisper, ref:
       https://github.com/openai/whisper/blob/main/whisper/model.py

    Args:
        idim (int): Input dimension.
        odim (int): Output dimension.
        dropout_rate (float): Dropout rate.

    """

    def __init__(self, idim: int, odim: int, dropout_rate: float,
                 pos_enc_class: torch.nn.Module):
        """Construct an Conv1dSubsampling2 object."""
        super().__init__()
        self.conv = torch.nn.Sequential(
            torch.nn.Conv1d(idim, odim, kernel_size=3, padding=1),
            torch.nn.GELU(),
            torch.nn.Conv1d(odim, odim, kernel_size=3, stride=2, padding=1),
            torch.nn.GELU(),
        )
        self.pos_enc = pos_enc_class
        # The right context for every conv layer is computed by:
        # (kernel_size - 1) * frame_rate_of_this_layer
        self.subsampling_rate = 2
        # 4 = (3 - 1) * 1 + (3 - 1) * 1
        self.right_context = 4

    def forward(
        self,
        x: torch.Tensor,
        x_mask: torch.Tensor,
        offset: Union[int, torch.Tensor] = 0
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Subsample x.

        Args:
            x (torch.Tensor): Input tensor (#batch, time, idim).
            x_mask (torch.Tensor): Input mask (#batch, 1, time).

        Returns:
            torch.Tensor: Subsampled tensor (#batch, time', odim),
                where time' = time // 2.
            torch.Tensor: Subsampled mask (#batch, 1, time'),
                where time' = time // 2.
            torch.Tensor: positional encoding

        """
        time = x.size(1)
        x = x.transpose(1, 2)  # (b, f, t)
        x = self.conv(x)
        x = x.transpose(1, 2)  # (b, t, f)
        x, pos_emb = self.pos_enc(x, offset)
        return x, pos_emb, x_mask[:, :, (time + 1) % 2::2]


class Conv2dSubsampling4(BaseSubsampling):
    """Convolutional 2D subsampling (to 1/4 length).

    Args:
        idim (int): Input dimension.
        odim (int): Output dimension.
        dropout_rate (float): Dropout rate.

    """

    def __init__(self, idim: int, odim: int, dropout_rate: float,
                 pos_enc_class: torch.nn.Module):
        """Construct an Conv2dSubsampling4 object."""
        super().__init__()
        self.conv = torch.nn.Sequential(
            torch.nn.Conv2d(1, odim, 3, 2),
            torch.nn.ReLU(),
            torch.nn.Conv2d(odim, odim, 3, 2),
            torch.nn.ReLU(),
        )
        self.out = torch.nn.Sequential(
            torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 2), odim))
        self.pos_enc = pos_enc_class
        # The right context for every conv layer is computed by:
        # (kernel_size - 1) * frame_rate_of_this_layer
        self.subsampling_rate = 4
        # 6 = (3 - 1) * 1 + (3 - 1) * 2
        self.right_context = 6

    def forward(
        self,
        x: torch.Tensor,
        x_mask: torch.Tensor,
        offset: Union[int, torch.Tensor] = 0
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Subsample x.

        Args:
            x (torch.Tensor): Input tensor (#batch, time, idim).
            x_mask (torch.Tensor): Input mask (#batch, 1, time).

        Returns:
            torch.Tensor: Subsampled tensor (#batch, time', odim),
                where time' = time // 4.
            torch.Tensor: Subsampled mask (#batch, 1, time'),
                where time' = time // 4.
            torch.Tensor: positional encoding

        """
        x = x.unsqueeze(1)  # (b, c=1, t, f)
        x = self.conv(x)
        b, c, t, f = x.size()
        x = self.out(x.transpose(1, 2).contiguous().view(b, t, c * f))
        x, pos_emb = self.pos_enc(x, offset)
        return x, pos_emb, x_mask[:, :, 2::2][:, :, 2::2]


class Conv2dSubsampling6(BaseSubsampling):
    """Convolutional 2D subsampling (to 1/6 length).
    Args:
        idim (int): Input dimension.
        odim (int): Output dimension.
        dropout_rate (float): Dropout rate.
        pos_enc (torch.nn.Module): Custom position encoding layer.
    """

    def __init__(self, idim: int, odim: int, dropout_rate: float,
                 pos_enc_class: torch.nn.Module):
        """Construct an Conv2dSubsampling6 object."""
        super().__init__()
        self.conv = torch.nn.Sequential(
            torch.nn.Conv2d(1, odim, 3, 2),
            torch.nn.ReLU(),
            torch.nn.Conv2d(odim, odim, 5, 3),
            torch.nn.ReLU(),
        )
        self.linear = torch.nn.Linear(odim * (((idim - 1) // 2 - 2) // 3),
                                      odim)
        self.pos_enc = pos_enc_class
        # 10 = (3 - 1) * 1 + (5 - 1) * 2
        self.subsampling_rate = 6
        self.right_context = 10

    def forward(
        self,
        x: torch.Tensor,
        x_mask: torch.Tensor,
        offset: Union[int, torch.Tensor] = 0
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Subsample x.
        Args:
            x (torch.Tensor): Input tensor (#batch, time, idim).
            x_mask (torch.Tensor): Input mask (#batch, 1, time).

        Returns:
            torch.Tensor: Subsampled tensor (#batch, time', odim),
                where time' = time // 6.
            torch.Tensor: Subsampled mask (#batch, 1, time'),
                where time' = time // 6.
            torch.Tensor: positional encoding
        """
        x = x.unsqueeze(1)  # (b, c, t, f)
        x = self.conv(x)
        b, c, t, f = x.size()
        x = self.linear(x.transpose(1, 2).contiguous().view(b, t, c * f))
        x, pos_emb = self.pos_enc(x, offset)
        return x, pos_emb, x_mask[:, :, 2::2][:, :, 4::3]


class Conv2dSubsampling8(BaseSubsampling):
    """Convolutional 2D subsampling (to 1/8 length).

    Args:
        idim (int): Input dimension.
        odim (int): Output dimension.
        dropout_rate (float): Dropout rate.

    """

    def __init__(self, idim: int, odim: int, dropout_rate: float,
                 pos_enc_class: torch.nn.Module):
        """Construct an Conv2dSubsampling8 object."""
        super().__init__()
        self.conv = torch.nn.Sequential(
            torch.nn.Conv2d(1, odim, 3, 2),
            torch.nn.ReLU(),
            torch.nn.Conv2d(odim, odim, 3, 2),
            torch.nn.ReLU(),
            torch.nn.Conv2d(odim, odim, 3, 2),
            torch.nn.ReLU(),
        )
        self.linear = torch.nn.Linear(
            odim * ((((idim - 1) // 2 - 1) // 2 - 1) // 2), odim)
        self.pos_enc = pos_enc_class
        self.subsampling_rate = 8
        # 14 = (3 - 1) * 1 + (3 - 1) * 2 + (3 - 1) * 4
        self.right_context = 14

    def forward(
        self,
        x: torch.Tensor,
        x_mask: torch.Tensor,
        offset: Union[int, torch.Tensor] = 0
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Subsample x.

        Args:
            x (torch.Tensor): Input tensor (#batch, time, idim).
            x_mask (torch.Tensor): Input mask (#batch, 1, time).

        Returns:
            torch.Tensor: Subsampled tensor (#batch, time', odim),
                where time' = time // 8.
            torch.Tensor: Subsampled mask (#batch, 1, time'),
                where time' = time // 8.
            torch.Tensor: positional encoding
        """
        x = x.unsqueeze(1)  # (b, c, t, f)
        x = self.conv(x)
        b, c, t, f = x.size()
        x = self.linear(x.transpose(1, 2).contiguous().view(b, t, c * f))
        x, pos_emb = self.pos_enc(x, offset)
        return x, pos_emb, x_mask[:, :, 2::2][:, :, 2::2][:, :, 2::2]


class LegacyLinearNoSubsampling(BaseSubsampling):
    """Linear transform the input without subsampling

    Args:
        idim (int): Input dimension.
        odim (int): Output dimension.
        dropout_rate (float): Dropout rate.

    """

    def __init__(self, idim: int, odim: int, dropout_rate: float,
                 pos_enc_class: torch.nn.Module):
        """Construct an linear object."""
        super().__init__()
        self.out = torch.nn.Sequential(
            torch.nn.Linear(idim, odim),
            torch.nn.LayerNorm(odim, eps=1e-5),
            torch.nn.Dropout(dropout_rate),
            torch.nn.ReLU(),
        )
        self.pos_enc = pos_enc_class
        self.right_context = 0
        self.subsampling_rate = 1

    def forward(
        self,
        x: torch.Tensor,
        x_mask: torch.Tensor,
        offset: Union[int, torch.Tensor] = 0
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Input x.

        Args:
            x (torch.Tensor): Input tensor (#batch, time, idim).
            x_mask (torch.Tensor): Input mask (#batch, 1, time).

        Returns:
            torch.Tensor: linear input tensor (#batch, time', odim),
                where time' = time .
            torch.Tensor: linear input mask (#batch, 1, time'),
                where time' = time .

        """
        x = self.out(x)
        x, pos_emb = self.pos_enc(x, offset)
        return x, pos_emb, x_mask
</file>

<file path="src/chatterbox/models/s3gen/transformer/upsample_encoder.py">
# Copyright (c) 2021 Mobvoi Inc (Binbin Zhang, Di Wu)
#               2022 Xingchen Song (sxc19@mails.tsinghua.edu.cn)
#               2024 Alibaba Inc (Xiang Lyu)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Modified from ESPnet(https://github.com/espnet/espnet)
"""Encoder definition."""
from typing import Tuple

import torch
from torch import nn
from torch.nn import functional as F

from .convolution import ConvolutionModule
from .encoder_layer import ConformerEncoderLayer
from .positionwise_feed_forward import PositionwiseFeedForward
from ..utils.class_utils import (
    COSYVOICE_EMB_CLASSES,
    COSYVOICE_SUBSAMPLE_CLASSES,
    COSYVOICE_ATTENTION_CLASSES,
    COSYVOICE_ACTIVATION_CLASSES,
)
from ..utils.mask import make_pad_mask
from ..utils.mask import add_optional_chunk_mask


class Upsample1D(nn.Module):
    """A 1D upsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        use_conv_transpose (`bool`, default `False`):
            option to use a convolution transpose.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
    """

    def __init__(self, channels: int, out_channels: int, stride: int = 2):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels
        self.stride = stride
        # In this mode, first repeat interpolate, than conv with stride=1
        self.conv = nn.Conv1d(self.channels, self.out_channels, stride * 2 + 1, stride=1, padding=0)

    def forward(self, inputs: torch.Tensor, input_lengths: torch.Tensor):
        outputs = F.interpolate(inputs, scale_factor=float(self.stride), mode="nearest")
        outputs = F.pad(outputs, (self.stride * 2, 0), value=0.0)
        outputs = self.conv(outputs)
        return outputs, input_lengths * self.stride


class PreLookaheadLayer(nn.Module):
    def __init__(self, channels: int, pre_lookahead_len: int = 1):
        super().__init__()
        self.channels = channels
        self.pre_lookahead_len = pre_lookahead_len
        self.conv1 = nn.Conv1d(
            channels, channels,
            kernel_size=pre_lookahead_len + 1,
            stride=1, padding=0,
        )
        self.conv2 = nn.Conv1d(
            channels, channels,
            kernel_size=3, stride=1, padding=0,
        )

    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        """
        inputs: (batch_size, seq_len, channels)
        """
        outputs = inputs.transpose(1, 2).contiguous()
        # look ahead
        outputs = F.pad(outputs, (0, self.pre_lookahead_len), mode='constant', value=0.0)
        outputs = F.leaky_relu(self.conv1(outputs))
        # outputs
        outputs = F.pad(outputs, (2, 0), mode='constant', value=0.0)
        outputs = self.conv2(outputs)
        outputs = outputs.transpose(1, 2).contiguous()

        # residual connection
        outputs = outputs + inputs
        return outputs


class UpsampleConformerEncoder(torch.nn.Module):

    def __init__(
        self,
        input_size: int = 512,
        output_size: int = 512,
        attention_heads: int = 8,
        linear_units: int = 2048,
        num_blocks: int = 6,
        dropout_rate: float = 0.1,
        positional_dropout_rate: float = 0.1,
        attention_dropout_rate: float = 0.1,
        input_layer: str = "linear",
        pos_enc_layer_type: str = "rel_pos_espnet",
        normalize_before: bool = True,
        static_chunk_size: int = 0,
        use_dynamic_chunk: bool = False,
        global_cmvn: torch.nn.Module = None,
        use_dynamic_left_chunk: bool = False,
        positionwise_conv_kernel_size: int = 1,
        macaron_style: bool = False,
        selfattention_layer_type: str = "rel_selfattn",
        activation_type: str = "swish",
        use_cnn_module: bool = False,
        cnn_module_kernel: int = 15,
        causal: bool = False,
        cnn_module_norm: str = "batch_norm",
        key_bias: bool = True,
        gradient_checkpointing: bool = False,
    ):
        """
        Args:
            input_size (int): input dim
            output_size (int): dimension of attention
            attention_heads (int): the number of heads of multi head attention
            linear_units (int): the hidden units number of position-wise feed
                forward
            num_blocks (int): the number of decoder blocks
            dropout_rate (float): dropout rate
            attention_dropout_rate (float): dropout rate in attention
            positional_dropout_rate (float): dropout rate after adding
                positional encoding
            input_layer (str): input layer type.
                optional [linear, conv2d, conv2d6, conv2d8]
            pos_enc_layer_type (str): Encoder positional encoding layer type.
                opitonal [abs_pos, scaled_abs_pos, rel_pos, no_pos]
            normalize_before (bool):
                True: use layer_norm before each sub-block of a layer.
                False: use layer_norm after each sub-block of a layer.
            static_chunk_size (int): chunk size for static chunk training and
                decoding
            use_dynamic_chunk (bool): whether use dynamic chunk size for
                training or not, You can only use fixed chunk(chunk_size > 0)
                or dyanmic chunk size(use_dynamic_chunk = True)
            global_cmvn (Optional[torch.nn.Module]): Optional GlobalCMVN module
            use_dynamic_left_chunk (bool): whether use dynamic left chunk in
                dynamic chunk training
            key_bias: whether use bias in attention.linear_k, False for whisper models.
            gradient_checkpointing: rerunning a forward-pass segment for each
                checkpointed segment during backward.
        """
        super().__init__()
        self._output_size = output_size

        self.global_cmvn = global_cmvn
        self.embed = COSYVOICE_SUBSAMPLE_CLASSES[input_layer](
            input_size,
            output_size,
            dropout_rate,
            COSYVOICE_EMB_CLASSES[pos_enc_layer_type](output_size,
                                                      positional_dropout_rate),
        )

        self.normalize_before = normalize_before
        self.after_norm = torch.nn.LayerNorm(output_size, eps=1e-5)
        self.static_chunk_size = static_chunk_size
        self.use_dynamic_chunk = use_dynamic_chunk
        self.use_dynamic_left_chunk = use_dynamic_left_chunk
        self.gradient_checkpointing = gradient_checkpointing
        activation = COSYVOICE_ACTIVATION_CLASSES[activation_type]()
        # self-attention module definition
        encoder_selfattn_layer_args = (
            attention_heads,
            output_size,
            attention_dropout_rate,
            key_bias,
        )
        # feed-forward module definition
        positionwise_layer_args = (
            output_size,
            linear_units,
            dropout_rate,
            activation,
        )
        # convolution module definition
        convolution_layer_args = (output_size, cnn_module_kernel, activation,
                                  cnn_module_norm, causal)
        self.pre_lookahead_layer = PreLookaheadLayer(channels=512, pre_lookahead_len=3)
        self.encoders = torch.nn.ModuleList([
            ConformerEncoderLayer(
                output_size,
                COSYVOICE_ATTENTION_CLASSES[selfattention_layer_type](
                    *encoder_selfattn_layer_args),
                PositionwiseFeedForward(*positionwise_layer_args),
                PositionwiseFeedForward(
                    *positionwise_layer_args) if macaron_style else None,
                ConvolutionModule(
                    *convolution_layer_args) if use_cnn_module else None,
                dropout_rate,
                normalize_before,
            ) for _ in range(num_blocks)
        ])
        self.up_layer = Upsample1D(channels=512, out_channels=512, stride=2)
        self.up_embed = COSYVOICE_SUBSAMPLE_CLASSES[input_layer](
            input_size,
            output_size,
            dropout_rate,
            COSYVOICE_EMB_CLASSES[pos_enc_layer_type](output_size,
                                                      positional_dropout_rate),
        )
        self.up_encoders = torch.nn.ModuleList([
            ConformerEncoderLayer(
                output_size,
                COSYVOICE_ATTENTION_CLASSES[selfattention_layer_type](
                    *encoder_selfattn_layer_args),
                PositionwiseFeedForward(*positionwise_layer_args),
                PositionwiseFeedForward(
                    *positionwise_layer_args) if macaron_style else None,
                ConvolutionModule(
                    *convolution_layer_args) if use_cnn_module else None,
                dropout_rate,
                normalize_before,
            ) for _ in range(4)
        ])

    def output_size(self) -> int:
        return self._output_size

    def forward(
        self,
        xs: torch.Tensor,
        xs_lens: torch.Tensor,
        decoding_chunk_size: int = 0,
        num_decoding_left_chunks: int = -1,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Embed positions in tensor.

        Args:
            xs: padded input tensor (B, T, D)
            xs_lens: input length (B)
            decoding_chunk_size: decoding chunk size for dynamic chunk
                0: default for training, use random dynamic chunk.
                <0: for decoding, use full chunk.
                >0: for decoding, use fixed chunk size as set.
            num_decoding_left_chunks: number of left chunks, this is for decoding,
            the chunk size is decoding_chunk_size.
                >=0: use num_decoding_left_chunks
                <0: use all left chunks
        Returns:
            encoder output tensor xs, and subsampled masks
            xs: padded output tensor (B, T' ~= T/subsample_rate, D)
            masks: torch.Tensor batch padding mask after subsample
                (B, 1, T' ~= T/subsample_rate)
        NOTE(xcsong):
            We pass the `__call__` method of the modules instead of `forward` to the
            checkpointing API because `__call__` attaches all the hooks of the module.
            https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2
        """
        T = xs.size(1)
        masks = ~make_pad_mask(xs_lens, T).unsqueeze(1)  # (B, 1, T)
        if self.global_cmvn is not None:
            xs = self.global_cmvn(xs)
        xs, pos_emb, masks = self.embed(xs, masks)
        mask_pad = masks  # (B, 1, T/subsample_rate)
        chunk_masks = add_optional_chunk_mask(xs, masks,
                                              self.use_dynamic_chunk,
                                              self.use_dynamic_left_chunk,
                                              decoding_chunk_size,
                                              self.static_chunk_size,
                                              num_decoding_left_chunks)
        # lookahead + conformer encoder
        xs = self.pre_lookahead_layer(xs)
        xs = self.forward_layers(xs, chunk_masks, pos_emb, mask_pad)

        # upsample + conformer encoder
        xs = xs.transpose(1, 2).contiguous()
        xs, xs_lens = self.up_layer(xs, xs_lens)
        xs = xs.transpose(1, 2).contiguous()
        T = xs.size(1)
        masks = ~make_pad_mask(xs_lens, T).unsqueeze(1)  # (B, 1, T)
        xs, pos_emb, masks = self.up_embed(xs, masks)
        mask_pad = masks  # (B, 1, T/subsample_rate)
        chunk_masks = add_optional_chunk_mask(xs, masks,
                                              self.use_dynamic_chunk,
                                              self.use_dynamic_left_chunk,
                                              decoding_chunk_size,
                                              self.static_chunk_size * self.up_layer.stride,
                                              num_decoding_left_chunks)
        xs = self.forward_up_layers(xs, chunk_masks, pos_emb, mask_pad)

        if self.normalize_before:
            xs = self.after_norm(xs)
        # Here we assume the mask is not changed in encoder layers, so just
        # return the masks before encoder layers, and the masks will be used
        # for cross attention with decoder later
        return xs, masks

    def forward_layers(self, xs: torch.Tensor, chunk_masks: torch.Tensor,
                       pos_emb: torch.Tensor,
                       mask_pad: torch.Tensor) -> torch.Tensor:
        for layer in self.encoders:
            xs, chunk_masks, _, _ = layer(xs, chunk_masks, pos_emb, mask_pad)
        return xs

    def forward_up_layers(self, xs: torch.Tensor, chunk_masks: torch.Tensor,
                          pos_emb: torch.Tensor,
                          mask_pad: torch.Tensor) -> torch.Tensor:
        for layer in self.up_encoders:
            xs, chunk_masks, _, _ = layer(xs, chunk_masks, pos_emb, mask_pad)
        return xs
</file>

<file path="src/chatterbox/models/s3gen/utils/class_utils.py">
# Copyright [2023-11-28] <sxc19@mails.tsinghua.edu.cn, Xingchen Song>
#            2024 Alibaba Inc (authors: Xiang Lyu)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import torch

from ..transformer.activation import Swish
from ..transformer.subsampling import (
    LinearNoSubsampling,
    EmbedinigNoSubsampling,
    Conv1dSubsampling2,
    Conv2dSubsampling4,
    Conv2dSubsampling6,
    Conv2dSubsampling8,
)
from ..transformer.embedding import (
    PositionalEncoding,
    RelPositionalEncoding,
    WhisperPositionalEncoding,
    LearnablePositionalEncoding,
    NoPositionalEncoding)
from ..transformer.attention import (MultiHeadedAttention,
    RelPositionMultiHeadedAttention)
from ..transformer.embedding import EspnetRelPositionalEncoding
from ..transformer.subsampling import LegacyLinearNoSubsampling


COSYVOICE_ACTIVATION_CLASSES = {
    "hardtanh": torch.nn.Hardtanh,
    "tanh": torch.nn.Tanh,
    "relu": torch.nn.ReLU,
    "selu": torch.nn.SELU,
    "swish": getattr(torch.nn, "SiLU", Swish),
    "gelu": torch.nn.GELU,
}

COSYVOICE_SUBSAMPLE_CLASSES = {
    "linear": LinearNoSubsampling,
    "linear_legacy": LegacyLinearNoSubsampling,
    "embed": EmbedinigNoSubsampling,
    "conv1d2": Conv1dSubsampling2,
    "conv2d": Conv2dSubsampling4,
    "conv2d6": Conv2dSubsampling6,
    "conv2d8": Conv2dSubsampling8,
    'paraformer_dummy': torch.nn.Identity
}

COSYVOICE_EMB_CLASSES = {
    "embed": PositionalEncoding,
    "abs_pos": PositionalEncoding,
    "rel_pos": RelPositionalEncoding,
    "rel_pos_espnet": EspnetRelPositionalEncoding,
    "no_pos": NoPositionalEncoding,
    "abs_pos_whisper": WhisperPositionalEncoding,
    "embed_learnable_pe": LearnablePositionalEncoding,
}

COSYVOICE_ATTENTION_CLASSES = {
    "selfattn": MultiHeadedAttention,
    "rel_selfattn": RelPositionMultiHeadedAttention,
}
</file>

<file path="src/chatterbox/models/s3gen/utils/intmeanflow.py">
import torch
import torch.nn as nn


def get_intmeanflow_time_mixer(dims):
    """"
    Diagonal init as described in 3.3 https://arxiv.org/pdf/2510.07979
    """
    layer = nn.Linear(dims * 2, dims, bias=False)

    with torch.no_grad():
        target_weight = torch.zeros(dims, 2 * dims)
        target_weight[:, 0:dims] = torch.eye(dims)
        layer.weight.data = target_weight

    return layer

if __name__ == '__main__':

    D_example = 6

    W_layer = get_intmeanflow_time_mixer(D_example)

    print(f"Layer weight (AFTER init):\n{W_layer.weight.data}\n")

    e_t = torch.tensor([0., 1., 2., 3., 4., 5.])
    e_r = torch.tensor([6., 7., 8., 9., 10., 11.])
    e_concat = torch.cat([e_t, e_r]).unsqueeze(0)  # Shape (1, 12)

    output = W_layer(e_concat)

    print(f"Test Input e_t: \n{e_t}")
    print(f"Test Input e_r: \n{e_r}")
    print(f"Test Input concat: \n{e_concat}")

    print(f"Forward Pass Output: \n{output.squeeze(0)}")
</file>

<file path="src/chatterbox/models/s3gen/xvector.py">
#!/usr/bin/env python3
# -*- encoding: utf-8 -*-
# Copyright FunASR (https://github.com/alibaba-damo-academy/FunASR). All Rights Reserved.
#  MIT License  (https://opensource.org/licenses/MIT)
# Modified from 3D-Speaker (https://github.com/alibaba-damo-academy/3D-Speaker)


from collections import OrderedDict
import torch
import torch.nn.functional as F
import torch.utils.checkpoint as cp
import torchaudio.compliance.kaldi as Kaldi


def pad_list(xs, pad_value):
    """Perform padding for the list of tensors.

    Args:
        xs (List): List of Tensors [(T_1, `*`), (T_2, `*`), ..., (T_B, `*`)].
        pad_value (float): Value for padding.

    Returns:
        Tensor: Padded tensor (B, Tmax, `*`).

    Examples:
        >>> x = [torch.ones(4), torch.ones(2), torch.ones(1)]
        >>> x
        [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])]
        >>> pad_list(x, 0)
        tensor([[1., 1., 1., 1.],
                [1., 1., 0., 0.],
                [1., 0., 0., 0.]])

    """
    n_batch = len(xs)
    max_len = max(x.size(0) for x in xs)
    pad = xs[0].new(n_batch, max_len, *xs[0].size()[1:]).fill_(pad_value)

    for i in range(n_batch):
        pad[i, : xs[i].size(0)] = xs[i]

    return pad


def extract_feature(audio):
    features = []
    feature_times = []
    feature_lengths = []
    for au in audio:
        feature = Kaldi.fbank(au.unsqueeze(0), num_mel_bins=80)
        feature = feature - feature.mean(dim=0, keepdim=True)
        features.append(feature)
        feature_times.append(au.shape[0])
        feature_lengths.append(feature.shape[0])
    # padding for batch inference
    features_padded = pad_list(features, pad_value=0)
    # features = torch.cat(features)
    return features_padded, feature_lengths, feature_times


class BasicResBlock(torch.nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicResBlock, self).__init__()
        self.conv1 = torch.nn.Conv2d(
            in_planes, planes, kernel_size=3, stride=(stride, 1), padding=1, bias=False
        )
        self.bn1 = torch.nn.BatchNorm2d(planes)
        self.conv2 = torch.nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = torch.nn.BatchNorm2d(planes)

        self.shortcut = torch.nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = torch.nn.Sequential(
                torch.nn.Conv2d(
                    in_planes,
                    self.expansion * planes,
                    kernel_size=1,
                    stride=(stride, 1),
                    bias=False,
                ),
                torch.nn.BatchNorm2d(self.expansion * planes),
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class FCM(torch.nn.Module):
    def __init__(self, block=BasicResBlock, num_blocks=[2, 2], m_channels=32, feat_dim=80):
        super(FCM, self).__init__()
        self.in_planes = m_channels
        self.conv1 = torch.nn.Conv2d(1, m_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = torch.nn.BatchNorm2d(m_channels)

        self.layer1 = self._make_layer(block, m_channels, num_blocks[0], stride=2)
        self.layer2 = self._make_layer(block, m_channels, num_blocks[0], stride=2)

        self.conv2 = torch.nn.Conv2d(
            m_channels, m_channels, kernel_size=3, stride=(2, 1), padding=1, bias=False
        )
        self.bn2 = torch.nn.BatchNorm2d(m_channels)
        self.out_channels = m_channels * (feat_dim // 8)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return torch.nn.Sequential(*layers)

    def forward(self, x):
        x = x.unsqueeze(1)
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = F.relu(self.bn2(self.conv2(out)))

        shape = out.shape
        out = out.reshape(shape[0], shape[1] * shape[2], shape[3])
        return out


def get_nonlinear(config_str, channels):
    nonlinear = torch.nn.Sequential()
    for name in config_str.split("-"):
        if name == "relu":
            nonlinear.add_module("relu", torch.nn.ReLU(inplace=True))
        elif name == "prelu":
            nonlinear.add_module("prelu", torch.nn.PReLU(channels))
        elif name == "batchnorm":
            nonlinear.add_module("batchnorm", torch.nn.BatchNorm1d(channels))
        elif name == "batchnorm_":
            nonlinear.add_module("batchnorm", torch.nn.BatchNorm1d(channels, affine=False))
        else:
            raise ValueError("Unexpected module ({}).".format(name))
    return nonlinear


def statistics_pooling(x, dim=-1, keepdim=False, unbiased=True, eps=1e-2):
    mean = x.mean(dim=dim)
    std = x.std(dim=dim, unbiased=unbiased)
    stats = torch.cat([mean, std], dim=-1)
    if keepdim:
        stats = stats.unsqueeze(dim=dim)
    return stats


class StatsPool(torch.nn.Module):
    def forward(self, x):
        return statistics_pooling(x)


class TDNNLayer(torch.nn.Module):
    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size,
        stride=1,
        padding=0,
        dilation=1,
        bias=False,
        config_str="batchnorm-relu",
    ):
        super(TDNNLayer, self).__init__()
        if padding < 0:
            assert (
                kernel_size % 2 == 1
            ), "Expect equal paddings, but got even kernel size ({})".format(kernel_size)
            padding = (kernel_size - 1) // 2 * dilation
        self.linear = torch.nn.Conv1d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            bias=bias,
        )
        self.nonlinear = get_nonlinear(config_str, out_channels)

    def forward(self, x):
        x = self.linear(x)
        x = self.nonlinear(x)
        return x


class CAMLayer(torch.nn.Module):
    def __init__(
        self, bn_channels, out_channels, kernel_size, stride, padding, dilation, bias, reduction=2
    ):
        super(CAMLayer, self).__init__()
        self.linear_local = torch.nn.Conv1d(
            bn_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            bias=bias,
        )
        self.linear1 = torch.nn.Conv1d(bn_channels, bn_channels // reduction, 1)
        self.relu = torch.nn.ReLU(inplace=True)
        self.linear2 = torch.nn.Conv1d(bn_channels // reduction, out_channels, 1)
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x):
        y = self.linear_local(x)
        context = x.mean(-1, keepdim=True) + self.seg_pooling(x)
        context = self.relu(self.linear1(context))
        m = self.sigmoid(self.linear2(context))
        return y * m

    def seg_pooling(self, x, seg_len=100, stype="avg"):
        if stype == "avg":
            seg = F.avg_pool1d(x, kernel_size=seg_len, stride=seg_len, ceil_mode=True)
        elif stype == "max":
            seg = F.max_pool1d(x, kernel_size=seg_len, stride=seg_len, ceil_mode=True)
        else:
            raise ValueError("Wrong segment pooling type.")
        shape = seg.shape
        seg = seg.unsqueeze(-1).expand(*shape, seg_len).reshape(*shape[:-1], -1)
        seg = seg[..., : x.shape[-1]]
        return seg


class CAMDenseTDNNLayer(torch.nn.Module):
    def __init__(
        self,
        in_channels,
        out_channels,
        bn_channels,
        kernel_size,
        stride=1,
        dilation=1,
        bias=False,
        config_str="batchnorm-relu",
        memory_efficient=False,
    ):
        super(CAMDenseTDNNLayer, self).__init__()
        assert kernel_size % 2 == 1, "Expect equal paddings, but got even kernel size ({})".format(
            kernel_size
        )
        padding = (kernel_size - 1) // 2 * dilation
        self.memory_efficient = memory_efficient
        self.nonlinear1 = get_nonlinear(config_str, in_channels)
        self.linear1 = torch.nn.Conv1d(in_channels, bn_channels, 1, bias=False)
        self.nonlinear2 = get_nonlinear(config_str, bn_channels)
        self.cam_layer = CAMLayer(
            bn_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            bias=bias,
        )

    def bn_function(self, x):
        return self.linear1(self.nonlinear1(x))

    def forward(self, x):
        if self.training and self.memory_efficient:
            x = cp.checkpoint(self.bn_function, x)
        else:
            x = self.bn_function(x)
        x = self.cam_layer(self.nonlinear2(x))
        return x


class CAMDenseTDNNBlock(torch.nn.ModuleList):
    def __init__(
        self,
        num_layers,
        in_channels,
        out_channels,
        bn_channels,
        kernel_size,
        stride=1,
        dilation=1,
        bias=False,
        config_str="batchnorm-relu",
        memory_efficient=False,
    ):
        super(CAMDenseTDNNBlock, self).__init__()
        for i in range(num_layers):
            layer = CAMDenseTDNNLayer(
                in_channels=in_channels + i * out_channels,
                out_channels=out_channels,
                bn_channels=bn_channels,
                kernel_size=kernel_size,
                stride=stride,
                dilation=dilation,
                bias=bias,
                config_str=config_str,
                memory_efficient=memory_efficient,
            )
            self.add_module("tdnnd%d" % (i + 1), layer)

    def forward(self, x):
        for layer in self:
            x = torch.cat([x, layer(x)], dim=1)
        return x


class TransitLayer(torch.nn.Module):
    def __init__(self, in_channels, out_channels, bias=True, config_str="batchnorm-relu"):
        super(TransitLayer, self).__init__()
        self.nonlinear = get_nonlinear(config_str, in_channels)
        self.linear = torch.nn.Conv1d(in_channels, out_channels, 1, bias=bias)

    def forward(self, x):
        x = self.nonlinear(x)
        x = self.linear(x)
        return x


class DenseLayer(torch.nn.Module):
    def __init__(self, in_channels, out_channels, bias=False, config_str="batchnorm-relu"):
        super(DenseLayer, self).__init__()
        self.linear = torch.nn.Conv1d(in_channels, out_channels, 1, bias=bias)
        self.nonlinear = get_nonlinear(config_str, out_channels)

    def forward(self, x):
        if len(x.shape) == 2:
            x = self.linear(x.unsqueeze(dim=-1)).squeeze(dim=-1)
        else:
            x = self.linear(x)
        x = self.nonlinear(x)
        return x

# @tables.register("model_classes", "CAMPPlus")
class CAMPPlus(torch.nn.Module):
    def __init__(
        self,
        feat_dim=80,
        embedding_size=192,
        growth_rate=32,
        bn_size=4,
        init_channels=128,
        config_str="batchnorm-relu",
        memory_efficient=True,
        output_level="segment",
        **kwargs,
    ):
        super().__init__()

        self.head = FCM(feat_dim=feat_dim)
        channels = self.head.out_channels
        self.output_level = output_level

        self.xvector = torch.nn.Sequential(
            OrderedDict(
                [
                    (
                        "tdnn",
                        TDNNLayer(
                            channels,
                            init_channels,
                            5,
                            stride=2,
                            dilation=1,
                            padding=-1,
                            config_str=config_str,
                        ),
                    ),
                ]
            )
        )
        channels = init_channels
        for i, (num_layers, kernel_size, dilation) in enumerate(
            zip((12, 24, 16), (3, 3, 3), (1, 2, 2))
        ):
            block = CAMDenseTDNNBlock(
                num_layers=num_layers,
                in_channels=channels,
                out_channels=growth_rate,
                bn_channels=bn_size * growth_rate,
                kernel_size=kernel_size,
                dilation=dilation,
                config_str=config_str,
                memory_efficient=memory_efficient,
            )
            self.xvector.add_module("block%d" % (i + 1), block)
            channels = channels + num_layers * growth_rate
            self.xvector.add_module(
                "transit%d" % (i + 1),
                TransitLayer(channels, channels // 2, bias=False, config_str=config_str),
            )
            channels //= 2

        self.xvector.add_module("out_nonlinear", get_nonlinear(config_str, channels))

        if self.output_level == "segment":
            self.xvector.add_module("stats", StatsPool())
            self.xvector.add_module(
                "dense", DenseLayer(channels * 2, embedding_size, config_str="batchnorm_")
            )
        else:
            assert (
                self.output_level == "frame"
            ), "`output_level` should be set to 'segment' or 'frame'. "

        for m in self.modules():
            if isinstance(m, (torch.nn.Conv1d, torch.nn.Linear)):
                torch.nn.init.kaiming_normal_(m.weight.data)
                if m.bias is not None:
                    torch.nn.init.zeros_(m.bias)

    def forward(self, x):
        x = x.permute(0, 2, 1)  # (B,T,F) => (B,F,T)
        x = self.head(x)
        x = self.xvector(x)
        if self.output_level == "frame":
            x = x.transpose(1, 2)
        return x

    def inference(self, audio_list):
        speech, speech_lengths, speech_times = extract_feature(audio_list)
        results = self.forward(speech.to(torch.float32))
        return results
</file>

<file path="src/chatterbox/models/s3tokenizer/__init__.py">
from .s3tokenizer import (
    S3_SR,
    S3_HOP,
    S3_TOKEN_HOP,
    S3_TOKEN_RATE,
    SPEECH_VOCAB_SIZE,
    S3Tokenizer,
)


SOS = SPEECH_VOCAB_SIZE
EOS = SPEECH_VOCAB_SIZE + 1



def drop_invalid_tokens(x):
    """Drop SoS and EoS"""
    assert len(x.shape) == 1 or (len(x.shape) == 2 and x.shape[0] == 1), "only batch size of one allowed for now"
    if SOS in x:
        s = (x == SOS).nonzero(as_tuple=True)[0].squeeze(0) + 1
    else:
        s = 0

    if EOS in x:
        e = (x == EOS).nonzero(as_tuple=True)[0].squeeze(0)
    else:
        e = None

    x = x[s: e]
    return x
</file>

<file path="src/chatterbox/models/s3tokenizer/s3tokenizer.py">
from typing import List, Tuple

import numpy as np
import librosa
import torch
import torch.nn.functional as F
from s3tokenizer.utils import padding
from s3tokenizer.model_v2 import (
    S3TokenizerV2,
    ModelConfig,
)


# Sampling rate of the inputs to S3TokenizerV2
S3_SR = 16_000
S3_HOP = 160  # 100 frames/sec
S3_TOKEN_HOP = 640  # 25 tokens/sec
S3_TOKEN_RATE = 25
SPEECH_VOCAB_SIZE = 6561


class S3Tokenizer(S3TokenizerV2):
    """
    s3tokenizer.S3TokenizerV2 with the following changes:
    - a more integrated `forward`
    - compute `log_mel_spectrogram` using `_mel_filters` and `window` in `register_buffers`
    """

    ignore_state_dict_missing = ("_mel_filters", "window")

    def __init__(
        self,
        name: str="speech_tokenizer_v2_25hz",
        config: ModelConfig = ModelConfig()
    ):
        super().__init__(name)

        self.n_fft = 400
        _mel_filters = librosa.filters.mel(
            sr=S3_SR,
            n_fft=self.n_fft,
            n_mels=config.n_mels
        )
        self.register_buffer(
            "_mel_filters",
            torch.FloatTensor(_mel_filters),
        )

        self.register_buffer(
            "window",
            torch.hann_window(self.n_fft),
        )

    def pad(self, wavs, sr) -> List[torch.Tensor]:
        """
        Given a list of wavs with the same `sample_rate`, pad them so that the length is multiple of 40ms (S3 runs at 25 token/sec).
        """
        processed_wavs = []
        for wav in wavs:
            if isinstance(wav, np.ndarray):
                wav = torch.from_numpy(wav)
            if wav.dim() == 1:
                wav = wav.unsqueeze(0)

            n_tokens = (wav.shape[1] / sr) * S3_TOKEN_RATE
            n_tokens = np.ceil(n_tokens)
            intended_wav_len = n_tokens * (sr / S3_TOKEN_RATE)
            intended_wav_len = int(intended_wav_len)
            wav = torch.nn.functional.pad(
                wav,
                (0, intended_wav_len - wav.shape[-1]),
                mode="constant",
                value=0
            )
            processed_wavs.append(wav)
        return processed_wavs

    def _prepare_audio(self, wavs):
        """Prepare a list of audios for s3tokenizer processing."""
        processed_wavs = []
        for wav in wavs:
            if isinstance(wav, np.ndarray):
                wav = torch.from_numpy(wav)
            if wav.dim() == 1:
                wav = wav.unsqueeze(0)

            processed_wavs.append(wav)
        return processed_wavs

    @torch.no_grad()
    def forward(
        self,
        wavs: torch.Tensor,
        accelerator: 'Accelerator'=None,
        max_len: int=None,
    ) -> Tuple[torch.Tensor, torch.LongTensor]:
        """
        NOTE: mel-spec has a hop size of 160 points (100 frame/sec).
        FIXME: this class inherits `nn.Module` but doesn't accept `torch.Tensor` and handles a list of wavs one by one, which is unexpected.

        Args
        ----
        - `wavs`: 16 kHz speech audio
        - `max_len` max length to truncate the output sequence to (25 token/sec).
        NOTE: please pad the waveform if longer sequence is needed.
        """
        processed_wavs = self._prepare_audio(wavs)
        mels, mel_lens = [], []
        for wav in processed_wavs:
            wav = wav.to(self.device)
            mel = self.log_mel_spectrogram(wav)  # [B=1, F, T]
            if max_len is not None:
                mel = mel[..., :max_len * 4]  # num_mel_frames = 4 * num_tokens
            mels.append(mel.squeeze(0))

        mels, mel_lens = padding(mels)
        if accelerator is None:
            tokenizer = self
        else:
            tokenizer = accelerator.unwrap_model(self)

        speech_tokens, speech_token_lens = tokenizer.quantize(mels, mel_lens.to(self.device))
        return (
            speech_tokens.long().detach(),
            speech_token_lens.long().detach(),
        )

    def log_mel_spectrogram(
        self,
        audio: torch.Tensor,
        padding: int = 0,
    ):
        """
        Compute the log-Mel spectrogram of

        Parameters
        ----------
        audio: torch.Tensor, shape = (*)
            The path to audio or either a NumPy array or Tensor containing the
            audio waveform in 16 kHz

        padding: int
            Number of zero samples to pad to the right

        Returns
        -------
        torch.Tensor, shape = (128, n_frames)
            A Tensor that contains the Mel spectrogram
        """
        if not torch.is_tensor(audio):
            audio = torch.from_numpy(audio)

        audio = audio.to(self.device)
        if padding > 0:
            audio = F.pad(audio, (0, padding))
        stft = torch.stft(
            audio, self.n_fft, S3_HOP,
            window=self.window.to(self.device),
            return_complex=True
        )
        magnitudes = stft[..., :-1].abs()**2

        mel_spec = self._mel_filters.to(self.device) @ magnitudes

        log_spec = torch.clamp(mel_spec, min=1e-10).log10()
        log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)
        log_spec = (log_spec + 4.0) / 4.0
        return log_spec
</file>

<file path="src/chatterbox/models/t3/__init__.py">
from .t3 import T3
</file>

<file path="src/chatterbox/models/t3/inference/t3_hf_backend.py">
from typing import Optional

import torch
from torch import nn as nn
from transformers import LlamaConfig, LlamaModel, LlamaPreTrainedModel, GenerationMixin
from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions


class T3HuggingfaceBackend(LlamaPreTrainedModel, GenerationMixin):
    """
    Override some HuggingFace interface methods so we can use the standard `generate` method with our
    custom embedding / logit layers.

    NOTE: need to extend "*PreTrainedModel" to avoid re-initializing weights!
    """

    def __init__(
        self,
        config: LlamaConfig,
        llama: LlamaModel,
        *,
        speech_enc,
        speech_head,
        latents_queue=None,
        logits_queue=None,
        alignment_stream_analyzer: 'AlignmentStreamAnalyzer'=None,
    ):
        super().__init__(config)
        self.model = llama
        self.speech_enc = speech_enc
        self.speech_head = speech_head
        self._added_cond = False
        self.alignment_stream_analyzer = alignment_stream_analyzer

    @torch.inference_mode()
    def prepare_inputs_for_generation(
        self, input_ids: torch.Tensor, decoder_cond: torch.Tensor, use_cache: bool, past_key_values=None,
        # This argument was introduced in some recent version of transformers (>=4.29.1)
        cache_position=None
    ):
        """
        This is a method used by huggingface's generate() method.
        Overridden here to apply our custom speech token embedding layer.

        :param input_ids: (B, S) int64 tensors of input tokens.
        :param decoder_cond: (B, T, C) float32 tensor of conditioning (prefixed to <input_embeds>)
        """

        # Make use of the kv cache: only the last input ID is new, we trim away all the ones before
        if not use_cache:
            past_key_values = None
        if past_key_values is not None:
            input_ids = input_ids[:, -1:]

        # custom speech token embedding layer
        inputs_embeds = self.speech_enc(input_ids)

        # prefix decoder conditioning if applicable
        if not self._added_cond:
            assert past_key_values is not None # should be first step
            if decoder_cond.size(0) != inputs_embeds.size(0):
                decoder_cond = decoder_cond.expand(inputs_embeds.size(0), -1, -1)
            inputs_embeds = torch.cat([decoder_cond, inputs_embeds], dim=1)
            self._added_cond = True

        return {
            "inputs_embeds": inputs_embeds,
            "past_key_values": past_key_values,
            "use_cache": use_cache,
        }

    @torch.inference_mode()
    def forward(
        self,
        inputs_embeds: torch.Tensor,
        past_key_values: Optional[torch.Tensor]=None,
        use_cache=True,
        output_attentions=False,
        output_hidden_states=True,
        return_dict=True,
    ):
        """
        This is a method used by huggingface's generate() method.
        Overridden here to apply our custom layer norm and speech logit projection layers.

        :param inputs_embeds: (B, S, C) float32 tensor of conditioning inputs. If past key values are given,
        S should be 1.
        """
        is_large_input = inputs_embeds.size(1) != 1
        has_cache = past_key_values is not None and len(past_key_values) > 0
        assert not (is_large_input and has_cache)
        assert return_dict
        assert output_hidden_states

        tfmr_out = self.model(
            inputs_embeds=inputs_embeds,
            past_key_values=past_key_values,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
        )
        hidden_states = tfmr_out.hidden_states[-1]  # (B, seq, dim)

        logits = self.speech_head(hidden_states)
        # assert inputs_embeds.size(0) == 1 # (disabled for CFG)

        # NOTE: hallucination handler may modify logits to force emit an EOS token
        # logits = self.alignment_stream_analyzer.step(logits)

        return CausalLMOutputWithCrossAttentions(
            logits=logits,
            past_key_values=tfmr_out.past_key_values,
            hidden_states=tfmr_out.hidden_states,
            attentions=tfmr_out.attentions,
        )
</file>

<file path="src/chatterbox/models/t3/modules/cond_enc.py">
from dataclasses import dataclass
from typing import Optional

import torch
from torch import nn, Tensor

from .perceiver import Perceiver
from .t3_config import T3Config


@dataclass
class T3Cond:
    """
    Dataclass container for most / all conditioning info.
    TODO: serialization methods aren't used, keeping them around for convenience
    """

    speaker_emb: Tensor
    clap_emb: Optional[Tensor] = None
    cond_prompt_speech_tokens: Optional[Tensor] = None
    cond_prompt_speech_emb: Optional[Tensor] = None
    emotion_adv: Optional[Tensor] = 0.5

    def to(self, *, device=None, dtype=None):
        "Cast to a device and dtype. Dtype casting is ignored for long/int tensors."
        for k, v in self.__dict__.items():
            if torch.is_tensor(v):
                is_fp = type(v.view(-1)[0].item()) is not int
                setattr(self, k, v.to(device=device, dtype=dtype if is_fp else None))
        return self

    def save(self, fpath):
        torch.save(self.__dict__, fpath)

    @staticmethod
    def load(fpath, map_location="cpu"):
        kwargs = torch.load(fpath, map_location=map_location, weights_only=True)
        return T3Cond(**kwargs)


class T3CondEnc(nn.Module):
    """
    Handle all non-text conditioning, like speaker embeddings / prompts, CLAP, emotion, etc.
    """

    def __init__(self, hp: T3Config):
        super().__init__()
        self.hp = hp
        if hp.encoder_type == "voice_encoder":
            self.spkr_enc = nn.Linear(hp.speaker_embed_size, hp.n_channels)
        else:
            raise NotImplementedError(str(hp.encoder_type))

        # emotion adv
        self.emotion_adv_fc = None
        if hp.emotion_adv:
            self.emotion_adv_fc = nn.Linear(1, hp.n_channels, bias=False)

        # perceiver resampler
        self.perceiver = None
        if hp.use_perceiver_resampler:
            self.perceiver = Perceiver()

    def forward(self, cond: T3Cond):
        # Validate
        assert (cond.cond_prompt_speech_tokens is None) == (cond.cond_prompt_speech_emb is None), \
            "no embeddings for cond_prompt_speech_tokens"

        # Speaker embedding projection
        cond_spkr = self.spkr_enc(cond.speaker_emb.view(-1, self.hp.speaker_embed_size))[:, None]  # (B, 1, dim)
        empty = torch.zeros_like(cond_spkr[:, :0])  # (B, 0, dim)

        # TODO CLAP
        assert cond.clap_emb is None, "clap_embed not implemented"
        cond_clap = empty  # (B, 0, dim)

        # Cond prompt
        cond_prompt_speech_emb = cond.cond_prompt_speech_emb
        if cond_prompt_speech_emb is None:
            cond_prompt_speech_emb = empty  # (B, 0, dim)
        elif self.hp.use_perceiver_resampler:
            cond_prompt_speech_emb = self.perceiver(cond_prompt_speech_emb)

        # Emotion Adv: must provide a value if this model uses emotion conditioning
        cond_emotion_adv = empty  # (B, 0, dim)
        if self.hp.emotion_adv:
            assert cond.emotion_adv is not None
            cond_emotion_adv = self.emotion_adv_fc(cond.emotion_adv.view(-1, 1, 1))

        # Concat and return
        cond_embeds = torch.cat((
            cond_spkr,
            cond_clap,
            cond_prompt_speech_emb,
            cond_emotion_adv,
        ), dim=1)
        return cond_embeds
</file>

<file path="src/chatterbox/models/t3/modules/learned_pos_emb.py">
from typing import Union

import torch
from torch import nn, Tensor


class LearnedPositionEmbeddings(nn.Module):
    def __init__(self, seq_len, model_dim, init=.02):
        super().__init__()
        self.emb = nn.Embedding(seq_len, model_dim)
        # Initializing this way is standard for GPT-2
        self.emb.weight.data.normal_(mean=0.0, std=init)

    def forward(self, x):
        """
        Returns positional embeddings for index 0 up to the length of x
        """
        sl = x.shape[1]
        return self.emb(torch.arange(0, sl, device=x.device))

    def get_fixed_embedding(self, idx: 'Union[int, Tensor]'):
        """
        Args:
            idx: scalar int or an integer tensor of shape (T,) or (B, T)
        Returns:
            positional embeddings for given indices, shape (B, T, dim), ie (1, 1, dim) for int input
        """
        device = self.emb.weight.device
        idx = idx.to(device) if torch.is_tensor(idx) else torch.tensor(idx, device=device)
        idx = torch.atleast_2d(idx)
        assert idx.ndim == 2
        return self.emb(idx)  # (B, T, dim)
</file>

<file path="src/chatterbox/models/t3/modules/perceiver.py">
# Copyright (c) 2025 Resemble AI
# Author: Manmay Nakhashi
# MIT License
import math

import torch
from torch import nn
import torch.nn.functional as F
from einops import rearrange


class RelativePositionBias(nn.Module):
    def __init__(self, scale, causal=False, num_buckets=32, max_distance=128, heads=8):
        super().__init__()
        self.scale = scale
        self.causal = causal
        self.num_buckets = num_buckets
        self.max_distance = max_distance
        self.relative_attention_bias = nn.Embedding(num_buckets, heads)

    @staticmethod
    def _relative_position_bucket(relative_position, causal=True, num_buckets=32, max_distance=128):
        ret = 0
        n = -relative_position
        if not causal:
            num_buckets //= 2
            ret += (n < 0).long() * num_buckets
            n = torch.abs(n)
        else:
            n = torch.max(n, torch.zeros_like(n))

        max_exact = num_buckets // 2
        is_small = n < max_exact

        val_if_large = max_exact + (
                torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)
        ).long()
        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))

        ret += torch.where(is_small, n, val_if_large)
        return ret

    def forward(self, qk_dots):
        i, j, device = *qk_dots.shape[-2:], qk_dots.device
        q_pos = torch.arange(i, dtype=torch.long, device=device)
        k_pos = torch.arange(j, dtype=torch.long, device=device)
        rel_pos = k_pos[None, :] - q_pos[:, None]
        rp_bucket = self._relative_position_bucket(rel_pos, causal=self.causal, num_buckets=self.num_buckets,
                                                   max_distance=self.max_distance)
        values = self.relative_attention_bias(rp_bucket)
        bias = rearrange(values, 'i j h -> () h i j')
        return qk_dots + (bias * self.scale)


class AttentionQKV(nn.Module):
    def __init__(self, n_heads, head_dim, dropout_rate=0.1, scale=None, flash=False):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = head_dim
        self.scale = scale if scale is not None else head_dim ** -0.5
        self.flash = flash
        self.dropout_rate = dropout_rate
        self.dropout = nn.Dropout(dropout_rate)
        self.flash_config = self.setup_flash_config() if flash else None

    def setup_flash_config(self):
        # Setup flash attention configuration
        flash_config = {
            'enable_flash': True,
            'enable_math': True,
            'enable_mem_efficient': True
        }
        return flash_config

    def forward(self, q, k, v, mask=None):
        q, k, v = [self.split_heads(tensor) for tensor in [q, k, v]]
        if self.flash:
            out = self.flash_attention(q, k, v, mask=mask)
        else:
            out = self.scaled_dot_product_attention(q, k, v, mask=mask)

        return self.combine_heads(out)

    def scaled_dot_product_attention(self, q, k, v, mask=None):
        sim = torch.einsum("bhlt,bhls->bhts", q, k) * self.scale
        if mask is not None:
            sim = sim.masked_fill(mask == 0, float('-inf'))
        attn = torch.softmax(sim, dim=-1)
        attn = self.dropout(attn)
        return torch.einsum("bhts,bhls->bhlt", attn, v)

    def flash_attention(self, q, k, v, mask=None):
        config = self.flash_config if self.flash_config else {}
        with torch.backends.cuda.sdp_kernel(**config):
            out = F.scaled_dot_product_attention(
                q, k, v,
                attn_mask=mask,
                dropout_p=self.dropout_rate if self.training else 0.
            )
        return out

    def split_heads(self, x):
        bs, length, _ = x.shape
        x = x.view(bs, length, self.n_heads, self.head_dim)
        return x.permute(0, 2, 1, 3)

    def combine_heads(self, x):
        bs, _, length, _ = x.shape
        x = x.permute(0, 2, 1, 3).contiguous()
        return x.view(bs, length, -1)


class AttentionBlock2(nn.Module):
    """
    An attention block that allows spatial positions to attend to each other,
    using AttentionQKV and separate linear transformations for Q, K, and V.
    """

    def __init__(
        self,
        channels,
        num_heads=1,
        num_head_channels=-1,
        relative_pos_embeddings=False,
        flash_attention=True,
        dropout_rate=0.2,
        scale=None
    ):
        super().__init__()
        self.channels = channels

        if num_head_channels == -1:
            self.num_heads = num_heads
        else:
            assert (
                channels % num_head_channels == 0
            ), f"channels {channels} is not divisible by num_head_channels {num_head_channels}"
            self.num_heads = channels // num_head_channels

        self.norm = nn.LayerNorm(channels)

        # Separate linear layers for Q, K, and V
        self.to_q = nn.Linear(channels, channels)
        self.to_k = nn.Linear(channels, channels)
        self.to_v = nn.Linear(channels, channels)

        self.attention = AttentionQKV(self.num_heads, channels // self.num_heads, dropout_rate=dropout_rate, flash=flash_attention, scale=scale)

        self.proj_out = nn.Linear(channels, channels)

        if relative_pos_embeddings:
            self.relative_pos_embeddings = RelativePositionBias(scale=(channels // self.num_heads) ** .5, causal=False, heads=num_heads, num_buckets=32, max_distance=64)
        else:
            self.relative_pos_embeddings = None

    def forward(self, x1, x2, mask=None):
        b1, c1, *spatial1 = x1.shape
        b2, c2, *spatial2 = x2.shape

        x1_norm = self.norm(x1)
        x2_norm = self.norm(x2)

        q = self.to_q(x1_norm)
        k = self.to_k(x2_norm)
        v = self.to_v(x2_norm)

        h = self.attention(q, k, v, mask=mask)
        h = self.proj_out(h)

        return (x1 + h).reshape(b1, c1, *spatial1)


class Perceiver(nn.Module):
    """Inspired by https://arxiv.org/abs/2103.03206"""
    def __init__(self, pre_attention_query_token=32, pre_attention_query_size=1024, embedding_dim=1024, num_attn_heads=4):
        """
        Initialize the perceiver module.

        :param pre_attention_query_token: Number of query tokens for pre-attention
        :param pre_attention_query_size: Size of each query token
        :param embedding_dim: Dimension of the embedding space
        :param num_attn_heads: Number of attention heads
        """
        super().__init__()

        # Initialize the pre-attention query parameter
        self.pre_attention_query = torch.nn.Parameter(
            torch.empty(1, pre_attention_query_token, pre_attention_query_size)
        )

        # Calculate the variance for uniform initialization
        query_variance = math.sqrt(3.0) * math.sqrt(2.0 / (pre_attention_query_token + pre_attention_query_token))

        # Initialize the pre-attention query with uniform distribution
        self.pre_attention_query.data.uniform_(-query_variance, query_variance)

        # Initialize the attention block
        self.attn = AttentionBlock2(embedding_dim, num_attn_heads)

    def forward(self, h):
        """
        Forward pass of the perceiver module.
        :param h: Input tensor
        :return: Output after applying attention mechanisms
        """
        # Expand the pre-attention query to match the batch size of the input
        query_ = self.pre_attention_query.expand(h.shape[0], -1, -1)
        # Apply the first attention mechanism (cross-attention)
        pre_att = self.attn(query_, h)
        # Apply the second attention mechanism (self-attention)
        attn = self.attn(pre_att, pre_att)
        return attn
</file>

<file path="src/chatterbox/models/utils.py">
class AttrDict(dict):
    def __init__(self, *args, **kwargs):
        super(AttrDict, self).__init__(*args, **kwargs)
        self.__dict__ = self
</file>

<file path="src/chatterbox/models/voice_encoder/__init__.py">
from .voice_encoder import VoiceEncoder, VoiceEncConfig
</file>

<file path="src/chatterbox/models/voice_encoder/config.py">
class VoiceEncConfig:
    num_mels = 40
    sample_rate = 16000
    speaker_embed_size = 256
    ve_hidden_size = 256
    flatten_lstm_params = False
    n_fft = 400
    hop_size = 160
    win_size = 400
    fmax = 8000
    fmin = 0
    preemphasis = 0.
    mel_power = 2.0
    mel_type = "amp"
    normalized_mels = False
    ve_partial_frames = 160
    ve_final_relu = True
    stft_magnitude_min = 1e-4
</file>

<file path="src/chatterbox/models/voice_encoder/melspec.py">
from functools import lru_cache

from scipy import signal
import numpy as np
import librosa


@lru_cache()
def mel_basis(hp):
    assert hp.fmax <= hp.sample_rate // 2
    return librosa.filters.mel(
        sr=hp.sample_rate,
        n_fft=hp.n_fft,
        n_mels=hp.num_mels,
        fmin=hp.fmin,
        fmax=hp.fmax)  # -> (nmel, nfreq)


def preemphasis(wav, hp):
    assert hp.preemphasis != 0
    wav = signal.lfilter([1, -hp.preemphasis], [1], wav)
    wav = np.clip(wav, -1, 1)
    return wav


def melspectrogram(wav, hp, pad=True):
    # Run through pre-emphasis
    if hp.preemphasis > 0:
        wav = preemphasis(wav, hp)
        assert np.abs(wav).max() - 1 < 1e-07

    # Do the stft
    spec_complex = _stft(wav, hp, pad=pad)

    # Get the magnitudes
    spec_magnitudes = np.abs(spec_complex)

    if hp.mel_power != 1.0:
        spec_magnitudes **= hp.mel_power

    # Get the mel and convert magnitudes->db
    mel = np.dot(mel_basis(hp), spec_magnitudes)
    if hp.mel_type == "db":
        mel = _amp_to_db(mel, hp)

    # Normalise the mel from db to 0,1
    if hp.normalized_mels:
        mel = _normalize(mel, hp).astype(np.float32)

    assert not pad or mel.shape[1] == 1 + len(wav) // hp.hop_size   # Sanity check
    return mel   # (M, T)


def _stft(y, hp, pad=True):
    # NOTE: after 0.8, pad mode defaults to constant, setting this to reflect for
    #   historical consistency and streaming-version consistency
    return librosa.stft(
        y,
        n_fft=hp.n_fft,
        hop_length=hp.hop_size,
        win_length=hp.win_size,
        center=pad,
        pad_mode="reflect",
    )


def _amp_to_db(x, hp):
    return 20 * np.log10(np.maximum(hp.stft_magnitude_min, x))


def _db_to_amp(x):
    return np.power(10.0, x * 0.05)


def _normalize(s, hp, headroom_db=15):
    min_level_db = 20 * np.log10(hp.stft_magnitude_min)
    s = (s - min_level_db) / (-min_level_db + headroom_db)
    return s
</file>

<file path="src/chatterbox/models/voice_encoder/voice_encoder.py">
# Adapted from https://github.com/CorentinJ/Real-Time-Voice-Cloning
# MIT License
from typing import List, Union, Optional

import numpy as np
from numpy.lib.stride_tricks import as_strided
import librosa
import torch
import torch.nn.functional as F
from torch import nn, Tensor

from .config import VoiceEncConfig
from .melspec import melspectrogram


def pack(arrays, seq_len: int=None, pad_value=0):
    """
    Given a list of length B of array-like objects of shapes (Ti, ...), packs them in a single tensor of
    shape (B, T, ...) by padding each individual array on the right.

    :param arrays: a list of array-like objects of matching shapes except for the first axis.
    :param seq_len: the value of T. It must be the maximum of the lengths Ti of the arrays at
    minimum. Will default to that value if None.
    :param pad_value: the value to pad the arrays with.
    :return: a (B, T, ...) tensor
    """
    if seq_len is None:
        seq_len = max(len(array) for array in arrays)
    else:
        assert seq_len >= max(len(array) for array in arrays)

    # Convert lists to np.array
    if isinstance(arrays[0], list):
        arrays = [np.array(array) for array in arrays]

    # Convert to tensor and handle device
    device = None
    if isinstance(arrays[0], torch.Tensor):
        tensors = arrays
        device = tensors[0].device
    else:
        tensors = [torch.as_tensor(array) for array in arrays]

    # Fill the packed tensor with the array data
    packed_shape = (len(tensors), seq_len, *tensors[0].shape[1:])
    packed_tensor = torch.full(packed_shape, pad_value, dtype=tensors[0].dtype, device=device)

    for i, tensor in enumerate(tensors):
        packed_tensor[i, :tensor.size(0)] = tensor

    return packed_tensor


def get_num_wins(
    n_frames: int,
    step: int,
    min_coverage: float,
    hp: VoiceEncConfig,
):
    assert n_frames > 0
    win_size = hp.ve_partial_frames
    n_wins, remainder = divmod(max(n_frames - win_size + step, 0), step)
    if n_wins == 0 or (remainder + (win_size - step)) / win_size >= min_coverage:
        n_wins += 1
    target_n = win_size + step * (n_wins - 1)
    return n_wins, target_n


def get_frame_step(
    overlap: float,
    rate: float,
    hp: VoiceEncConfig,
):
    # Compute how many frames separate two partial utterances
    assert 0 <= overlap < 1
    if rate is None:
        frame_step = int(np.round(hp.ve_partial_frames * (1 - overlap)))
    else:
        frame_step = int(np.round((hp.sample_rate / rate) / hp.ve_partial_frames))
    assert 0 < frame_step <= hp.ve_partial_frames
    return frame_step


def stride_as_partials(
    mel: np.ndarray,
    hp: VoiceEncConfig,
    overlap=0.5,
    rate: float=None,
    min_coverage=0.8,
):
    """
    Takes unscaled mels in (T, M) format
    TODO: doc
    """
    assert 0 < min_coverage <= 1
    frame_step = get_frame_step(overlap, rate, hp)

    # Compute how many partials can fit in the mel
    n_partials, target_len = get_num_wins(len(mel), frame_step, min_coverage, hp)

    # Trim or pad the mel spectrogram to match the number of partials
    if target_len > len(mel):
        mel = np.concatenate((mel, np.full((target_len - len(mel), hp.num_mels), 0)))
    elif target_len < len(mel):
        mel = mel[:target_len]

    # Ensure the numpy array data is float32 and contiguous in memory
    mel = mel.astype(np.float32, order="C")

    # Re-arrange the array in memory to be of shape (N, P, M) with partials overlapping eachother,
    # where N is the number of partials, P is the number of frames of each partial and M the
    # number of channels of the mel spectrograms.
    shape = (n_partials, hp.ve_partial_frames, hp.num_mels)
    strides = (mel.strides[0] * frame_step, mel.strides[0], mel.strides[1])
    partials = as_strided(mel, shape, strides)
    return partials


class VoiceEncoder(nn.Module):
    def __init__(self, hp=VoiceEncConfig()):
        super().__init__()

        self.hp = hp

        # Network definition
        self.lstm = nn.LSTM(self.hp.num_mels, self.hp.ve_hidden_size, num_layers=3, batch_first=True)
        if hp.flatten_lstm_params:
            self.lstm.flatten_parameters()
        self.proj = nn.Linear(self.hp.ve_hidden_size, self.hp.speaker_embed_size)

        # Cosine similarity scaling (fixed initial parameter values)
        self.similarity_weight = nn.Parameter(torch.tensor([10.]), requires_grad=True)
        self.similarity_bias = nn.Parameter(torch.tensor([-5.]), requires_grad=True)

    @property
    def device(self):
        return next(self.parameters()).device

    def forward(self, mels: torch.FloatTensor):
        """
        Computes the embeddings of a batch of partial utterances.

        :param mels: a batch of unscaled mel spectrograms of same duration as a float32 tensor
        of shape (B, T, M) where T is hp.ve_partial_frames
        :return: the embeddings as a float32 tensor of shape (B, E) where E is
        hp.speaker_embed_size. Embeddings are L2-normed and thus lay in the range [-1, 1].
        """
        if self.hp.normalized_mels and (mels.min() < 0 or mels.max() > 1):
            raise Exception(f"Mels outside [0, 1]. Min={mels.min()}, Max={mels.max()}")

        # Pass the input through the LSTM layers
        _, (hidden, _) = self.lstm(mels)

        # Project the final hidden state
        raw_embeds = self.proj(hidden[-1])
        if self.hp.ve_final_relu:
            raw_embeds = F.relu(raw_embeds)

        # L2 normalize the embeddings.
        return raw_embeds / torch.linalg.norm(raw_embeds, dim=1, keepdim=True)

    def inference(self, mels: torch.Tensor, mel_lens, overlap=0.5, rate: float=None, min_coverage=0.8, batch_size=None):
        """
        Computes the embeddings of a batch of full utterances with gradients.

        :param mels: (B, T, M) unscaled mels
        :return: (B, E) embeddings on CPU
        """
        mel_lens = mel_lens.tolist() if torch.is_tensor(mel_lens) else mel_lens

        # Compute where to split the utterances into partials
        frame_step = get_frame_step(overlap, rate, self.hp)
        n_partials, target_lens = zip(*(get_num_wins(l, frame_step, min_coverage, self.hp) for l in mel_lens))

        # Possibly pad the mels to reach the target lengths
        len_diff = max(target_lens) - mels.size(1)
        if len_diff > 0:
            pad = torch.full((mels.size(0), len_diff, self.hp.num_mels), 0, dtype=torch.float32)
            mels = torch.cat((mels, pad.to(mels.device)), dim=1)

        # Group all partials together so that we can batch them easily
        partials = [
            mel[i * frame_step: i * frame_step + self.hp.ve_partial_frames]
            for mel, n_partial in zip(mels, n_partials) for i in range(n_partial)
        ]
        assert all(partials[0].shape == partial.shape for partial in partials)
        partials = torch.stack(partials)

        # Forward the partials
        n_chunks = int(np.ceil(len(partials) / (batch_size or len(partials))))
        partial_embeds = torch.cat([self(batch) for batch in partials.chunk(n_chunks)], dim=0).cpu()

        # Reduce the partial embeds into full embeds and L2-normalize them
        slices = np.concatenate(([0], np.cumsum(n_partials)))
        raw_embeds = [torch.mean(partial_embeds[start:end], dim=0) for start, end in zip(slices[:-1], slices[1:])]
        raw_embeds = torch.stack(raw_embeds)
        embeds = raw_embeds / torch.linalg.norm(raw_embeds, dim=1, keepdim=True)

        return embeds

    @staticmethod
    def utt_to_spk_embed(utt_embeds: np.ndarray):
        """
        Takes an array of L2-normalized utterance embeddings, computes the mean embedding and L2-normalize it to get a
        speaker embedding.
        """
        assert utt_embeds.ndim == 2
        utt_embeds = np.mean(utt_embeds, axis=0)
        return utt_embeds / np.linalg.norm(utt_embeds, 2)

    @staticmethod
    def voice_similarity(embeds_x: np.ndarray, embeds_y: np.ndarray):
        """
        Cosine similarity for L2-normalized utterance embeddings or speaker embeddings
        """
        embeds_x = embeds_x if embeds_x.ndim == 1 else VoiceEncoder.utt_to_spk_embed(embeds_x)
        embeds_y = embeds_y if embeds_y.ndim == 1 else VoiceEncoder.utt_to_spk_embed(embeds_y)
        return embeds_x @ embeds_y

    def embeds_from_mels(
        self, mels: Union[Tensor, List[np.ndarray]], mel_lens=None, as_spk=False, batch_size=32, **kwargs
    ):
        """
        Convenience function for deriving utterance or speaker embeddings from mel spectrograms.

        :param mels: unscaled mels strictly within [0, 1] as either a (B, T, M) tensor or a list of (Ti, M) arrays.
        :param mel_lens: if passing mels as a tensor, individual mel lengths
        :param as_spk: whether to return utterance embeddings or a single speaker embedding
        :param kwargs: args for inference()

        :returns: embeds as a (B, E) float32 numpy array if <as_spk> is False, else as a (E,) array
        """
        # Load mels in memory and pack them
        if isinstance(mels, List):
            mels = [np.asarray(mel) for mel in mels]
            assert all(m.shape[1] == mels[0].shape[1] for m in mels), "Mels aren't in (B, T, M) format"
            mel_lens = [mel.shape[0] for mel in mels]
            mels = pack(mels)

        # Embed them
        with torch.inference_mode():
            utt_embeds = self.inference(mels.to(self.device), mel_lens, batch_size=batch_size, **kwargs).numpy()

        return self.utt_to_spk_embed(utt_embeds) if as_spk else utt_embeds

    def embeds_from_wavs(
        self,
        wavs: List[np.ndarray],
        sample_rate,
        as_spk=False,
        batch_size=32,
        trim_top_db: Optional[float]=20,
        **kwargs
    ):
        """
        Wrapper around embeds_from_mels

        :param trim_top_db: this argument was only added for the sake of compatibility with metavoice's implementation
        """
        if sample_rate != self.hp.sample_rate:
            wavs = [
                librosa.resample(wav, orig_sr=sample_rate, target_sr=self.hp.sample_rate, res_type="kaiser_fast")
                for wav in wavs
            ]

        if trim_top_db:
            wavs = [librosa.effects.trim(wav, top_db=trim_top_db)[0] for wav in wavs]

        if "rate" not in kwargs:
            kwargs["rate"] = 1.3  # Resemble's default value.

        mels = [melspectrogram(w, self.hp).T for w in wavs]

        return self.embeds_from_mels(mels, as_spk=as_spk, batch_size=batch_size, **kwargs)
</file>

<file path="src/chatterbox/tts_turbo.py">
import os
import math
from dataclasses import dataclass
from pathlib import Path

import librosa
import torch
import perth
import pyloudnorm as ln

from safetensors.torch import load_file
from huggingface_hub import snapshot_download
from transformers import AutoTokenizer

from .models.t3 import T3
from .models.s3tokenizer import S3_SR
from .models.s3gen import S3GEN_SR, S3Gen
from .models.tokenizers import EnTokenizer
from .models.voice_encoder import VoiceEncoder
from .models.t3.modules.cond_enc import T3Cond
from .models.t3.modules.t3_config import T3Config
from .models.s3gen.const import S3GEN_SIL
import logging
logger = logging.getLogger(__name__)

REPO_ID = "ResembleAI/chatterbox-turbo"


def punc_norm(text: str) -> str:
    """
        Quick cleanup func for punctuation from LLMs or
        containing chars not seen often in the dataset
    """
    if len(text) == 0:
        return "You need to add some text for me to talk."

    # Capitalise first letter
    if text[0].islower():
        text = text[0].upper() + text[1:]

    # Remove multiple space chars
    text = " ".join(text.split())

    # Replace uncommon/llm punc
    punc_to_replace = [
        ("…", ", "),
        (":", ","),
        ("—", "-"),
        ("–", "-"),
        (" ,", ","),
        ("“", "\""),
        ("”", "\""),
        ("‘", "'"),
        ("’", "'"),
    ]
    for old_char_sequence, new_char in punc_to_replace:
        text = text.replace(old_char_sequence, new_char)

    # Add full stop if no ending punc
    text = text.rstrip(" ")
    sentence_enders = {".", "!", "?", "-", ","}
    if not any(text.endswith(p) for p in sentence_enders):
        text += "."

    return text


@dataclass
class Conditionals:
    """
    Conditionals for T3 and S3Gen
    - T3 conditionals:
        - speaker_emb
        - clap_emb
        - cond_prompt_speech_tokens
        - cond_prompt_speech_emb
        - emotion_adv
    - S3Gen conditionals:
        - prompt_token
        - prompt_token_len
        - prompt_feat
        - prompt_feat_len
        - embedding
    """
    t3: T3Cond
    gen: dict

    def to(self, device):
        self.t3 = self.t3.to(device=device)
        for k, v in self.gen.items():
            if torch.is_tensor(v):
                self.gen[k] = v.to(device=device)
        return self

    def save(self, fpath: Path):
        arg_dict = dict(
            t3=self.t3.__dict__,
            gen=self.gen
        )
        torch.save(arg_dict, fpath)

    @classmethod
    def load(cls, fpath, map_location="cpu"):
        if isinstance(map_location, str):
            map_location = torch.device(map_location)
        kwargs = torch.load(fpath, map_location=map_location, weights_only=True)
        return cls(T3Cond(**kwargs['t3']), kwargs['gen'])


class ChatterboxTurboTTS:
    ENC_COND_LEN = 15 * S3_SR
    DEC_COND_LEN = 10 * S3GEN_SR

    def __init__(
        self,
        t3: T3,
        s3gen: S3Gen,
        ve: VoiceEncoder,
        tokenizer: EnTokenizer,
        device: str,
        conds: Conditionals = None,
    ):
        self.sr = S3GEN_SR  # sample rate of synthesized audio
        self.t3 = t3
        self.s3gen = s3gen
        self.ve = ve
        self.tokenizer = tokenizer
        self.device = device
        self.conds = conds
        self.watermarker = perth.PerthImplicitWatermarker()

    @classmethod
    def from_local(cls, ckpt_dir, device) -> 'ChatterboxTurboTTS':
        ckpt_dir = Path(ckpt_dir)

        # Always load to CPU first for non-CUDA devices to handle CUDA-saved models
        if device in ["cpu", "mps"]:
            map_location = torch.device('cpu')
        else:
            map_location = None

        ve = VoiceEncoder()
        ve.load_state_dict(
            load_file(ckpt_dir / "ve.safetensors")
        )
        ve.to(device).eval()

        # Turbo specific hp
        hp = T3Config(text_tokens_dict_size=50276)
        hp.llama_config_name = "GPT2_medium"
        hp.speech_tokens_dict_size = 6563
        hp.input_pos_emb = None
        hp.speech_cond_prompt_len = 375
        hp.use_perceiver_resampler = False
        hp.emotion_adv = False

        t3 = T3(hp)
        t3_state = load_file(ckpt_dir / "t3_turbo_v1.safetensors")
        if "model" in t3_state.keys():
            t3_state = t3_state["model"][0]
        t3.load_state_dict(t3_state)
        del t3.tfmr.wte
        t3.to(device).eval()

        s3gen = S3Gen(meanflow=True)
        weights = load_file(ckpt_dir / "s3gen_meanflow.safetensors")
        s3gen.load_state_dict(
            weights, strict=True
        )
        s3gen.to(device).eval()

        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        if len(tokenizer) != 50276:
            print(f"WARNING: Tokenizer len {len(tokenizer)} != 50276")

        conds = None
        builtin_voice = ckpt_dir / "conds.pt"
        if builtin_voice.exists():
            conds = Conditionals.load(builtin_voice, map_location=map_location).to(device)

        return cls(t3, s3gen, ve, tokenizer, device, conds=conds)

    @classmethod
    def from_pretrained(cls, device) -> 'ChatterboxTurboTTS':
        # Check if MPS is available on macOS
        if device == "mps" and not torch.backends.mps.is_available():
            if not torch.backends.mps.is_built():
                print("MPS not available because the current PyTorch install was not built with MPS enabled.")
            else:
                print("MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.")
            device = "cpu"

        local_path = snapshot_download(
            repo_id=REPO_ID,
            token=os.getenv("HF_TOKEN") or True,
            # Optional: Filter to download only what you need
            allow_patterns=["*.safetensors", "*.json", "*.txt", "*.pt", "*.model"]
        )

        return cls.from_local(local_path, device)

    def norm_loudness(self, wav, sr, target_lufs=-27):
        try:
            meter = ln.Meter(sr)
            loudness = meter.integrated_loudness(wav)
            gain_db = target_lufs - loudness
            gain_linear = 10.0 ** (gain_db / 20.0)
            if math.isfinite(gain_linear) and gain_linear > 0.0:
                wav = wav * gain_linear
        except Exception as e:
            print(f"Warning: Error in norm_loudness, skipping: {e}")

        return wav

    def prepare_conditionals(self, wav_fpath, exaggeration=0.5, norm_loudness=True):
        ## Load and norm reference wav
        s3gen_ref_wav, _sr = librosa.load(wav_fpath, sr=S3GEN_SR)

        assert len(s3gen_ref_wav) / _sr > 5.0, "Audio prompt must be longer than 5 seconds!"

        if norm_loudness:
            s3gen_ref_wav = self.norm_loudness(s3gen_ref_wav, _sr)

        ref_16k_wav = librosa.resample(s3gen_ref_wav, orig_sr=S3GEN_SR, target_sr=S3_SR)

        s3gen_ref_wav = s3gen_ref_wav[:self.DEC_COND_LEN]
        s3gen_ref_dict = self.s3gen.embed_ref(s3gen_ref_wav, S3GEN_SR, device=self.device)

        # Speech cond prompt tokens
        if plen := self.t3.hp.speech_cond_prompt_len:
            s3_tokzr = self.s3gen.tokenizer
            t3_cond_prompt_tokens, _ = s3_tokzr.forward([ref_16k_wav[:self.ENC_COND_LEN]], max_len=plen)
            t3_cond_prompt_tokens = torch.atleast_2d(t3_cond_prompt_tokens).to(self.device)

        # Voice-encoder speaker embedding
        ve_embed = torch.from_numpy(self.ve.embeds_from_wavs([ref_16k_wav], sample_rate=S3_SR))
        ve_embed = ve_embed.mean(axis=0, keepdim=True).to(self.device)

        t3_cond = T3Cond(
            speaker_emb=ve_embed,
            cond_prompt_speech_tokens=t3_cond_prompt_tokens,
            emotion_adv=exaggeration * torch.ones(1, 1, 1),
        ).to(device=self.device)
        self.conds = Conditionals(t3_cond, s3gen_ref_dict)

    def generate(
        self,
        text,
        repetition_penalty=1.2,
        min_p=0.00,
        top_p=0.95,
        audio_prompt_path=None,
        exaggeration=0.0,
        cfg_weight=0.0,
        temperature=0.8,
        top_k=1000,
        norm_loudness=True,
    ):
        if audio_prompt_path:
            self.prepare_conditionals(audio_prompt_path, exaggeration=exaggeration, norm_loudness=norm_loudness)
        else:
            assert self.conds is not None, "Please `prepare_conditionals` first or specify `audio_prompt_path`"

        if cfg_weight > 0.0 or exaggeration > 0.0 or min_p > 0.0:
            logger.warning("CFG, min_p and exaggeration are not supported by Turbo version and will be ignored.")

        # Norm and tokenize text
        text = punc_norm(text)
        text_tokens = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        text_tokens = text_tokens.input_ids.to(self.device)

        speech_tokens = self.t3.inference_turbo(
            t3_cond=self.conds.t3,
            text_tokens=text_tokens,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
        )

        # Remove OOV tokens and add silence to end
        speech_tokens = speech_tokens[speech_tokens < 6561]
        speech_tokens = speech_tokens.to(self.device)
        silence = torch.tensor([S3GEN_SIL, S3GEN_SIL, S3GEN_SIL]).long().to(self.device)
        speech_tokens = torch.cat([speech_tokens, silence])

        wav, _ = self.s3gen.inference(
            speech_tokens=speech_tokens,
            ref_dict=self.conds.gen,
            n_cfm_timesteps=2,
        )
        wav = wav.squeeze(0).detach().cpu().numpy()
        watermarked_wav = self.watermarker.apply_watermark(wav, sample_rate=self.sr)
        return torch.from_numpy(watermarked_wav).unsqueeze(0)
</file>

<file path="candle/examples/test_vocoder.rs">
use candle_core::{DType, Device};
use candle_nn::VarBuilder;

fn main() -> anyhow::Result<()> {
    let device = Device::Cpu;

    // 1. Load debug tensors
    let debug_path = "debug_tensors.safetensors";
    if !std::path::Path::new(debug_path).exists() {
        anyhow::bail!("debug_tensors.safetensors not found. Run extract_debug.py first.");
    }

    let tensors = candle_core::safetensors::load(debug_path, &device)?;
    let ref_mel = tensors
        .get("ref_mel")
        .ok_or_else(|| anyhow::anyhow!("ref_mel not found"))?
        .clone();
    let ref_audio = tensors
        .get("ref_audio")
        .ok_or_else(|| anyhow::anyhow!("ref_audio not found"))?
        .clone();

    println!("Loaded ground truth mel: {:?}", ref_mel.dims());

    // 2. Load HiFiGAN
    let model_path = "C:/Users/Steve Business/.cache/huggingface/hub/models--ResembleAI--chatterbox-turbo/snapshots/749d1c1a46eb10492095d68fbcf55691ccf137cd/s3gen_meanflow.safetensors";
    let vb = unsafe { VarBuilder::from_mmaped_safetensors(&[model_path], DType::F32, &device)? };

    let config = candle::hifigan::HiFTConfig {
        in_channels: 80,
        base_channels: 512,
        nb_harmonics: 8,
        sampling_rate: 24000,
        upsample_rates: vec![8, 5, 3],
        upsample_kernel_sizes: vec![16, 11, 7],
        resblock_kernel_sizes: vec![3, 7, 11],
        resblock_dilation_sizes: vec![vec![1, 3, 5], vec![1, 3, 5], vec![1, 3, 5]],
        n_fft: 16,
        hop_len: 2,
    };

    let vocoder = candle::hifigan::HiFTGenerator::new(config, vb.pp("mel2wav"))?;
    println!("Vocoder loaded.");

    // 3. Run inference (with B=1)
    let mel_input = ref_mel.unsqueeze(0)?;
    println!("Running vocoder inference on mel {:?}...", mel_input.dims());
    let audio = vocoder.inference(&mel_input)?;
    println!("Inference complete: {:?}", audio.dims());

    // 4. Save output
    let samples = audio.flatten_all()?.to_vec1::<f32>()?;
    candle::audio::save_wav("test_vocoder_isolated.wav", &samples, 24000)
        .map_err(|e| anyhow::anyhow!(e))?;
    println!("Saved test_vocoder_isolated.wav");

    // 5. Verify Mel Extraction (Rust)
    let ref_audio_vec = ref_audio.flatten_all()?.to_vec1::<f32>()?;
    let rust_mel = candle::audio::compute_mel_spectrogram(
        &ref_audio_vec,
        24000,
        &device,
        &candle::audio::MelConfig::for_24k(80),
    )?;

    println!("Rust mel shape: {:?}", rust_mel.dims());

    // Compare first few values: ref_mel is [80, T], rust_mel is [1, 80, T]
    let gt_val = ref_mel.get(0)?.get(0)?.to_vec0::<f32>()?;
    let rust_val = rust_mel.get(0)?.get(0)?.get(0)?.to_vec0::<f32>()?;
    println!(
        "Comparison mel[0,0]: GT={:.6}, Rust={:.6}",
        gt_val, rust_val
    );

    Ok(())
}
</file>

<file path="candle/examples/verify_shapes.rs">
use candle_core::{DType, Device, Tensor};
use candle_nn::VarBuilder;

/// Comprehensive shape verification test for all pipeline components
/// Tests each component's forward pass with dummy tensors to catch shape mismatches
/// before running full inference.
fn main() -> anyhow::Result<()> {
    let device = Device::Cpu;
    let model_dir = "C:/Users/Steve Business/.cache/huggingface/hub/models--ResembleAI--chatterbox-turbo/snapshots/749d1c1a46eb10492095d68fbcf55691ccf137cd";

    println!("=== COMPREHENSIVE SHAPE VERIFICATION TEST ===\n");

    // Sample dimensions
    let batch = 1;
    let time_frames = 100; // ~4 seconds at 25fps token rate
    let num_mels_40 = 40;
    let num_mels_80 = 80;

    // =====================================================
    // TEST 1: VoiceEncoder (ve.safetensors)
    // =====================================================
    println!("--- TEST 1: VoiceEncoder ---");
    let ve_path = format!("{}/ve.safetensors", model_dir);
    let vb_ve = unsafe { VarBuilder::from_mmaped_safetensors(&[&ve_path], DType::F32, &device)? };
    let ve = candle::voice_encoder::VoiceEncoder::new(
        candle::voice_encoder::VoiceEncoderConfig::default(),
        vb_ve,
    )?;

    // VoiceEncoder expects: (B, T, 40) - time-major format
    let mel_40_input = Tensor::randn(0f32, 1f32, (batch, time_frames, num_mels_40), &device)?;
    println!("  Input: {:?}", mel_40_input.dims());

    match ve.forward(&mel_40_input) {
        Ok(out) => println!("  ✓ Output: {:?} (expected [1, 256])", out.dims()),
        Err(e) => println!("  ✗ Error: {}", e),
    }

    // =====================================================
    // TEST 2: CAMPPlus (speaker_encoder in s3gen_meanflow.safetensors)
    // =====================================================
    println!("\n--- TEST 2: CAMPPlus ---");
    let s3gen_path = format!("{}/s3gen_meanflow.safetensors", model_dir);
    let vb_s3 =
        unsafe { VarBuilder::from_mmaped_safetensors(&[&s3gen_path], DType::F32, &device)? };
    let campplus = candle::campplus::CAMPPlus::new(80, 192, vb_s3.pp("speaker_encoder"))?;

    // CAMPPlus expects: (B, C, T) = (B, 80, T) - channel-first format
    let mel_80_input = Tensor::randn(0f32, 1f32, (batch, num_mels_80, time_frames), &device)?;
    println!("  Input: {:?}", mel_80_input.dims());

    match campplus.forward(&mel_80_input) {
        Ok(out) => println!("  ✓ Output: {:?} (expected [1, 192])", out.dims()),
        Err(e) => println!("  ✗ Error: {}", e),
    }

    // =====================================================
    // TEST 3: S3Tokenizer (from local s3tokenizer-v2-model)
    // =====================================================
    println!("\n--- TEST 3: S3TokenizerV2 ---");
    let s3tok_path = "D:/chatterbox-rs/s3tokenizer-v2-model/model.safetensors";
    if std::path::Path::new(s3tok_path).exists() {
        let vb_s3tok =
            unsafe { VarBuilder::from_mmaped_safetensors(&[s3tok_path], DType::F32, &device)? };
        let s3tokenizer = candle::s3tokenizer::S3TokenizerV2::new(
            &candle::s3tokenizer::ModelConfig::default(),
            vb_s3tok,
        )?;

        // S3Tokenizer expects: (B, 128, T) mel spectrogram (padded 80->128)
        let mel_128_input = Tensor::randn(0f32, 1f32, (batch, 128, time_frames), &device)?;
        println!("  Input: {:?}", mel_128_input.dims());

        match s3tokenizer.encode(&mel_128_input) {
            Ok(out) => println!("  ✓ Output tokens: {:?}", out.dims()),
            Err(e) => println!("  ✗ Error: {}", e),
        }
    } else {
        println!("  (Skipped - model not found at {})", s3tok_path);
    }

    // =====================================================
    // TEST 4: S3Gen Flow (flow.* in s3gen_meanflow.safetensors)
    // =====================================================
    println!("\n--- TEST 4: S3Gen ---");
    let s3gen = candle::s3gen::S3Gen::new(vb_s3.clone(), true)?;

    // S3Gen.forward expects speech_tokens: (B, T) and spks: Option<(B, 80)>
    let speech_tokens = Tensor::from_vec(vec![100u32; 20], (batch, 20), &device)?;
    let spk_emb = Tensor::randn(0f32, 1f32, (batch, 80), &device)?;
    println!("  Speech tokens: {:?}", speech_tokens.dims());
    println!("  Speaker embedding: {:?}", spk_emb.dims());

    let dummy_mel_80 = Tensor::randn(0f32, 1f32, (batch, 80, 20), &device)?;
    match s3gen.forward(&speech_tokens, Some(&spk_emb), Some(&dummy_mel_80)) {
        Ok(out) => println!("  ✓ Output: {:?}", out.dims()),
        Err(e) => println!("  ✗ Error: {}", e),
    }

    // =====================================================
    // TEST 5: HiFiGAN vocoder (mel2wav.* in s3gen_meanflow.safetensors)
    // =====================================================
    println!("\n--- TEST 5: HiFiGAN ---");
    let hifigan_config = candle::hifigan::HiFTConfig {
        in_channels: 80,
        base_channels: 512,
        nb_harmonics: 8,
        sampling_rate: 24000,
        upsample_rates: vec![8, 5, 3],
        upsample_kernel_sizes: vec![16, 11, 7],
        resblock_kernel_sizes: vec![3, 7, 11],
        resblock_dilation_sizes: vec![vec![1, 3, 5], vec![1, 3, 5], vec![1, 3, 5]],
        n_fft: 16,
        hop_len: 4,
    };
    let hifigan = candle::hifigan::HiFTGenerator::new(hifigan_config, vb_s3.pp("mel2wav"))?;

    // HiFiGAN expects: (B, 80, T) mel spectrogram
    let mel_for_vocoder = Tensor::randn(0f32, 1f32, (batch, 80, 40), &device)?;
    println!("  Input mel: {:?}", mel_for_vocoder.dims());

    match hifigan.inference(&mel_for_vocoder) {
        Ok(out) => println!("  ✓ Output audio: {:?}", out.dims()),
        Err(e) => println!("  ✗ Error: {}", e),
    }

    // =====================================================
    // TEST 6: T3 model (t3_turbo_v1.safetensors)
    // =====================================================
    println!("\n--- TEST 6: T3 ---");
    let t3_path = format!("{}/t3_turbo_v1.safetensors", model_dir);
    let vb_t3 = unsafe { VarBuilder::from_mmaped_safetensors(&[&t3_path], DType::F32, &device)? };
    let t3_config = candle::t3_model::T3Config {
        text_tokens_dict_size: 50276,
        speech_tokens_dict_size: 6563,
        hidden_size: 1024,
        num_layers: 24,
        num_heads: 16,
        vocab_size: 50276,
        speaker_embed_size: 256,
        start_speech_token: 6561,
        stop_speech_token: 6562,
        speech_cond_prompt_len: Some(375),
        use_perceiver_resampler: false,
        emotion_adv: false,
        n_positions: 8196,
    };
    let t3 = candle::t3_model::T3::new(t3_config, vb_t3)?;

    // T3 inputs
    let text_tokens = Tensor::from_vec(vec![100u32; 10], (batch, 10), &device)?;
    let speaker_emb = Tensor::randn(0f32, 1f32, (batch, 256), &device)?;
    let prompt_tokens = Tensor::from_vec(vec![100u32; 50], (batch, 50), &device)?;
    println!("  Text tokens: {:?}", text_tokens.dims());
    println!("  Speaker embedding: {:?}", speaker_emb.dims());
    println!("  Prompt tokens: {:?}", prompt_tokens.dims());

    match t3.generate(
        &text_tokens,
        &speaker_emb,
        Some(&prompt_tokens),
        None,
        10, // max_new_tokens (small for test)
        0.8,
        0.95,
        50,
        1.2,
        42,
    ) {
        Ok(out) => println!("  ✓ Generated tokens: {:?}", out.dims()),
        Err(e) => println!("  ✗ Error: {}", e),
    }

    println!("\n=== SHAPE VERIFICATION COMPLETE ===");
    Ok(())
}
</file>

<file path="candle/src/sampling.rs">
use candle_core::{DType, Result, Tensor};
use rand::{distributions::Distribution, thread_rng};

pub struct LogitsProcessor {
    rng: rand::rngs::ThreadRng,
    temperature: Option<f64>,
    top_p: Option<f64>,
    top_k: Option<usize>,
    min_p: Option<f64>,
}

impl LogitsProcessor {
    pub fn new(
        _seed: u64,
        temperature: Option<f64>,
        top_p: Option<f64>,
        top_k: Option<usize>,
    ) -> Self {
        Self::new_with_min_p(_seed, temperature, top_p, top_k, None)
    }

    pub fn new_with_min_p(
        _seed: u64,
        temperature: Option<f64>,
        top_p: Option<f64>,
        top_k: Option<usize>,
        min_p: Option<f64>,
    ) -> Self {
        // In a real implementation we might use a seeded RNG, but thread_rng is fine for now
        // if the user provided a seed, we could use StdRng::seed_from_u64(seed)
        Self {
            rng: thread_rng(),
            temperature,
            top_p,
            top_k,
            min_p,
        }
    }

    pub fn apply_repetition_penalty(&self, logits: &mut Vec<f32>, tokens: &[u32], penalty: f32) {
        if penalty == 1.0 {
            return;
        }
        let mut seen = std::collections::HashSet::new();
        for &t in tokens {
            if seen.insert(t) {
                let logit = logits[t as usize];
                if logit < 0.0 {
                    logits[t as usize] = logit * penalty;
                } else {
                    logits[t as usize] = logit / penalty;
                }
            }
        }
    }

    pub fn sample(
        &mut self,
        logits: &Tensor,
        prev_tokens: &[u32],
        repetition_penalty: Option<f32>,
    ) -> Result<u32> {
        let logits = logits.to_dtype(DType::F32)?;
        let device = logits.device();
        let mut logits_v: Vec<f32> = logits.to_vec1()?;

        // 0. Repetition penalty
        if let Some(penalty) = repetition_penalty {
            self.apply_repetition_penalty(&mut logits_v, prev_tokens, penalty);
        }

        // 1. Temperature scaling
        if let Some(temp) = self.temperature {
            if temp > 0.0 && temp != 1.0 {
                for l in logits_v.iter_mut() {
                    *l /= temp as f32;
                }
            }
        }

        // 2. Top-K filtering
        if let Some(k) = self.top_k {
            if k > 0 && k < logits_v.len() {
                let mut indexed_logits: Vec<(usize, f32)> =
                    logits_v.iter().enumerate().map(|(i, &l)| (i, l)).collect();
                indexed_logits.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());

                let min_kept_logit = indexed_logits[k - 1].1;
                for l in logits_v.iter_mut() {
                    if *l < min_kept_logit {
                        *l = f32::NEG_INFINITY;
                    }
                }
            }
        }

        // Softmax
        let logits_t = Tensor::from_vec(logits_v, (logits.dim(0)?,), device)?;
        let probs = candle_nn::ops::softmax(&logits_t, 0)?;
        let mut probs_v: Vec<f32> = probs.to_vec1()?;

        // 3. Min-P filtering: zero out probabilities below max_prob * min_p threshold
        if let Some(min_p_threshold) = self.min_p {
            if min_p_threshold > 0.0 {
                let max_prob = probs_v.iter().cloned().fold(0.0f32, f32::max);
                let threshold = max_prob * min_p_threshold as f32;
                for p in probs_v.iter_mut() {
                    if *p < threshold {
                        *p = 0.0;
                    }
                }
                // Re-normalize
                let sum: f32 = probs_v.iter().sum();
                if sum > 0.0 {
                    for p in probs_v.iter_mut() {
                        *p /= sum;
                    }
                }
            }
        }

        // 3. Top-P (Nucleus) filtering (applied to probabilities)
        let sampled_idx = if let Some(p) = self.top_p {
            if p > 0.0 && p < 1.0 {
                let mut indexed_probs: Vec<(usize, f32)> =
                    probs_v.iter().enumerate().map(|(i, &pr)| (i, pr)).collect();
                indexed_probs.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());

                let mut cumulative_prob = 0.0;
                let mut cutoff_idx = indexed_probs.len();
                for (i, (_, pr)) in indexed_probs.iter().enumerate() {
                    cumulative_prob += pr;
                    if cumulative_prob > p as f32 {
                        cutoff_idx = i + 1;
                        break;
                    }
                }

                // Re-normalize top-p
                let top_indexed = &indexed_probs[..cutoff_idx];
                let sum: f32 = top_indexed.iter().map(|(_, pr)| pr).sum();

                let dist = rand::distributions::WeightedIndex::new(
                    top_indexed.iter().map(|(_, pr)| pr / sum),
                )
                .unwrap();
                top_indexed[dist.sample(&mut self.rng)].0
            } else {
                self.simple_sample(&probs_v)
            }
        } else {
            self.simple_sample(&probs_v)
        };

        Ok(sampled_idx as u32)
    }

    fn simple_sample(&mut self, probs: &[f32]) -> usize {
        let dist = rand::distributions::WeightedIndex::new(probs).unwrap();
        dist.sample(&mut self.rng)
    }
}
</file>

<file path="example_vc.py">
import torch
import torchaudio as ta

from chatterbox.vc import ChatterboxVC

# Automatically detect the best available device
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"

print(f"Using device: {device}")

AUDIO_PATH = "YOUR_FILE.wav"
TARGET_VOICE_PATH = "YOUR_FILE.wav"

model = ChatterboxVC.from_pretrained(device)
wav = model.generate(
    audio=AUDIO_PATH,
    target_voice_path=TARGET_VOICE_PATH,
)
ta.save("testvc.wav", wav, model.sr)
</file>

<file path="multilingual_app.py">
import random
import numpy as np
import torch
from chatterbox.mtl_tts import ChatterboxMultilingualTTS, SUPPORTED_LANGUAGES
import gradio as gr

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"🚀 Running on device: {DEVICE}")

# --- Global Model Initialization ---
MODEL = None

LANGUAGE_CONFIG = {
    "ar": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ar_f/ar_prompts2.flac",
        "text": "في الشهر الماضي، وصلنا إلى معلم جديد بمليارين من المشاهدات على قناتنا على يوتيوب."
    },
    "da": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/da_m1.flac",
        "text": "Sidste måned nåede vi en ny milepæl med to milliarder visninger på vores YouTube-kanal."
    },
    "de": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/de_f1.flac",
        "text": "Letzten Monat haben wir einen neuen Meilenstein erreicht: zwei Milliarden Aufrufe auf unserem YouTube-Kanal."
    },
    "el": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/el_m.flac",
        "text": "Τον περασμένο μήνα, φτάσαμε σε ένα νέο ορόσημο με δύο δισεκατομμύρια προβολές στο κανάλι μας στο YouTube."
    },
    "en": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/en_f1.flac",
        "text": "Last month, we reached a new milestone with two billion views on our YouTube channel."
    },
    "es": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/es_f1.flac",
        "text": "El mes pasado alcanzamos un nuevo hito: dos mil millones de visualizaciones en nuestro canal de YouTube."
    },
    "fi": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/fi_m.flac",
        "text": "Viime kuussa saavutimme uuden virstanpylvään kahden miljardin katselukerran kanssa YouTube-kanavallamme."
    },
    "fr": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/fr_f1.flac",
        "text": "Le mois dernier, nous avons atteint un nouveau jalon avec deux milliards de vues sur notre chaîne YouTube."
    },
    "he": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/he_m1.flac",
        "text": "בחודש שעבר הגענו לאבן דרך חדשה עם שני מיליארד צפיות בערוץ היוטיוב שלנו."
    },
    "hi": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/hi_f1.flac",
        "text": "पिछले महीने हमने एक नया मील का पत्थर छुआ: हमारे YouTube चैनल पर दो अरब व्यूज़।"
    },
    "it": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/it_m1.flac",
        "text": "Il mese scorso abbiamo raggiunto un nuovo traguardo: due miliardi di visualizzazioni sul nostro canale YouTube."
    },
    "ja": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ja/ja_prompts1.flac",
        "text": "先月、私たちのYouTubeチャンネルで二十億回の再生回数という新たなマイルストーンに到達しました。"
    },
    "ko": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ko_f.flac",
        "text": "지난달 우리는 유튜브 채널에서 이십억 조회수라는 새로운 이정표에 도달했습니다."
    },
    "ms": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ms_f.flac",
        "text": "Bulan lepas, kami mencapai pencapaian baru dengan dua bilion tontonan di saluran YouTube kami."
    },
    "nl": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/nl_m.flac",
        "text": "Vorige maand bereikten we een nieuwe mijlpaal met twee miljard weergaven op ons YouTube-kanaal."
    },
    "no": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/no_f1.flac",
        "text": "Forrige måned nådde vi en ny milepæl med to milliarder visninger på YouTube-kanalen vår."
    },
    "pl": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/pl_m.flac",
        "text": "W zeszłym miesiącu osiągnęliśmy nowy kamień milowy z dwoma miliardami wyświetleń na naszym kanale YouTube."
    },
    "pt": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/pt_m1.flac",
        "text": "No mês passado, alcançámos um novo marco: dois mil milhões de visualizações no nosso canal do YouTube."
    },
    "ru": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ru_m.flac",
        "text": "В прошлом месяце мы достигли нового рубежа: два миллиарда просмотров на нашем YouTube-канале."
    },
    "sv": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/sv_f.flac",
        "text": "Förra månaden nådde vi en ny milstolpe med två miljarder visningar på vår YouTube-kanal."
    },
    "sw": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/sw_m.flac",
        "text": "Mwezi uliopita, tulifika hatua mpya ya maoni ya bilioni mbili kweny kituo chetu cha YouTube."
    },
    "tr": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/tr_m.flac",
        "text": "Geçen ay YouTube kanalımızda iki milyar görüntüleme ile yeni bir dönüm noktasına ulaştık."
    },
    "zh": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/zh_f2.flac",
        "text": "上个月，我们达到了一个新的里程碑. 我们的YouTube频道观看次数达到了二十亿次，这绝对令人难以置信。"
    },
}

# --- UI Helpers ---
def default_audio_for_ui(lang: str) -> str | None:
    return LANGUAGE_CONFIG.get(lang, {}).get("audio")


def default_text_for_ui(lang: str) -> str:
    return LANGUAGE_CONFIG.get(lang, {}).get("text", "")


def get_supported_languages_display() -> str:
    """Generate a formatted display of all supported languages."""
    language_items = []
    for code, name in sorted(SUPPORTED_LANGUAGES.items()):
        language_items.append(f"**{name}** (`{code}`)")
    
    # Split into 2 lines
    mid = len(language_items) // 2
    line1 = " • ".join(language_items[:mid])
    line2 = " • ".join(language_items[mid:])
    
    return f"""
### 🌍 Supported Languages ({len(SUPPORTED_LANGUAGES)} total)
{line1}

{line2}
"""


def get_or_load_model():
    """Loads the ChatterboxMultilingualTTS model if it hasn't been loaded already,
    and ensures it's on the correct device."""
    global MODEL
    if MODEL is None:
        print("Model not loaded, initializing...")
        try:
            MODEL = ChatterboxMultilingualTTS.from_pretrained(DEVICE)
            if hasattr(MODEL, 'to') and str(MODEL.device) != DEVICE:
                MODEL.to(DEVICE)
            print(f"Model loaded successfully. Internal device: {getattr(MODEL, 'device', 'N/A')}")
        except Exception as e:
            print(f"Error loading model: {e}")
            raise
    return MODEL

# Attempt to load the model at startup.
try:
    get_or_load_model()
except Exception as e:
    print(f"CRITICAL: Failed to load model on startup. Application may not function. Error: {e}")

def set_seed(seed: int):
    """Sets the random seed for reproducibility across torch, numpy, and random."""
    torch.manual_seed(seed)
    if DEVICE == "cuda":
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    np.random.seed(seed)
    
def resolve_audio_prompt(language_id: str, provided_path: str | None) -> str | None:
    """
    Decide which audio prompt to use:
    - If user provided a path (upload/mic/url), use it.
    - Else, fall back to language-specific default (if any).
    """
    if provided_path and str(provided_path).strip():
        return provided_path
    return LANGUAGE_CONFIG.get(language_id, {}).get("audio")


def generate_tts_audio(
    text_input: str,
    language_id: str,
    audio_prompt_path_input: str = None,
    exaggeration_input: float = 0.5,
    temperature_input: float = 0.8,
    seed_num_input: int = 0,
    cfgw_input: float = 0.5
) -> tuple[int, np.ndarray]:
    """
    Generate high-quality speech audio from text using Chatterbox Multilingual model with optional reference audio styling.
    Supported languages: English, French, German, Spanish, Italian, Portuguese, and Hindi.
    
    This tool synthesizes natural-sounding speech from input text. When a reference audio file 
    is provided, it captures the speaker's voice characteristics and speaking style. The generated audio 
    maintains the prosody, tone, and vocal qualities of the reference speaker, or uses default voice if no reference is provided.

    Args:
        text_input (str): The text to synthesize into speech (maximum 300 characters)
        language_id (str): The language code for synthesis (eg. en, fr, de, es, it, pt, hi)
        audio_prompt_path_input (str, optional): File path or URL to the reference audio file that defines the target voice style. Defaults to None.
        exaggeration_input (float, optional): Controls speech expressiveness (0.25-2.0, neutral=0.5, extreme values may be unstable). Defaults to 0.5.
        temperature_input (float, optional): Controls randomness in generation (0.05-5.0, higher=more varied). Defaults to 0.8.
        seed_num_input (int, optional): Random seed for reproducible results (0 for random generation). Defaults to 0.
        cfgw_input (float, optional): CFG/Pace weight controlling generation guidance (0.2-1.0). Defaults to 0.5, 0 for language transfer. 

    Returns:
        tuple[int, np.ndarray]: A tuple containing the sample rate (int) and the generated audio waveform (numpy.ndarray)
    """
    current_model = get_or_load_model()

    if current_model is None:
        raise RuntimeError("TTS model is not loaded.")

    if seed_num_input != 0:
        set_seed(int(seed_num_input))

    print(f"Generating audio for text: '{text_input[:50]}...'")
    
    # Handle optional audio prompt
    chosen_prompt = audio_prompt_path_input or default_audio_for_ui(language_id)

    generate_kwargs = {
        "exaggeration": exaggeration_input,
        "temperature": temperature_input,
        "cfg_weight": cfgw_input,
    }
    if chosen_prompt:
        generate_kwargs["audio_prompt_path"] = chosen_prompt
        print(f"Using audio prompt: {chosen_prompt}")
    else:
        print("No audio prompt provided; using default voice.")
        
    wav = current_model.generate(
        text_input[:300],  # Truncate text to max chars
        language_id=language_id,
        **generate_kwargs
    )
    print("Audio generation complete.")
    return (current_model.sr, wav.squeeze(0).numpy())

with gr.Blocks() as demo:
    gr.Markdown(
        """
        # Chatterbox Multilingual Demo
        Generate high-quality multilingual speech from text with reference audio styling, supporting 23 languages.
        """
    )
    
    # Display supported languages
    gr.Markdown(get_supported_languages_display())
    with gr.Row():
        with gr.Column():
            initial_lang = "fr"
            text = gr.Textbox(
                value=default_text_for_ui(initial_lang),
                label="Text to synthesize (max chars 300)",
                max_lines=5
            )
            
            language_id = gr.Dropdown(
                choices=list(ChatterboxMultilingualTTS.get_supported_languages().keys()),
                value=initial_lang,
                label="Language",
                info="Select the language for text-to-speech synthesis"
            )
            
            ref_wav = gr.Audio(
                sources=["upload", "microphone"],
                type="filepath",
                label="Reference Audio File (Optional)",
                value=default_audio_for_ui(initial_lang)
            )
            
            gr.Markdown(
                "💡 **Note**: Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clip's language. To mitigate this, set the CFG weight to 0.",
                elem_classes=["audio-note"]
            )
            
            exaggeration = gr.Slider(
                0.25, 2, step=.05, label="Exaggeration (Neutral = 0.5, extreme values can be unstable)", value=.5
            )
            cfg_weight = gr.Slider(
                0.2, 1, step=.05, label="CFG/Pace", value=0.5
            )

            with gr.Accordion("More options", open=False):
                seed_num = gr.Number(value=0, label="Random seed (0 for random)")
                temp = gr.Slider(0.05, 5, step=.05, label="Temperature", value=.8)

            run_btn = gr.Button("Generate", variant="primary")

        with gr.Column():
            audio_output = gr.Audio(label="Output Audio")

        def on_language_change(lang, current_ref, current_text):
            return default_audio_for_ui(lang), default_text_for_ui(lang)

        language_id.change(
            fn=on_language_change,
            inputs=[language_id, ref_wav, text],
            outputs=[ref_wav, text],
            show_progress=False
        )

    run_btn.click(
        fn=generate_tts_audio,
        inputs=[
            text,
            language_id,
            ref_wav,
            exaggeration,
            temp,
            seed_num,
            cfg_weight,
        ],
        outputs=[audio_output],
    )

demo.launch(mcp_server=True)
</file>

<file path="src/chatterbox/models/s3gen/const.py">
S3GEN_SR = 24000
S3GEN_SIL = 4299
</file>

<file path="src/chatterbox/models/s3gen/decoder.py">
# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Zhihao Du)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import pack, rearrange, repeat

from .utils.mask import add_optional_chunk_mask
from .matcha.decoder import SinusoidalPosEmb, Block1D, ResnetBlock1D, Downsample1D, \
    TimestepEmbedding, Upsample1D
from .matcha.transformer import BasicTransformerBlock
from .utils.intmeanflow import get_intmeanflow_time_mixer


def mask_to_bias(mask: torch.Tensor, dtype: torch.dtype) -> torch.Tensor:
    assert mask.dtype == torch.bool
    assert dtype in [torch.float32, torch.bfloat16, torch.float16]
    mask = mask.to(dtype)
    # attention mask bias
    # NOTE(Mddct): torch.finfo jit issues
    #     chunk_masks = (1.0 - chunk_masks) * torch.finfo(dtype).min
    mask = (1.0 - mask) * -1.0e+10
    return mask



class Transpose(torch.nn.Module):
    def __init__(self, dim0: int, dim1: int):
        super().__init__()
        self.dim0 = dim0
        self.dim1 = dim1

    def forward(self, x: torch.Tensor):
        x = torch.transpose(x, self.dim0, self.dim1)
        return x


class CausalBlock1D(Block1D):
    def __init__(self, dim: int, dim_out: int):
        super(CausalBlock1D, self).__init__(dim, dim_out)
        self.block = torch.nn.Sequential(
            CausalConv1d(dim, dim_out, 3),
            Transpose(1, 2),
            nn.LayerNorm(dim_out),
            Transpose(1, 2),
            nn.Mish(),
        )

    def forward(self, x: torch.Tensor, mask: torch.Tensor):
        output = self.block(x * mask)
        return output * mask


class CausalResnetBlock1D(ResnetBlock1D):
    def __init__(self, dim: int, dim_out: int, time_emb_dim: int, groups: int = 8):
        super(CausalResnetBlock1D, self).__init__(dim, dim_out, time_emb_dim, groups)
        self.block1 = CausalBlock1D(dim, dim_out)
        self.block2 = CausalBlock1D(dim_out, dim_out)


class CausalConv1d(torch.nn.Conv1d):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
        padding_mode: str = 'zeros',
        device=None,
        dtype=None
    ) -> None:
        super(CausalConv1d, self).__init__(in_channels, out_channels,
                                           kernel_size, stride,
                                           padding=0, dilation=dilation,
                                           groups=groups, bias=bias,
                                           padding_mode=padding_mode,
                                           device=device, dtype=dtype)
        assert stride == 1
        self.causal_padding = (kernel_size - 1, 0)

    def forward(self, x: torch.Tensor):
        x = F.pad(x, self.causal_padding)
        x = super(CausalConv1d, self).forward(x)
        return x
class ConditionalDecoder(nn.Module):
    def __init__(
        self,
        in_channels=320,
        out_channels=80,
        causal=True,
        channels=[256],
        dropout=0.0,
        attention_head_dim=64,
        n_blocks=4,
        num_mid_blocks=12,
        num_heads=8,
        act_fn="gelu",
        meanflow=False,
    ):
        """
        This decoder requires an input with the same shape of the target. So, if your text content
        is shorter or longer than the outputs, please re-sampling it before feeding to the decoder.
        """
        super().__init__()
        channels = tuple(channels)
        self.meanflow = meanflow
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.causal = causal
        self.time_embeddings = SinusoidalPosEmb(in_channels)
        time_embed_dim = channels[0] * 4
        self.time_mlp = TimestepEmbedding(
            in_channels=in_channels,
            time_embed_dim=time_embed_dim,
            act_fn="silu",
        )

        self.down_blocks = nn.ModuleList([])
        self.mid_blocks = nn.ModuleList([])
        self.up_blocks = nn.ModuleList([])

        # NOTE jrm: `static_chunk_size` is missing?
        self.static_chunk_size = 0

        output_channel = in_channels
        for i in range(len(channels)):  # pylint: disable=consider-using-enumerate
            input_channel = output_channel
            output_channel = channels[i]
            is_last = i == len(channels) - 1
            resnet = CausalResnetBlock1D(dim=input_channel, dim_out=output_channel, time_emb_dim=time_embed_dim) if self.causal else \
                ResnetBlock1D(dim=input_channel, dim_out=output_channel, time_emb_dim=time_embed_dim)
            transformer_blocks = nn.ModuleList(
                [
                    BasicTransformerBlock(
                        dim=output_channel,
                        num_attention_heads=num_heads,
                        attention_head_dim=attention_head_dim,
                        dropout=dropout,
                        activation_fn=act_fn,
                    )
                    for _ in range(n_blocks)
                ]
            )
            downsample = (
                Downsample1D(output_channel) if not is_last else
                CausalConv1d(output_channel, output_channel, 3) if self.causal else nn.Conv1d(output_channel, output_channel, 3, padding=1)
            )
            self.down_blocks.append(nn.ModuleList([resnet, transformer_blocks, downsample]))

        for _ in range(num_mid_blocks):
            input_channel = channels[-1]
            out_channels = channels[-1]
            resnet = CausalResnetBlock1D(dim=input_channel, dim_out=output_channel, time_emb_dim=time_embed_dim) if self.causal else \
                ResnetBlock1D(dim=input_channel, dim_out=output_channel, time_emb_dim=time_embed_dim)

            transformer_blocks = nn.ModuleList(
                [
                    BasicTransformerBlock(
                        dim=output_channel,
                        num_attention_heads=num_heads,
                        attention_head_dim=attention_head_dim,
                        dropout=dropout,
                        activation_fn=act_fn,
                    )
                    for _ in range(n_blocks)
                ]
            )

            self.mid_blocks.append(nn.ModuleList([resnet, transformer_blocks]))

        channels = channels[::-1] + (channels[0],)
        for i in range(len(channels) - 1):
            input_channel = channels[i] * 2
            output_channel = channels[i + 1]
            is_last = i == len(channels) - 2
            resnet = CausalResnetBlock1D(
                dim=input_channel,
                dim_out=output_channel,
                time_emb_dim=time_embed_dim,
            ) if self.causal else ResnetBlock1D(
                dim=input_channel,
                dim_out=output_channel,
                time_emb_dim=time_embed_dim,
            )
            transformer_blocks = nn.ModuleList(
                [
                    BasicTransformerBlock(
                        dim=output_channel,
                        num_attention_heads=num_heads,
                        attention_head_dim=attention_head_dim,
                        dropout=dropout,
                        activation_fn=act_fn,
                    )
                    for _ in range(n_blocks)
                ]
            )
            upsample = (
                Upsample1D(output_channel, use_conv_transpose=True)
                if not is_last
                else CausalConv1d(output_channel, output_channel, 3) if self.causal else nn.Conv1d(output_channel, output_channel, 3, padding=1)
            )
            self.up_blocks.append(nn.ModuleList([resnet, transformer_blocks, upsample]))
        self.final_block = CausalBlock1D(channels[-1], channels[-1]) if self.causal else Block1D(channels[-1], channels[-1])
        self.final_proj = nn.Conv1d(channels[-1], self.out_channels, 1)
        self.initialize_weights()
        self.time_embed_mixer = None
        if self.meanflow:
            self.time_embed_mixer = get_intmeanflow_time_mixer(time_embed_dim)


    @property
    def dtype(self):
        return self.final_proj.weight.dtype

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, nonlinearity="relu")
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.GroupNorm):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, nonlinearity="relu")
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x, mask, mu, t, spks=None, cond=None, r=None):
        """Forward pass of the UNet1DConditional model.

        Args:
            x: (B, 80, T)
            mask (_type_)
            t (_type_): shape (batch_size)
            spks (_type_, optional) Defaults to None.
            cond (_type_, optional)
            r: end time for meanflow mode (shape (1,) tensor)

        Raises:
            ValueError: _description_
            ValueError: _description_

        Returns:
            _type_: _description_
        """
        t = self.time_embeddings(t).to(t.dtype)
        t = self.time_mlp(t)

        if self.meanflow:
            r = self.time_embeddings(r).to(t.dtype)
            r = self.time_mlp(r)
            concat_embed = torch.cat([t, r], dim=1)
            t = self.time_embed_mixer(concat_embed)

        x = pack([x, mu], "b * t")[0]

        if spks is not None:
            spks = repeat(spks, "b c -> b c t", t=x.shape[-1])
            x = pack([x, spks], "b * t")[0]
        if cond is not None:
            x = pack([x, cond], "b * t")[0]

        hiddens = []
        masks = [mask]
        for resnet, transformer_blocks, downsample in self.down_blocks:
            mask_down = masks[-1]
            x = resnet(x, mask_down, t)
            x = rearrange(x, "b c t -> b t c").contiguous()
            # attn_mask = torch.matmul(mask_down.transpose(1, 2).contiguous(), mask_down)
            attn_mask = add_optional_chunk_mask(x, mask_down.bool(), False, False, 0, self.static_chunk_size, -1)
            attn_mask = mask_to_bias(attn_mask == 1, x.dtype)
            for transformer_block in transformer_blocks:
                x = transformer_block(
                    hidden_states=x,
                    attention_mask=attn_mask,
                    timestep=t,
                )
            x = rearrange(x, "b t c -> b c t").contiguous()
            hiddens.append(x)  # Save hidden states for skip connections
            x = downsample(x * mask_down)
            masks.append(mask_down[:, :, ::2])
        masks = masks[:-1]
        mask_mid = masks[-1]

        for resnet, transformer_blocks in self.mid_blocks:
            x = resnet(x, mask_mid, t)
            x = rearrange(x, "b c t -> b t c").contiguous()
            # attn_mask = torch.matmul(mask_mid.transpose(1, 2).contiguous(), mask_mid)
            attn_mask = add_optional_chunk_mask(x, mask_mid.bool(), False, False, 0, self.static_chunk_size, -1)
            attn_mask = mask_to_bias(attn_mask == 1, x.dtype)
            for transformer_block in transformer_blocks:
                x = transformer_block(
                    hidden_states=x,
                    attention_mask=attn_mask,
                    timestep=t,
                )
            x = rearrange(x, "b t c -> b c t").contiguous()

        for resnet, transformer_blocks, upsample in self.up_blocks:
            mask_up = masks.pop()
            skip = hiddens.pop()
            x = pack([x[:, :, :skip.shape[-1]], skip], "b * t")[0]
            x = resnet(x, mask_up, t)
            x = rearrange(x, "b c t -> b t c").contiguous()
            # attn_mask = torch.matmul(mask_up.transpose(1, 2).contiguous(), mask_up)
            attn_mask = add_optional_chunk_mask(x, mask_up.bool(), False, False, 0, self.static_chunk_size, -1)
            attn_mask = mask_to_bias(attn_mask == 1, x.dtype)
            for transformer_block in transformer_blocks:
                x = transformer_block(
                    hidden_states=x,
                    attention_mask=attn_mask,
                    timestep=t,
                )
            x = rearrange(x, "b t c -> b c t").contiguous()
            x = upsample(x * mask_up)
        x = self.final_block(x, mask_up)
        output = self.final_proj(x * mask_up)
        return output * mask
</file>

<file path="src/chatterbox/models/s3gen/utils/mask.py">
# Copyright (c) 2019 Shigeki Karita
#               2020 Mobvoi Inc (Binbin Zhang)
#               2024 Alibaba Inc (authors: Xiang Lyu)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch

'''
def subsequent_mask(
        size: int,
        device: torch.device = torch.device("cpu"),
) -> torch.Tensor:
    """Create mask for subsequent steps (size, size).

    This mask is used only in decoder which works in an auto-regressive mode.
    This means the current step could only do attention with its left steps.

    In encoder, fully attention is used when streaming is not necessary and
    the sequence is not long. In this  case, no attention mask is needed.

    When streaming is need, chunk-based attention is used in encoder. See
    subsequent_chunk_mask for the chunk-based attention mask.

    Args:
        size (int): size of mask
        str device (str): "cpu" or "cuda" or torch.Tensor.device
        dtype (torch.device): result dtype

    Returns:
        torch.Tensor: mask

    Examples:
        >>> subsequent_mask(3)
        [[1, 0, 0],
         [1, 1, 0],
         [1, 1, 1]]
    """
    ret = torch.ones(size, size, device=device, dtype=torch.bool)
    return torch.tril(ret)
'''


def subsequent_chunk_mask(
        size: int,
        chunk_size: int,
        num_left_chunks: int = -1,
        device: torch.device = torch.device("cpu"),
) -> torch.Tensor:
    """Create mask for subsequent steps (size, size) with chunk size,
       this is for streaming encoder

    Args:
        size (int): size of mask
        chunk_size (int): size of chunk
        num_left_chunks (int): number of left chunks
            <0: use full chunk
            >=0: use num_left_chunks
        device (torch.device): "cpu" or "cuda" or torch.Tensor.device

    Returns:
        torch.Tensor: mask

    Examples:
        >>> subsequent_chunk_mask(4, 2)
        [[1, 1, 0, 0],
         [1, 1, 0, 0],
         [1, 1, 1, 1],
         [1, 1, 1, 1]]
    """
    # NOTE this modified implementation meets onnx export requirements, but it doesn't support num_left_chunks
    # actually this is not needed after we have inference cache implemented, will remove it later
    pos_idx = torch.arange(size, device=device)
    block_value = (torch.div(pos_idx, chunk_size, rounding_mode='trunc') + 1) * chunk_size
    ret = pos_idx.unsqueeze(0) < block_value.unsqueeze(1)
    return ret


def add_optional_chunk_mask(xs: torch.Tensor,
                            masks: torch.Tensor,
                            use_dynamic_chunk: bool,
                            use_dynamic_left_chunk: bool,
                            decoding_chunk_size: int,
                            static_chunk_size: int,
                            num_decoding_left_chunks: int,
                            enable_full_context: bool = True):
    """ Apply optional mask for encoder.

    Args:
        xs (torch.Tensor): padded input, (B, L, D), L for max length
        mask (torch.Tensor): mask for xs, (B, 1, L)
        use_dynamic_chunk (bool): whether to use dynamic chunk or not
        use_dynamic_left_chunk (bool): whether to use dynamic left chunk for
            training.
        decoding_chunk_size (int): decoding chunk size for dynamic chunk, it's
            0: default for training, use random dynamic chunk.
            <0: for decoding, use full chunk.
            >0: for decoding, use fixed chunk size as set.
        static_chunk_size (int): chunk size for static chunk training/decoding
            if it's greater than 0, if use_dynamic_chunk is true,
            this parameter will be ignored
        num_decoding_left_chunks: number of left chunks, this is for decoding,
            the chunk size is decoding_chunk_size.
            >=0: use num_decoding_left_chunks
            <0: use all left chunks
        enable_full_context (bool):
            True: chunk size is either [1, 25] or full context(max_len)
            False: chunk size ~ U[1, 25]

    Returns:
        torch.Tensor: chunk mask of the input xs.
    """
    # Whether to use chunk mask or not
    if use_dynamic_chunk:
        max_len = xs.size(1)
        if decoding_chunk_size < 0:
            chunk_size = max_len
            num_left_chunks = -1
        elif decoding_chunk_size > 0:
            chunk_size = decoding_chunk_size
            num_left_chunks = num_decoding_left_chunks
        else:
            # chunk size is either [1, 25] or full context(max_len).
            # Since we use 4 times subsampling and allow up to 1s(100 frames)
            # delay, the maximum frame is 100 / 4 = 25.
            chunk_size = torch.randint(1, max_len, (1, )).item()
            num_left_chunks = -1
            if chunk_size > max_len // 2 and enable_full_context:
                chunk_size = max_len
            else:
                chunk_size = chunk_size % 25 + 1
                if use_dynamic_left_chunk:
                    max_left_chunks = (max_len - 1) // chunk_size
                    num_left_chunks = torch.randint(0, max_left_chunks,
                                                    (1, )).item()
        chunk_masks = subsequent_chunk_mask(xs.size(1), chunk_size,
                                            num_left_chunks,
                                            xs.device)  # (L, L)
        chunk_masks = chunk_masks.unsqueeze(0)  # (1, L, L)
        chunk_masks = masks & chunk_masks  # (B, L, L)
    elif static_chunk_size > 0:
        num_left_chunks = num_decoding_left_chunks
        chunk_masks = subsequent_chunk_mask(xs.size(1), static_chunk_size,
                                            num_left_chunks,
                                            xs.device)  # (L, L)
        chunk_masks = chunk_masks.unsqueeze(0)  # (1, L, L)
        chunk_masks = masks & chunk_masks  # (B, L, L)
    else:
        chunk_masks = masks
    assert chunk_masks.dtype == torch.bool
    if (chunk_masks.sum(dim=-1) == 0).sum().item() != 0:
        logging.warning('get chunk_masks all false at some timestep, force set to true, make sure they are masked in futuer computation!')
        chunk_masks[chunk_masks.sum(dim=-1)==0] = True
    return chunk_masks


def make_pad_mask(lengths: torch.Tensor, max_len: int = 0) -> torch.Tensor:
    """Make mask tensor containing indices of padded part.

    See description of make_non_pad_mask.

    Args:
        lengths (torch.Tensor): Batch of lengths (B,).
    Returns:
        torch.Tensor: Mask tensor containing indices of padded part.

    Examples:
        >>> lengths = [5, 3, 2]
        >>> make_pad_mask(lengths)
        masks = [[0, 0, 0, 0 ,0],
                 [0, 0, 0, 1, 1],
                 [0, 0, 1, 1, 1]]
    """
    lengths = lengths.long()
    batch_size = lengths.size(0)
    max_len = max_len if max_len > 0 else lengths.max().item()
    seq_range = torch.arange(0,
                             max_len,
                             dtype=torch.int64,
                             device=lengths.device)
    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)
    seq_length_expand = lengths.unsqueeze(-1)
    mask = seq_range_expand >= seq_length_expand
    return mask
</file>

<file path="src/chatterbox/models/s3gen/utils/mel.py">
"""mel-spectrogram extraction in Matcha-TTS"""
import logging
from librosa.filters import mel as librosa_mel_fn
import torch
import numpy as np

logger = logging.getLogger(__name__)


# NOTE: they decalred these global vars
mel_basis = {}
hann_window = {}


def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):
    return torch.log(torch.clamp(x, min=clip_val) * C)


def spectral_normalize_torch(magnitudes):
    output = dynamic_range_compression_torch(magnitudes)
    return output

"""
feat_extractor: !name:matcha.utils.audio.mel_spectrogram
    n_fft: 1920
    num_mels: 80
    sampling_rate: 24000
    hop_size: 480
    win_size: 1920
    fmin: 0
    fmax: 8000
    center: False

"""

def mel_spectrogram(y, n_fft=1920, num_mels=80, sampling_rate=24000, hop_size=480, win_size=1920,
                    fmin=0, fmax=8000, center=False):
    """Copied from https://github.com/shivammehta25/Matcha-TTS/blob/main/matcha/utils/audio.py
    Set default values according to Cosyvoice's config.
    """

    if isinstance(y, np.ndarray):
        y = torch.tensor(y).float()

    if len(y.shape) == 1:
        y = y[None, ]

    # Debug: Check for audio clipping (values outside [-1.0, 1.0] range)
    min_val = torch.min(y)
    max_val = torch.max(y)
    if min_val < -1.0 or max_val > 1.0:
        logger.warning(f"Audio values outside normalized range: min={min_val.item():.4f}, max={max_val.item():.4f}")

    global mel_basis, hann_window  # pylint: disable=global-statement,global-variable-not-assigned
    if f"{str(fmax)}_{str(y.device)}" not in mel_basis:
        mel = librosa_mel_fn(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)
        mel_basis[str(fmax) + "_" + str(y.device)] = torch.from_numpy(mel).float().to(y.device)
        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)

    y = torch.nn.functional.pad(
        y.unsqueeze(1), (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)), mode="reflect"
    )
    y = y.squeeze(1)

    spec = torch.view_as_real(
        torch.stft(
            y,
            n_fft,
            hop_length=hop_size,
            win_length=win_size,
            window=hann_window[str(y.device)],
            center=center,
            pad_mode="reflect",
            normalized=False,
            onesided=True,
            return_complex=True,
        )
    )

    spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))

    spec = torch.matmul(mel_basis[str(fmax) + "_" + str(y.device)], spec)
    spec = spectral_normalize_torch(spec)

    return spec
</file>

<file path="src/chatterbox/models/t3/inference/alignment_stream_analyzer.py">
# Copyright (c) 2025 Resemble AI
# Author: John Meade, Jeremy Hsu
# MIT License
import logging
import torch
from dataclasses import dataclass
from types import MethodType


logger = logging.getLogger(__name__)


LLAMA_ALIGNED_HEADS = [(12, 15), (13, 11), (9, 2)]


@dataclass
class AlignmentAnalysisResult:
    # was this frame detected as being part of a noisy beginning chunk with potential hallucinations?
    false_start: bool
    # was this frame detected as being part of a long tail with potential hallucinations?
    long_tail: bool
    # was this frame detected as repeating existing text content?
    repetition: bool
    # was the alignment position of this frame too far from the previous frame?
    discontinuity: bool
    # has inference reached the end of the text tokens? eg, this remains false if inference stops early
    complete: bool
    # approximate position in the text token sequence. Can be used for generating online timestamps.
    position: int


class AlignmentStreamAnalyzer:
    def __init__(self, tfmr, queue, text_tokens_slice, alignment_layer_idx=9, eos_idx=0):
        """
        Some transformer TTS models implicitly solve text-speech alignment in one or more of their self-attention
        activation maps. This module exploits this to perform online integrity checks which streaming.
        A hook is injected into the specified attention layer, and heuristics are used to determine alignment
        position, repetition, etc.

        NOTE: currently requires no queues.
        """
        # self.queue = queue
        self.text_tokens_slice = (i, j) = text_tokens_slice
        self.eos_idx = eos_idx
        self.alignment = torch.zeros(0, j-i)
        # self.alignment_bin = torch.zeros(0, j-i)
        self.curr_frame_pos = 0
        self.text_position = 0

        self.started = False
        self.started_at = None

        self.complete = False
        self.completed_at = None
        
        # Track generated tokens for repetition detection
        self.generated_tokens = []

        # Using `output_attentions=True` is incompatible with optimized attention kernels, so
        # using it for all layers slows things down too much. We can apply it to just one layer
        # by intercepting the kwargs and adding a forward hook (credit: jrm)
        self.last_aligned_attns = []
        for i, (layer_idx, head_idx) in enumerate(LLAMA_ALIGNED_HEADS):
            self.last_aligned_attns += [None]
            self._add_attention_spy(tfmr, i, layer_idx, head_idx)

    def _add_attention_spy(self, tfmr, buffer_idx, layer_idx, head_idx):
        """
        Adds a forward hook to a specific attention layer to collect outputs.
        """
        def attention_forward_hook(module, input, output):
            """
            See `LlamaAttention.forward`; the output is a 3-tuple: `attn_output, attn_weights, past_key_value`.
            NOTE:
            - When `output_attentions=True`, `LlamaSdpaAttention.forward` calls `LlamaAttention.forward`.
            - `attn_output` has shape [B, H, T0, T0] for the 0th entry, and [B, H, 1, T0+i] for the rest i-th.
            """
            if isinstance(output, tuple) and len(output) > 1 and output[1] is not None:
                step_attention = output[1].cpu()  # (B, n_heads, T0, Ti)
                self.last_aligned_attns[buffer_idx] = step_attention[0, head_idx]  # (T0, Ti)

        target_layer = tfmr.layers[layer_idx].self_attn
        # Register hook and store the handle
        target_layer.register_forward_hook(attention_forward_hook)
        if hasattr(tfmr, 'config') and hasattr(tfmr.config, 'output_attentions'):
            self.original_output_attentions = tfmr.config.output_attentions
            tfmr.config.output_attentions = True

    def step(self, logits, next_token=None):
        """
        Emits an AlignmentAnalysisResult into the output queue, and potentially modifies the logits to force an EOS.
        """
        # extract approximate alignment matrix chunk (1 frame at a time after the first chunk)
        aligned_attn = torch.stack(self.last_aligned_attns).mean(dim=0) # (N, N)
        i, j = self.text_tokens_slice
        if self.curr_frame_pos == 0:
            # first chunk has conditioning info, text tokens, and BOS token
            A_chunk = aligned_attn[j:, i:j].clone().cpu() # (T, S)
        else:
            # subsequent chunks have 1 frame due to KV-caching
            A_chunk = aligned_attn[:, i:j].clone().cpu() # (1, S)

        # TODO: monotonic masking; could have issue b/c spaces are often skipped.
        A_chunk[:, self.curr_frame_pos + 1:] = 0


        self.alignment = torch.cat((self.alignment, A_chunk), dim=0)

        A = self.alignment
        T, S = A.shape

        # update position
        cur_text_posn = A_chunk[-1].argmax()
        discontinuity = not(-4 < cur_text_posn - self.text_position < 7) # NOTE: very lenient!
        if not discontinuity:
            self.text_position = cur_text_posn

        # Hallucinations at the start of speech show up as activations at the bottom of the attention maps!
        # To mitigate this, we just wait until there are no activations far off-diagonal in the last 2 tokens,
        # and there are some strong activations in the first few tokens.
        false_start = (not self.started) and (A[-2:, -2:].max() > 0.1 or A[:, :4].max() < 0.5)
        self.started = not false_start
        if self.started and self.started_at is None:
            self.started_at = T

        # Is generation likely complete?
        self.complete = self.complete or self.text_position >= S - 3
        if self.complete and self.completed_at is None:
            self.completed_at = T

        # NOTE: EOS rarely assigned activations, and second-last token is often punctuation, so use last 3 tokens.
        # NOTE: due to the false-start behaviour, we need to make sure we skip activations for the first few tokens.
        last_text_token_duration = A[15:, -3:].sum()

        # Activations for the final token that last too long are likely hallucinations.
        long_tail = self.complete and (A[self.completed_at:, -3:].sum(dim=0).max() >= 5) # 200ms

        # If there are activations in previous tokens after generation has completed, assume this is a repetition error.
        alignment_repetition = self.complete and (A[self.completed_at:, :-5].max(dim=1).values.sum() > 5)
        
        # Track generated tokens for repetition detection
        if next_token is not None:
            # Convert tensor to scalar if needed
            if isinstance(next_token, torch.Tensor):
                token_id = next_token.item() if next_token.numel() == 1 else next_token.view(-1)[0].item()
            else:
                token_id = next_token
            self.generated_tokens.append(token_id)
            
            # Keep only last 8 tokens to prevent memory issues
            if len(self.generated_tokens) > 8:
                self.generated_tokens = self.generated_tokens[-8:]
            
        # Check for excessive token repetition (3x same token in a row)
        token_repetition = (
            # self.complete and 
            len(self.generated_tokens) >= 3 and
            len(set(self.generated_tokens[-2:])) == 1
        )
        
        if token_repetition:
            repeated_token = self.generated_tokens[-1]
            logger.warning(f"🚨 Detected 2x repetition of token {repeated_token}")
            
        # Suppress EoS to prevent early termination
        if cur_text_posn < S - 3 and S > 5:  # Only suppress if text is longer than 5 tokens
            logits[..., self.eos_idx] = -2**15

        # If a bad ending is detected, force emit EOS by modifying logits
        # NOTE: this means logits may be inconsistent with latents!
        if long_tail or alignment_repetition or token_repetition:
            logger.warning(f"forcing EOS token, {long_tail=}, {alignment_repetition=}, {token_repetition=}")
            # (±2**15 is safe for all dtypes >= 16bit)
            logits = -(2**15) * torch.ones_like(logits)
            logits[..., self.eos_idx] = 2**15

        self.curr_frame_pos += 1
        return logits
</file>

<file path="src/chatterbox/models/t3/llama_configs.py">
LLAMA_520M_CONFIG_DICT = dict(
    # Arbitrary small number that won't cause problems when loading.
    # These param are unused due to custom input layers.
    vocab_size=8,
    # default params needed for loading most pretrained 1B weights
    max_position_embeddings=131072,
    hidden_size=1024,
    intermediate_size=4096,
    num_hidden_layers=30,
    num_attention_heads=16,
    attn_implementation="sdpa",
    head_dim=64,
    tie_word_embeddings=False,
    hidden_act="silu",
    attention_bias=False,
    attention_dropout=0.0,
    initializer_range=0.02,
    mlp_bias=False,
    model_type="llama",
    num_key_value_heads=16,
    pretraining_tp=1,
    rms_norm_eps=1e-05,
    rope_scaling=dict(
        factor=8.0,
        high_freq_factor=4.0,
        low_freq_factor=1.0,
        original_max_position_embeddings=8192,
        rope_type="llama3"
    ),
    rope_theta=500000.0,
    torch_dtype="bfloat16",
    use_cache=True,
)

GPT2_MEDIUM_CONFIG = {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 8196,
  "n_embd": 1024,
  "hidden_size": 1024,
  "n_head": 16,
  "n_layer": 24,
  "n_positions": 8196,
  "n_special": 0,
  "predict_special_tokens": True,
  "resid_pdrop": 0.1,
  "summary_activation": None,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": True,
  "summary_type": "cls_index",
  "summary_use_proj": True,
  "task_specific_params": {
    "text-generation": {
      "do_sample": True,
      "max_length": 50
    }
  },
  "vocab_size": 50276,
}

LLAMA_CONFIGS = {
    "Llama_520M": LLAMA_520M_CONFIG_DICT,
    "GPT2_medium": GPT2_MEDIUM_CONFIG,
}
</file>

<file path="src/chatterbox/models/tokenizers/__init__.py">
from .tokenizer import EnTokenizer, MTLTokenizer
</file>

<file path="src/chatterbox/mtl_tts.py">
from dataclasses import dataclass
from pathlib import Path
import os

import librosa
import torch
import perth
import torch.nn.functional as F
from safetensors.torch import load_file as load_safetensors
from huggingface_hub import snapshot_download

from .models.t3 import T3
from .models.t3.modules.t3_config import T3Config
from .models.s3tokenizer import S3_SR, drop_invalid_tokens
from .models.s3gen import S3GEN_SR, S3Gen
from .models.tokenizers import MTLTokenizer
from .models.voice_encoder import VoiceEncoder
from .models.t3.modules.cond_enc import T3Cond


REPO_ID = "ResembleAI/chatterbox"

# Supported languages for the multilingual model
SUPPORTED_LANGUAGES = {
  "ar": "Arabic",
  "da": "Danish",
  "de": "German",
  "el": "Greek",
  "en": "English",
  "es": "Spanish",
  "fi": "Finnish",
  "fr": "French",
  "he": "Hebrew",
  "hi": "Hindi",
  "it": "Italian",
  "ja": "Japanese",
  "ko": "Korean",
  "ms": "Malay",
  "nl": "Dutch",
  "no": "Norwegian",
  "pl": "Polish",
  "pt": "Portuguese",
  "ru": "Russian",
  "sv": "Swedish",
  "sw": "Swahili",
  "tr": "Turkish",
  "zh": "Chinese",
}


def punc_norm(text: str) -> str:
    """
        Quick cleanup func for punctuation from LLMs or
        containing chars not seen often in the dataset
    """
    if len(text) == 0:
        return "You need to add some text for me to talk."

    # Capitalise first letter
    if text[0].islower():
        text = text[0].upper() + text[1:]

    # Remove multiple space chars
    text = " ".join(text.split())

    # Replace uncommon/llm punc
    punc_to_replace = [
        ("...", ", "),
        ("…", ", "),
        (":", ","),
        (" - ", ", "),
        (";", ", "),
        ("—", "-"),
        ("–", "-"),
        (" ,", ","),
        ("“", "\""),
        ("”", "\""),
        ("‘", "'"),
        ("’", "'"),
    ]
    for old_char_sequence, new_char in punc_to_replace:
        text = text.replace(old_char_sequence, new_char)

    # Add full stop if no ending punc
    text = text.rstrip(" ")
    sentence_enders = {".", "!", "?", "-", ",","、","，","。","？","！"}
    if not any(text.endswith(p) for p in sentence_enders):
        text += "."

    return text


@dataclass
class Conditionals:
    """
    Conditionals for T3 and S3Gen
    - T3 conditionals:
        - speaker_emb
        - clap_emb
        - cond_prompt_speech_tokens
        - cond_prompt_speech_emb
        - emotion_adv
    - S3Gen conditionals:
        - prompt_token
        - prompt_token_len
        - prompt_feat
        - prompt_feat_len
        - embedding
    """
    t3: T3Cond
    gen: dict

    def to(self, device):
        self.t3 = self.t3.to(device=device)
        for k, v in self.gen.items():
            if torch.is_tensor(v):
                self.gen[k] = v.to(device=device)
        return self

    def save(self, fpath: Path):
        arg_dict = dict(
            t3=self.t3.__dict__,
            gen=self.gen
        )
        torch.save(arg_dict, fpath)

    @classmethod
    def load(cls, fpath, map_location="cpu"):
        kwargs = torch.load(fpath, map_location=map_location, weights_only=True)
        return cls(T3Cond(**kwargs['t3']), kwargs['gen'])


class ChatterboxMultilingualTTS:
    ENC_COND_LEN = 6 * S3_SR
    DEC_COND_LEN = 10 * S3GEN_SR

    def __init__(
        self,
        t3: T3,
        s3gen: S3Gen,
        ve: VoiceEncoder,
        tokenizer: MTLTokenizer,
        device: str,
        conds: Conditionals = None,
    ):
        self.sr = S3GEN_SR  # sample rate of synthesized audio
        self.t3 = t3
        self.s3gen = s3gen
        self.ve = ve
        self.tokenizer = tokenizer
        self.device = device
        self.conds = conds
        self.watermarker = perth.PerthImplicitWatermarker()

    @classmethod
    def get_supported_languages(cls):
        """Return dictionary of supported language codes and names."""
        return SUPPORTED_LANGUAGES.copy()

    @classmethod
    def from_local(cls, ckpt_dir, device) -> 'ChatterboxMultilingualTTS':
        ckpt_dir = Path(ckpt_dir)

        ve = VoiceEncoder()
        ve.load_state_dict(
            torch.load(ckpt_dir / "ve.pt", weights_only=True)
        )
        ve.to(device).eval()

        t3 = T3(T3Config.multilingual())
        t3_state = load_safetensors(ckpt_dir / "t3_mtl23ls_v2.safetensors")
        if "model" in t3_state.keys():
            t3_state = t3_state["model"][0]
        t3.load_state_dict(t3_state)
        t3.to(device).eval()

        s3gen = S3Gen()
        s3gen.load_state_dict(
            torch.load(ckpt_dir / "s3gen.pt", weights_only=True)
        )
        s3gen.to(device).eval()

        tokenizer = MTLTokenizer(
            str(ckpt_dir / "grapheme_mtl_merged_expanded_v1.json")
        )

        conds = None
        if (builtin_voice := ckpt_dir / "conds.pt").exists():
            conds = Conditionals.load(builtin_voice).to(device)

        return cls(t3, s3gen, ve, tokenizer, device, conds=conds)

    @classmethod
    def from_pretrained(cls, device: torch.device) -> 'ChatterboxMultilingualTTS':
        ckpt_dir = Path(
            snapshot_download(
                repo_id=REPO_ID,
                repo_type="model",
                revision="main", 
                allow_patterns=["ve.pt", "t3_mtl23ls_v2.safetensors", "s3gen.pt", "grapheme_mtl_merged_expanded_v1.json", "conds.pt", "Cangjie5_TC.json"],
                token=os.getenv("HF_TOKEN"),
            )
        )
        return cls.from_local(ckpt_dir, device)
    
    def prepare_conditionals(self, wav_fpath, exaggeration=0.5):
        ## Load reference wav
        s3gen_ref_wav, _sr = librosa.load(wav_fpath, sr=S3GEN_SR)

        ref_16k_wav = librosa.resample(s3gen_ref_wav, orig_sr=S3GEN_SR, target_sr=S3_SR)

        s3gen_ref_wav = s3gen_ref_wav[:self.DEC_COND_LEN]
        s3gen_ref_dict = self.s3gen.embed_ref(s3gen_ref_wav, S3GEN_SR, device=self.device)

        # Speech cond prompt tokens
        t3_cond_prompt_tokens = None
        if plen := self.t3.hp.speech_cond_prompt_len:
            s3_tokzr = self.s3gen.tokenizer
            t3_cond_prompt_tokens, _ = s3_tokzr.forward([ref_16k_wav[:self.ENC_COND_LEN]], max_len=plen)
            t3_cond_prompt_tokens = torch.atleast_2d(t3_cond_prompt_tokens).to(self.device)

        # Voice-encoder speaker embedding
        ve_embed = torch.from_numpy(self.ve.embeds_from_wavs([ref_16k_wav], sample_rate=S3_SR))
        ve_embed = ve_embed.mean(axis=0, keepdim=True).to(self.device)

        t3_cond = T3Cond(
            speaker_emb=ve_embed,
            cond_prompt_speech_tokens=t3_cond_prompt_tokens,
            emotion_adv=exaggeration * torch.ones(1, 1, 1),
        ).to(device=self.device)
        self.conds = Conditionals(t3_cond, s3gen_ref_dict)

    def generate(
        self,
        text,
        language_id,
        audio_prompt_path=None,
        exaggeration=0.5,
        cfg_weight=0.5,
        temperature=0.8,
        repetition_penalty=2.0,
        min_p=0.05,
        top_p=1.0,
    ):
        # Validate language_id
        if language_id and language_id.lower() not in SUPPORTED_LANGUAGES:
            supported_langs = ", ".join(SUPPORTED_LANGUAGES.keys())
            raise ValueError(
                f"Unsupported language_id '{language_id}'. "
                f"Supported languages: {supported_langs}"
            )
        
        if audio_prompt_path:
            self.prepare_conditionals(audio_prompt_path, exaggeration=exaggeration)
        else:
            assert self.conds is not None, "Please `prepare_conditionals` first or specify `audio_prompt_path`"

        # Update exaggeration if needed
        if float(exaggeration) != float(self.conds.t3.emotion_adv[0, 0, 0].item()):
            _cond: T3Cond = self.conds.t3
            self.conds.t3 = T3Cond(
                speaker_emb=_cond.speaker_emb,
                cond_prompt_speech_tokens=_cond.cond_prompt_speech_tokens,
                emotion_adv=exaggeration * torch.ones(1, 1, 1),
            ).to(device=self.device)

        # Norm and tokenize text
        text = punc_norm(text)
        text_tokens = self.tokenizer.text_to_tokens(text, language_id=language_id.lower() if language_id else None).to(self.device)
        text_tokens = torch.cat([text_tokens, text_tokens], dim=0)  # Need two seqs for CFG

        sot = self.t3.hp.start_text_token
        eot = self.t3.hp.stop_text_token
        text_tokens = F.pad(text_tokens, (1, 0), value=sot)
        text_tokens = F.pad(text_tokens, (0, 1), value=eot)

        with torch.inference_mode():
            speech_tokens = self.t3.inference(
                t3_cond=self.conds.t3,
                text_tokens=text_tokens,
                max_new_tokens=1000,  # TODO: use the value in config
                temperature=temperature,
                cfg_weight=cfg_weight,
                repetition_penalty=repetition_penalty,
                min_p=min_p,
                top_p=top_p,
            )
            # Extract only the conditional batch.
            speech_tokens = speech_tokens[0]

            # TODO: output becomes 1D
            speech_tokens = drop_invalid_tokens(speech_tokens)
            speech_tokens = speech_tokens.to(self.device)

            wav, _ = self.s3gen.inference(
                speech_tokens=speech_tokens,
                ref_dict=self.conds.gen,
            )
            wav = wav.squeeze(0).detach().cpu().numpy()
            watermarked_wav = self.watermarker.apply_watermark(wav, sample_rate=self.sr)
        return torch.from_numpy(watermarked_wav).unsqueeze(0)
</file>

<file path="candle/src/modules.rs">
use candle_core::{Result, Tensor, Module};
use candle_nn::{Conv1d, Conv1dConfig, LayerNorm, Linear, VarBuilder};

// Swish activation
pub struct Swish;
impl Module for Swish {
    fn forward(&self, xs: &Tensor) -> Result<Tensor> {
        let sig = candle_nn::ops::sigmoid(xs)?;
        xs.broadcast_mul(&sig)
    }
}

// Convolution Module for Conformer
// Pointwise -> Gated Conv -> Pointwise
pub struct ConvolutionModule {
    pointwise_conv1: Conv1d,
    depthwise_conv: Conv1d,
    norm: LayerNorm,
    pointwise_conv2: Conv1d,
    activation: Swish,
}

impl ConvolutionModule {
    pub fn new(channels: usize, kernel_size: usize, vb: VarBuilder) -> Result<Self> {
        // Pointwise 1
        let pointwise_conv1 = candle_nn::conv1d(channels, 2 * channels, 1, Default::default(), vb.pp("pointwise_conv1"))?;

        // Depthwise
        let cfg = Conv1dConfig {
            padding: (kernel_size - 1) / 2,
            groups: channels,
            ..Default::default()
        };
        let depthwise_conv = candle_nn::conv1d(channels, channels, kernel_size, cfg, vb.pp("depthwise_conv"))?;

        let norm = candle_nn::layer_norm(channels, 1e-5, vb.pp("norm"))?;
        let pointwise_conv2 = candle_nn::conv1d(channels, channels, 1, Default::default(), vb.pp("pointwise_conv2"))?;

        Ok(Self {
            pointwise_conv1,
            depthwise_conv,
            norm,
            pointwise_conv2,
            activation: Swish,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // x: (B, C, T)
        let x = self.pointwise_conv1.forward(x)?;
        // GLU
        let chunks = x.chunk(2, 1)?;
        let x1 = &chunks[0];
        let x2 = &chunks[1];
        let sig = candle_nn::ops::sigmoid(x2)?;
        let x = x1.broadcast_mul(&sig)?;

        let x = self.depthwise_conv.forward(&x)?;

        // Norm expects (B, T, C)? LayerNorm in Candle usually works on last dim.
        // Conv1d output is (B, C, T).
        let x = x.transpose(1, 2)?; // (B, T, C)
        let x = self.norm.forward(&x)?;
        let x = x.transpose(1, 2)?; // (B, C, T)

        let x = self.activation.forward(&x)?;
        self.pointwise_conv2.forward(&x)
    }
}

// FeedForward Module
pub struct FeedForwardModule {
    linear1: Linear,
    _dropout: f64, // Todo: implement dropout
    linear2: Linear,
    activation: Swish,
    norm: LayerNorm,
}

impl FeedForwardModule {
    pub fn new(dim: usize, hidden_dim: usize, vb: VarBuilder) -> Result<Self> {
        let norm = candle_nn::layer_norm(dim, 1e-5, vb.pp("norm"))?;
        let linear1 = candle_nn::linear(dim, hidden_dim, vb.pp("linear1"))?;
        let linear2 = candle_nn::linear(hidden_dim, dim, vb.pp("linear2"))?;

        Ok(Self {
            linear1,
            _dropout: 0.1,
            linear2,
            activation: Swish,
            norm,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // x: (B, T, C)
        let residual = x;
        let x = self.norm.forward(x)?;
        let x = self.linear1.forward(&x)?;
        let x = self.activation.forward(&x)?;
        // dropout
        let x = self.linear2.forward(&x)?;
        // dropout
        x.broadcast_add(residual)
    }
}

// MultiHeadedAttention
pub struct MultiHeadedAttention {
    linear_q: Linear,
    linear_k: Linear,
    linear_v: Linear,
    linear_out: Linear,
    num_heads: usize,
    head_dim: usize,
}

impl MultiHeadedAttention {
    pub fn new(dim: usize, num_heads: usize, vb: VarBuilder) -> Result<Self> {
        let head_dim = dim / num_heads;
        let linear_q = candle_nn::linear(dim, dim, vb.pp("linear_q"))?;
        let linear_k = candle_nn::linear(dim, dim, vb.pp("linear_k"))?;
        let linear_v = candle_nn::linear(dim, dim, vb.pp("linear_v"))?;
        let linear_out = candle_nn::linear(dim, dim, vb.pp("linear_out"))?;

        Ok(Self {
            linear_q,
            linear_k,
            linear_v,
            linear_out,
            num_heads,
            head_dim,
        })
    }

    pub fn forward(&self, x: &Tensor, mask: Option<&Tensor>) -> Result<Tensor> {
        // x: (B, T, C)
        let (b, t, c) = x.dims3()?;
        let q = self.linear_q.forward(x)?;
        let k = self.linear_k.forward(x)?;
        let v = self.linear_v.forward(x)?;

        let q = q.reshape((b, t, self.num_heads, self.head_dim))?.transpose(1, 2)?; // (B, H, T, D)
        let k = k.reshape((b, t, self.num_heads, self.head_dim))?.transpose(1, 2)?;
        let v = v.reshape((b, t, self.num_heads, self.head_dim))?.transpose(1, 2)?;

        let k_t = k.transpose(2, 3)?;
        let att = (q.matmul(&k_t)? / (self.head_dim as f64).sqrt())?; // (B, H, T, T)

        let att = if let Some(mask) = mask {
            att.broadcast_add(mask)?
        } else {
            att
        };

        let att = candle_nn::ops::softmax(&att, 3)?;
        let out = att.matmul(&v)?; // (B, H, T, D)

        let out = out.transpose(1, 2)?.reshape((b, t, c))?; // (B, T, C)
        self.linear_out.forward(&out)
    }
}

// ConformerEncoderLayer
pub struct ConformerEncoderLayer {
    ff1: FeedForwardModule,
    self_attn: MultiHeadedAttention,
    conv_module: ConvolutionModule,
    ff2: FeedForwardModule,
    norm: LayerNorm,
    // dropout: f64,
}

impl ConformerEncoderLayer {
    pub fn new(dim: usize, hidden_dim: usize, num_heads: usize, kernel_size: usize, vb: VarBuilder) -> Result<Self> {
        let ff1 = FeedForwardModule::new(dim, hidden_dim, vb.pp("ff1"))?;
        let self_attn = MultiHeadedAttention::new(dim, num_heads, vb.pp("self_attn"))?;
        let conv_module = ConvolutionModule::new(dim, kernel_size, vb.pp("conv_module"))?;
        let ff2 = FeedForwardModule::new(dim, hidden_dim, vb.pp("ff2"))?;
        let norm = candle_nn::layer_norm(dim, 1e-5, vb.pp("norm"))?;

        Ok(Self {
            ff1,
            self_attn,
            conv_module,
            ff2,
            norm,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // x: (B, T, C)

        // FF1
        let residual = x;
        let x = self.ff1.forward(x)?;
        let x = (x * 0.5)?; // Macaron style half step?
        let x = x.broadcast_add(residual)?;

        // Self Attention
        let residual = &x;
        let x = self.norm.forward(&x)?; // Norm before attention in Conformer? Or after? Usually before or in sandwich
        let x = self.self_attn.forward(&x, None)?;
        let x = x.broadcast_add(residual)?;

        // Conv
        // x: (B, T, C) -> (B, C, T) for conv
        let residual = &x;
        let x_t = x.transpose(1, 2)?;
        let x_conv = self.conv_module.forward(&x_t)?;
        let x = x_conv.transpose(1, 2)?;
        let x = x.broadcast_add(residual)?;

        // FF2
        let residual = &x;
        let x = self.ff2.forward(&x)?;
        let x = (x * 0.5)?;
        let x = x.broadcast_add(residual)?;

        let x = self.norm.forward(&x)?;
        Ok(x)
    }
}
</file>

<file path="candle/src/s3tokenizer.rs">
//! S3Tokenizer V2 implementation based on SenseVoice-Large.
//! Ported derived from: https://github.com/xingchensong/S3Tokenizer

use candle_core::{DType, Device, Module, Result, Tensor, D};
use candle_nn::{Conv1d, Conv1dConfig, LayerNorm, Linear, VarBuilder};

#[derive(Debug, Clone)]
pub struct ModelConfig {
    pub n_mels: usize,
    pub n_audio_ctx: usize,
    pub n_audio_state: usize,
    pub n_audio_head: usize,
    pub n_audio_layer: usize,
    pub n_codebook_size: usize,
    pub use_sdpa: bool,
}

impl Default for ModelConfig {
    fn default() -> Self {
        Self {
            n_mels: 128,
            n_audio_ctx: 1500,
            n_audio_state: 1280,
            n_audio_head: 20,
            n_audio_layer: 6,
            n_codebook_size: 6561, // 3^8
            use_sdpa: false,
        }
    }
}

/// Finite Scalar Quantization Codebook
pub struct FSQCodebook {
    project_down: Linear,
    level: usize,
}

impl FSQCodebook {
    pub fn new(dim: usize, level: usize, vb: VarBuilder) -> Result<Self> {
        let project_down = candle_nn::linear(dim, 8, vb.pp("project_down"))?;
        Ok(Self {
            project_down,
            level,
        })
    }

    pub fn encode(&self, x: &Tensor) -> Result<Tensor> {
        let (b, t, d) = x.dims3()?;
        let x = x.reshape((b * t, d))?;

        // h = project_down(x).tanh()
        let h = self.project_down.forward(&x)?.tanh()?;

        // h = h * 0.9990000128746033
        let h = (h * 0.9990000128746033)?;

        // h = h.round() + 1
        let h = (h.round()? + 1.0)?;

        // levels = level ^ arange(8)
        let device = x.device();
        let powers: Vec<f32> = (0..8).map(|i| (self.level as f32).powi(i as i32)).collect();
        let powers = Tensor::from_vec(powers, (8,), device)?.to_dtype(h.dtype())?;

        // mu = sum(h * powers)
        let mu = h.broadcast_mul(&powers.unsqueeze(0)?)?.sum(1)?;

        mu.reshape((b, t))?.to_dtype(DType::U32)
    }
}

/// Rotary Positional Embedding Utilities
pub fn precompute_freqs_cis(dim: usize, end: usize, device: &Device) -> Result<(Tensor, Tensor)> {
    let theta: f32 = 10000.0;
    let inv_freq: Vec<f32> = (0..dim)
        .step_by(2)
        .map(|i| 1.0 / (theta.powf(i as f32 / dim as f32)))
        .collect();
    let inv_freq = Tensor::from_vec(inv_freq, (dim / 2,), device)?;

    let t = Tensor::arange(0.0, end as f32, device)?;
    let freqs = t.unsqueeze(1)?.matmul(&inv_freq.unsqueeze(0)?)?; // (end, dim/2)

    let cos = freqs.cos()?;
    let sin = freqs.sin()?;

    // Concatenate for simpler application to half-vectors
    let cos = Tensor::cat(&[&cos, &cos], 1)?; // (end, dim)
    let sin = Tensor::cat(&[&sin, &sin], 1)?; // (end, dim)

    Ok((cos, sin))
}

pub fn apply_rotary_emb(
    xq: &Tensor,
    xk: &Tensor,
    cos: &Tensor,
    sin: &Tensor,
) -> Result<(Tensor, Tensor)> {
    // xq: (B, T, H, D), cos: (T, D)
    let (_b, _t, _h, d_full) = xq.dims4()?;
    let d = d_full / 2;

    let xq_l = xq.narrow(D::Minus1, 0, d)?;
    let xq_r = xq.narrow(D::Minus1, d, d)?;

    let xk_l = xk.narrow(D::Minus1, 0, d)?;
    let xk_r = xk.narrow(D::Minus1, d, d)?;

    // xq_r_rot = cat(-xq_r, xq_l)
    let xq_rot = Tensor::cat(&[&xq_r.neg()?, &xq_l], D::Minus1)?;
    let xk_rot = Tensor::cat(&[&xk_r.neg()?, &xk_l], D::Minus1)?;

    // Broadcast cos/sin: (1, T, 1, D)
    let cos = cos.unsqueeze(0)?.unsqueeze(2)?;
    let sin = sin.unsqueeze(0)?.unsqueeze(2)?;

    let xq_unbound = (xq.broadcast_mul(&cos)? + xq_rot.broadcast_mul(&sin)?)?;
    let xk_unbound = (xk.broadcast_mul(&cos)? + xk_rot.broadcast_mul(&sin)?)?;

    Ok((xq_unbound, xk_unbound))
}

/// Feedforward Sequential Memory Network Block
pub struct FSMNMultiHeadAttention {
    n_head: usize,
    _n_state: usize,
    query: Linear,
    key: Linear,
    value: Linear,
    out: Linear,
    fsmn_block: Conv1d,
    kernel_size: usize,
}

impl FSMNMultiHeadAttention {
    pub fn new(n_state: usize, n_head: usize, kernel_size: usize, vb: VarBuilder) -> Result<Self> {
        let query = candle_nn::linear_no_bias(n_state, n_state, vb.pp("query"))?;
        let key = candle_nn::linear_no_bias(n_state, n_state, vb.pp("key"))?;
        let value = candle_nn::linear_no_bias(n_state, n_state, vb.pp("value"))?;
        let out = candle_nn::linear(n_state, n_state, vb.pp("out"))?;

        let fsmn_config = Conv1dConfig {
            padding: 0,
            stride: 1,
            dilation: 1,
            groups: n_state,
            ..Default::default()
        };
        // FSMN block uses no bias in Python
        let fsmn_weight = vb
            .pp("fsmn_block")
            .get((n_state, 1, kernel_size), "weight")?;
        let fsmn_block = Conv1d::new(fsmn_weight, None, fsmn_config);

        Ok(Self {
            n_head,
            _n_state: n_state,
            query,
            key,
            value,
            out,
            fsmn_block,
            kernel_size,
        })
    }

    fn forward_fsmn(&self, x: &Tensor, mask: Option<&Tensor>) -> Result<Tensor> {
        // x: (B, T, D)
        let mut inputs = x.clone();
        if let Some(m) = mask {
            inputs = inputs.broadcast_mul(m)?;
        }

        // Pad for same-length FSMN
        let left_pad = (self.kernel_size - 1) / 2;
        let right_pad = self.kernel_size - 1 - left_pad;

        let x_padded = inputs
            .transpose(1, 2)?
            .pad_with_zeros(D::Minus1, left_pad, right_pad)?;
        let mut fsmn_out = self.fsmn_block.forward(&x_padded)?;
        fsmn_out = fsmn_out.transpose(1, 2)?;

        let res = (fsmn_out + inputs)?;
        if let Some(m) = mask {
            res.broadcast_mul(m)
        } else {
            Ok(res)
        }
    }

    pub fn forward(
        &self,
        x: &Tensor,
        mask: Option<&Tensor>,
        mask_pad: Option<&Tensor>,
        cos: Option<&Tensor>,
        sin: Option<&Tensor>,
    ) -> Result<Tensor> {
        let (b, t, d) = x.dims3()?;
        let head_dim = d / self.n_head;

        let q = self.query.forward(x)?;
        let k = self.key.forward(x)?;
        let v = self.value.forward(x)?;

        let mut q = q.reshape((b, t, self.n_head, head_dim))?;
        let mut k = k.reshape((b, t, self.n_head, head_dim))?;
        let v_reshaped = v.reshape((b, t, self.n_head, head_dim))?;

        if let (Some(c), Some(s)) = (cos, sin) {
            let (q_rot, k_rot) = apply_rotary_emb(&q, &k, c, s)?;
            q = q_rot;
            k = k_rot;
        }

        let fsm_memory = self.forward_fsmn(&v, mask_pad)?;

        // Attention calculation
        let q = q.transpose(1, 2)?.contiguous()?; // (B, H, T, D/H)
        let k = k.transpose(1, 2)?.transpose(2, 3)?.contiguous()?; // (B, H, D/H, T)

        let scale = (head_dim as f32).powf(-0.5);
        let mut qk = (q.matmul(&k)? * (scale as f64))?;

        if let Some(m) = mask {
            qk = qk.broadcast_add(m)?;
        }

        let soft_qk = candle_nn::ops::softmax(&qk, D::Minus1)?;
        let v_heads = v_reshaped.transpose(1, 2)?.contiguous()?; // (B, H, T, D/H)
        let attn_out = soft_qk.matmul(&v_heads)?; // (B, H, T, D/H)

        let wv = attn_out.transpose(1, 2)?.reshape((b, t, d))?;
        let out = self.out.forward(&wv)?;

        out + fsm_memory
    }
}

pub struct ResidualAttentionBlock {
    attn: FSMNMultiHeadAttention,
    attn_ln: LayerNorm,
    mlp: Vec<Box<dyn Module>>,
    mlp_ln: LayerNorm,
}

impl ResidualAttentionBlock {
    pub fn new(n_state: usize, n_head: usize, vb: VarBuilder) -> Result<Self> {
        let attn = FSMNMultiHeadAttention::new(n_state, n_head, 31, vb.pp("attn"))?;
        let attn_ln = candle_nn::layer_norm(n_state, 1e-5, vb.pp("attn_ln"))?;

        let n_mlp = n_state * 4;
        let mlp_0 = candle_nn::linear(n_state, n_mlp, vb.pp("mlp.0"))?;
        let mlp_2 = candle_nn::linear(n_mlp, n_state, vb.pp("mlp.2"))?;

        let mlp_ln = candle_nn::layer_norm(n_state, 1e-5, vb.pp("mlp_ln"))?;

        Ok(Self {
            attn,
            attn_ln,
            mlp: vec![Box::new(mlp_0), Box::new(mlp_2)],
            mlp_ln,
        })
    }

    pub fn forward(
        &self,
        x: &Tensor,
        mask: Option<&Tensor>,
        mask_pad: Option<&Tensor>,
        cos: Option<&Tensor>,
        sin: Option<&Tensor>,
    ) -> Result<Tensor> {
        let x_ln = self.attn_ln.forward(x)?;
        let attn_out = self.attn.forward(&x_ln, mask, mask_pad, cos, sin)?;
        let x = (x + attn_out)?;

        let x_ln_mlp = self.mlp_ln.forward(&x)?;
        let mut res = x_ln_mlp;
        // Manual MLP forward to handle GELU easily if not using Sequential
        res = self.mlp[0].forward(&res)?;
        res = res.gelu()?;
        res = self.mlp[1].forward(&res)?;

        x + res
    }
}

pub struct AudioEncoderV2 {
    conv1: Conv1d,
    conv2: Conv1d,
    blocks: Vec<ResidualAttentionBlock>,
    cos: Tensor,
    sin: Tensor,
    _stride: usize,
}

impl AudioEncoderV2 {
    pub fn new(config: &ModelConfig, vb: VarBuilder) -> Result<Self> {
        let n_state = config.n_audio_state;
        let c1_config = Conv1dConfig {
            padding: 1,
            stride: 2,
            dilation: 1,
            groups: 1,
            ..Default::default()
        };
        let conv1 = candle_nn::conv1d(config.n_mels, n_state, 3, c1_config, vb.pp("conv1"))?;

        let c2_config = Conv1dConfig {
            padding: 1,
            stride: 2,
            dilation: 1,
            groups: 1,
            ..Default::default()
        };
        let conv2 = candle_nn::conv1d(n_state, n_state, 3, c2_config, vb.pp("conv2"))?;

        let mut blocks = Vec::new();
        let vb_blocks = vb.pp("blocks");
        for i in 0..config.n_audio_layer {
            blocks.push(ResidualAttentionBlock::new(
                n_state,
                config.n_audio_head,
                vb_blocks.pp(&i.to_string()),
            )?);
        }

        // Precompute RoPE: 64 is head_dim, 2048 matches Python's 1024*2
        let (cos, sin) = precompute_freqs_cis(n_state / config.n_audio_head, 2048, vb.device())?;

        Ok(Self {
            conv1,
            conv2,
            blocks,
            cos,
            sin,
            _stride: 2,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // x: (B, Mel, T)
        let mut x = self.conv1.forward(x)?.gelu()?;
        x = self.conv2.forward(&x)?.gelu()?;

        x = x.transpose(1, 2)?; // (B, T/4, D)
        let (_, t, _) = x.dims3()?;

        let cos = self.cos.narrow(0, 0, t)?;
        let sin = self.sin.narrow(0, 0, t)?;

        for block in &self.blocks {
            x = block.forward(&x, None, None, Some(&cos), Some(&sin))?;
        }

        Ok(x)
    }
}

pub struct S3TokenizerV2 {
    encoder: AudioEncoderV2,
    quantizer: FSQCodebook,
}

impl S3TokenizerV2 {
    pub fn new(config: &ModelConfig, vb: VarBuilder) -> Result<Self> {
        let vb = vb.pp("s3_model");
        let encoder = AudioEncoderV2::new(config, vb.pp("encoder"))?;
        let quantizer = FSQCodebook::new(config.n_audio_state, 3, vb.pp("quantizer._codebook"))?;

        Ok(Self { encoder, quantizer })
    }

    pub fn encode(&self, mel: &Tensor) -> Result<Tensor> {
        let hidden = self.encoder.forward(mel)?;
        self.quantizer.encode(&hidden)
    }
}
</file>

<file path="src/chatterbox/__init__.py">
try:
    from importlib.metadata import version
except ImportError:
    from importlib_metadata import version  # For Python <3.8

__version__ = version("chatterbox-tts")


from .tts import ChatterboxTTS
from .vc import ChatterboxVC
from .mtl_tts import ChatterboxMultilingualTTS, SUPPORTED_LANGUAGES
</file>

<file path="src/chatterbox/models/s3gen/flow_matching.py">
# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Zhihao Du)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import threading
import torch
import torch.nn.functional as F
from .matcha.flow_matching import BASECFM
from .configs import CFM_PARAMS
from tqdm import tqdm


def cast_all(*args, dtype):
    return [a if (not a.dtype.is_floating_point) or a.dtype == dtype else a.to(dtype) for a in args]


class ConditionalCFM(BASECFM):
    def __init__(self, in_channels, cfm_params, n_spks=1, spk_emb_dim=64, estimator: torch.nn.Module = None):
        super().__init__(
            n_feats=in_channels,
            cfm_params=cfm_params,
            n_spks=n_spks,
            spk_emb_dim=spk_emb_dim,
        )
        self.t_scheduler = cfm_params.t_scheduler
        self.training_cfg_rate = cfm_params.training_cfg_rate
        self.inference_cfg_rate = cfm_params.inference_cfg_rate
        in_channels = in_channels + (spk_emb_dim if n_spks > 0 else 0)
        # Just change the architecture of the estimator here
        self.estimator = estimator

    @torch.inference_mode()
    def forward(self, mu, mask, n_timesteps, temperature=1.0, spks=None, cond=None, prompt_len=0, flow_cache=torch.zeros(1, 80, 0, 2)):
        """Forward diffusion

        Args:
            mu (torch.Tensor): output of encoder
                shape: (batch_size, n_feats, mel_timesteps)
            mask (torch.Tensor): output_mask
                shape: (batch_size, 1, mel_timesteps)
            n_timesteps (int): number of diffusion steps
            temperature (float, optional): temperature for scaling noise. Defaults to 1.0.
            spks (torch.Tensor, optional): speaker ids. Defaults to None.
                shape: (batch_size, spk_emb_dim)
            cond: Not used but kept for future purposes

        Returns:
            sample: generated mel-spectrogram
                shape: (batch_size, n_feats, mel_timesteps)
        """

        raise NotImplementedError("unused, needs updating for meanflow model")

        z = torch.randn_like(mu).to(mu.device).to(mu.dtype) * temperature
        cache_size = flow_cache.shape[2]
        # fix prompt and overlap part mu and z
        if cache_size != 0:
            z[:, :, :cache_size] = flow_cache[:, :, :, 0]
            mu[:, :, :cache_size] = flow_cache[:, :, :, 1]
        z_cache = torch.concat([z[:, :, :prompt_len], z[:, :, -34:]], dim=2)
        mu_cache = torch.concat([mu[:, :, :prompt_len], mu[:, :, -34:]], dim=2)
        flow_cache = torch.stack([z_cache, mu_cache], dim=-1)

        t_span = torch.linspace(0, 1, n_timesteps + 1, device=mu.device, dtype=mu.dtype)
        if self.t_scheduler == 'cosine':
            t_span = 1 - torch.cos(t_span * 0.5 * torch.pi)
        return self.solve_euler(z, t_span=t_span, mu=mu, mask=mask, spks=spks, cond=cond), flow_cache

    def solve_euler(self, x, t_span, mu, mask, spks, cond, meanflow=False):
        """
        Fixed euler solver for ODEs.
        Args:
            x (torch.Tensor): random noise
            t_span (torch.Tensor): n_timesteps interpolated
                shape: (n_timesteps + 1,)
            mu (torch.Tensor): output of encoder
                shape: (batch_size, n_feats, mel_timesteps)
            mask (torch.Tensor): output_mask
                shape: (batch_size, 1, mel_timesteps)
            spks (torch.Tensor, optional): speaker ids. Defaults to None.
                shape: (batch_size, spk_emb_dim)
            cond: Not used but kept for future purposes
            meanflow: meanflow mode
        """
        in_dtype = x.dtype
        x, t_span, mu, mask, spks, cond = cast_all(x, t_span, mu, mask, spks, cond, dtype=self.estimator.dtype)

        # Duplicated batch dims are for CFG
        # Do not use concat, it may cause memory format changed and trt infer with wrong results!
        B, T = mu.size(0), x.size(2)
        x_in    = torch.zeros([2 * B, 80, T], device=x.device, dtype=x.dtype)
        mask_in = torch.zeros([2 * B,  1, T], device=x.device, dtype=x.dtype)
        mu_in   = torch.zeros([2 * B, 80, T], device=x.device, dtype=x.dtype)
        t_in    = torch.zeros([2 * B       ], device=x.device, dtype=x.dtype)
        spks_in = torch.zeros([2 * B, 80   ], device=x.device, dtype=x.dtype)
        cond_in = torch.zeros([2 * B, 80, T], device=x.device, dtype=x.dtype)
        r_in    = torch.zeros([2 * B       ], device=x.device, dtype=x.dtype) # (only used for meanflow)

        for t, r in zip(t_span[:-1], t_span[1:]):
            t = t.unsqueeze(dim=0)
            r = r.unsqueeze(dim=0)
            # Shapes:
            #      x_in  ( 2B, 80, T )
            #   mask_in  ( 2B,  1, T )
            #     mu_in  ( 2B, 80, T )
            #      t_in  ( 2B,       )
            #   spks_in  ( 2B, 80,   )
            #   cond_in  ( 2B, 80, T )
            #      r_in  ( 2B,       )
            #         x  (  B, 80, T )
            #      mask  (  B,  1, T )
            #        mu  (  B, 80, T )
            #         t  (  B,       )
            #      spks  (  B, 80,   )
            #      cond  (  B, 80, T )
            #         r  (  B,       )

            x_in[:B] = x_in[B:] = x
            mask_in[:B] = mask_in[B:] = mask
            mu_in[:B] = mu
            t_in[:B] = t_in[B:] = t
            spks_in[:B] = spks
            cond_in[:B] = cond
            r_in[:B] = r_in[B:] = r # (only used for meanflow)
            dxdt = self.estimator.forward(
                x=x_in, mask=mask_in, mu=mu_in, t=t_in, spks=spks_in, cond=cond_in,
                r=r_in if meanflow else None,
            )
            dxdt, cfg_dxdt = torch.split(dxdt, [B, B], dim=0)
            dxdt = ((1.0 + self.inference_cfg_rate) * dxdt - self.inference_cfg_rate * cfg_dxdt)
            dt = r - t
            x = x + dt * dxdt



        return x.to(in_dtype)

    def compute_loss(self, x1, mask, mu, spks=None, cond=None):
        """Computes diffusion loss

        Args:
            x1 (torch.Tensor): Target
                shape: (batch_size, n_feats, mel_timesteps)
            mask (torch.Tensor): target mask
                shape: (batch_size, 1, mel_timesteps)
            mu (torch.Tensor): output of encoder
                shape: (batch_size, n_feats, mel_timesteps)
            spks (torch.Tensor, optional): speaker embedding. Defaults to None.
                shape: (batch_size, spk_emb_dim)

        Returns:
            loss: conditional flow matching loss
            y: conditional flow
                shape: (batch_size, n_feats, mel_timesteps)
        """
        b, _, t = mu.shape

        # random timestep
        t = torch.rand([b, 1, 1], device=mu.device, dtype=mu.dtype)
        if self.t_scheduler == 'cosine':
            t = 1 - torch.cos(t * 0.5 * torch.pi)
        # sample noise p(x_0)
        z = torch.randn_like(x1)

        y = (1 - (1 - self.sigma_min) * t) * z + t * x1
        u = x1 - (1 - self.sigma_min) * z

        # during training, we randomly drop condition to trade off mode coverage and sample fidelity
        if self.training_cfg_rate > 0:
            cfg_mask = torch.rand(b, device=x1.device) > self.training_cfg_rate
            mu = mu * cfg_mask.view(-1, 1, 1)
            spks = spks * cfg_mask.view(-1, 1)
            cond = cond * cfg_mask.view(-1, 1, 1)

        pred = self.estimator(y, mask, mu, t.squeeze(), spks, cond)
        loss = F.mse_loss(pred * mask, u * mask, reduction="sum") / (torch.sum(mask) * u.shape[1])
        return loss, y


class CausalConditionalCFM(ConditionalCFM):
    def __init__(self, in_channels=240, cfm_params=CFM_PARAMS, n_spks=1, spk_emb_dim=80, estimator=None):
        super().__init__(in_channels, cfm_params, n_spks, spk_emb_dim, estimator)
        # TODO: BAD BAD IDEA - IT'LL MESS UP DISTILLATION - SETTING TO NONE
        self.rand_noise = None

    @torch.inference_mode()
    def forward(self, mu, mask, n_timesteps, temperature=1.0, spks=None, cond=None, noised_mels=None, meanflow=False):
        """Forward diffusion

        Args:
            mu (torch.Tensor): output of encoder
                shape: (batch_size, n_feats, mel_timesteps)
            mask (torch.Tensor): output_mask
                shape: (batch_size, 1, mel_timesteps)
            n_timesteps (int): number of diffusion steps
            temperature (float, optional): temperature for scaling noise. Defaults to 1.0.
            spks (torch.Tensor, optional): speaker ids. Defaults to None.
                shape: (batch_size, spk_emb_dim)
            cond: Not used but kept for future purposes
            noised_mels: gt mels noised a time t
        Returns:
            sample: generated mel-spectrogram
                shape: (batch_size, n_feats, mel_timesteps)
        """

        B = mu.size(0)
        z = torch.randn_like(mu)

        if noised_mels is not None:
            prompt_len = mu.size(2) - noised_mels.size(2)
            z[..., prompt_len:] = noised_mels

        # time steps for reverse diffusion
        t_span = torch.linspace(0, 1, n_timesteps + 1, device=mu.device, dtype=mu.dtype)
        if (not meanflow) and (self.t_scheduler == 'cosine'):
            t_span = 1 - torch.cos(t_span * 0.5 * torch.pi)

        # NOTE: right now, the only meanflow models are also distilled models, which don't need CFG
        #   because they were distilled with CFG outputs. We would need to add another hparam and
        #   change the conditional logic here if we want to use CFG inference with a meanflow model.
        if meanflow:
            return self.basic_euler(z, t_span=t_span, mu=mu, mask=mask, spks=spks, cond=cond), None

        return self.solve_euler(z, t_span=t_span, mu=mu, mask=mask, spks=spks, cond=cond, meanflow=meanflow), None

    def basic_euler(self, x, t_span, mu, mask, spks, cond):
        in_dtype = x.dtype
        x, t_span, mu, mask, spks, cond = cast_all(x, t_span, mu, mask, spks, cond, dtype=self.estimator.dtype)

        print("S3 Token -> Mel Inference...")
        for t, r in tqdm(zip(t_span[..., :-1], t_span[..., 1:]), total=t_span.shape[-1] - 1):
            t, r = t[None], r[None]
            dxdt = self.estimator.forward(x, mask=mask, mu=mu, t=t, spks=spks, cond=cond, r=r)
            dt = r - t
            x = x + dt * dxdt

        return x.to(in_dtype)
</file>

<file path="src/chatterbox/models/s3gen/s3gen.py">
# Modified from CosyVoice https://github.com/FunAudioLLM/CosyVoice
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging

import numpy as np
import torch
import torchaudio as ta
from functools import lru_cache
from typing import Optional

from ..s3tokenizer import S3_SR, SPEECH_VOCAB_SIZE, S3Tokenizer
from .const import S3GEN_SR
from .flow import CausalMaskedDiffWithXvec
from .xvector import CAMPPlus
from .utils.mel import mel_spectrogram
from .f0_predictor import ConvRNNF0Predictor
from .hifigan import HiFTGenerator
from .transformer.upsample_encoder import UpsampleConformerEncoder
from .flow_matching import CausalConditionalCFM
from .decoder import ConditionalDecoder
from .configs import CFM_PARAMS


def drop_invalid_tokens(x):
    assert len(x.shape) <= 2 and x.shape[0] == 1, "only batch size of one allowed for now"
    return x[x < SPEECH_VOCAB_SIZE]


# TODO: global resampler cache
@lru_cache(100)
def get_resampler(src_sr, dst_sr, device):
    return ta.transforms.Resample(src_sr, dst_sr).to(device)


class S3Token2Mel(torch.nn.Module):
    """
    S3Gen's CFM decoder maps S3 speech tokens to mel-spectrograms.

    TODO: make these modules configurable?
    """
    def __init__(self, meanflow=False):
        super().__init__()
        self.tokenizer = S3Tokenizer("speech_tokenizer_v2_25hz")
        self.mel_extractor = mel_spectrogram # TODO: make it a torch module?
        self.speaker_encoder = CAMPPlus(
            # NOTE: This doesn't affect inference. It turns off activation checkpointing
            # (a training optimization), which causes a crazy DDP error with accelerate
            memory_efficient=False,
        )
        self.meanflow = meanflow

        encoder = UpsampleConformerEncoder(
            output_size=512,
            attention_heads=8,
            linear_units=2048,
            num_blocks=6,
            dropout_rate=0.1,
            positional_dropout_rate=0.1,
            attention_dropout_rate=0.1,
            normalize_before=True,
            input_layer='linear',
            pos_enc_layer_type='rel_pos_espnet',
            selfattention_layer_type='rel_selfattn',
            input_size=512,
            use_cnn_module=False,
            macaron_style=False,
        )

        estimator = ConditionalDecoder(
            in_channels=320,
            out_channels=80,
            causal=True,
            channels=[256],
            dropout=0.0,
            attention_head_dim=64,
            n_blocks=4,
            num_mid_blocks=12,
            num_heads=8,
            act_fn='gelu',
            meanflow=self.meanflow,
        )
        cfm_params = CFM_PARAMS
        decoder = CausalConditionalCFM(
            spk_emb_dim=80,
            cfm_params=cfm_params,
            estimator=estimator,
        )

        self.flow = CausalMaskedDiffWithXvec(
            encoder=encoder,
            decoder=decoder
        )

        self.resamplers = {}

    @property
    def device(self):
        params = self.tokenizer.parameters()
        return next(params).device

    @property
    def dtype(self):
        params = self.flow.parameters()
        return next(params).dtype

    def embed_ref(
        self,
        ref_wav: torch.Tensor,
        ref_sr: int,
        device="auto",
        ref_fade_out=True,
    ):
        device = self.device if device == "auto" else device
        if isinstance(ref_wav, np.ndarray):
            ref_wav = torch.from_numpy(ref_wav).float()

        if ref_wav.device != device:
            ref_wav = ref_wav.to(device)

        if len(ref_wav.shape) == 1:
            ref_wav = ref_wav.unsqueeze(0)  # (B, L)

        if ref_wav.size(1) > 10 * ref_sr:
            print("WARNING: s3gen received ref longer than 10s")

        ref_wav_24 = ref_wav
        if ref_sr != S3GEN_SR:
            ref_wav_24 = get_resampler(ref_sr, S3GEN_SR, device)(ref_wav)
        ref_wav_24 = ref_wav_24.to(device=device, dtype=self.dtype)

        ref_mels_24 = self.mel_extractor(ref_wav_24).transpose(1, 2).to(dtype=self.dtype)
        ref_mels_24_len = None

        # Resample to 16kHz
        ref_wav_16 = ref_wav
        if ref_sr != S3_SR:
            ref_wav_16 = get_resampler(ref_sr, S3_SR, device)(ref_wav)

        # Speaker embedding
        ref_x_vector = self.speaker_encoder.inference(ref_wav_16.to(dtype=self.dtype))

        # Tokenize 16khz reference
        ref_speech_tokens, ref_speech_token_lens = self.tokenizer(ref_wav_16.float())

        # Make sure mel_len = 2 * stoken_len (happens when the input is not padded to multiple of 40ms)
        if ref_mels_24.shape[1] != 2 * ref_speech_tokens.shape[1]:
            logging.warning(
                "Reference mel length is not equal to 2 * reference token length.\n"
            )
            ref_speech_tokens = ref_speech_tokens[:, :ref_mels_24.shape[1] // 2]
            ref_speech_token_lens[0] = ref_speech_tokens.shape[1]

        return dict(
            prompt_token=ref_speech_tokens.to(device),
            prompt_token_len=ref_speech_token_lens,
            prompt_feat=ref_mels_24,
            prompt_feat_len=ref_mels_24_len,
            embedding=ref_x_vector,
        )

    def forward(
        self,
        speech_tokens: torch.LongTensor,
        # locally-computed ref embedding (mutex with ref_dict)
        ref_wav: Optional[torch.Tensor],
        ref_sr: Optional[int],
        # pre-computed ref embedding (prod API)
        ref_dict: Optional[dict] = None,
        n_cfm_timesteps = None,
        finalize: bool = False,
        speech_token_lens=None,
        noised_mels=None,
    ):
        """
        Generate waveforms from S3 speech tokens and a reference waveform, which the speaker timbre is inferred from.

        NOTE:
        - The speaker encoder accepts 16 kHz waveform.
        - S3TokenizerV2 accepts 16 kHz waveform.
        - The mel-spectrogram for the reference assumes 24 kHz input signal.
        - This function is designed for batch_size=1 only.

        Args
        ----
        - `speech_tokens`: S3 speech tokens [B=1, T]
        - `ref_wav`: reference waveform (`torch.Tensor` with shape=[B=1, T])
        - `ref_sr`: reference sample rate
        - `finalize`: whether streaming is finished or not. Note that if False, the last 3 tokens will be ignored.
        """
        assert (ref_wav is None) ^ (ref_dict is None), f"Must provide exactly one of ref_wav or ref_dict (got {ref_wav} and {ref_dict})"

        if ref_dict is None:
            ref_dict = self.embed_ref(ref_wav, ref_sr)
        else:
            # type/device casting (all values will be numpy if it's from a prod API call)
            for rk in list(ref_dict):
                if isinstance(ref_dict[rk], np.ndarray):
                    ref_dict[rk] = torch.from_numpy(ref_dict[rk])
                if torch.is_tensor(ref_dict[rk]):
                    ref_dict[rk] = ref_dict[rk].to(device=self.device, dtype=self.dtype)

        speech_tokens = torch.atleast_2d(speech_tokens)

        # backcompat
        if speech_token_lens is None:
            speech_token_lens = torch.LongTensor([st.size(-1) for st in speech_tokens]).to(self.device)

        output_mels, _ = self.flow.inference(
            token=speech_tokens,
            token_len=speech_token_lens,
            finalize=finalize,
            noised_mels=noised_mels,
            n_timesteps=n_cfm_timesteps,
            meanflow=self.meanflow,
            **ref_dict,
        )
        return output_mels


class S3Token2Wav(S3Token2Mel):
    """
    The decoder of S3Gen is a concat of token-to-mel (CFM) and a mel-to-waveform (HiFiGAN) modules.

    TODO: make these modules configurable?
    """

    ignore_state_dict_missing = ("tokenizer._mel_filters", "tokenizer.window")

    def __init__(self, meanflow=False):
        super().__init__(meanflow)

        f0_predictor = ConvRNNF0Predictor()
        self.mel2wav = HiFTGenerator(
            sampling_rate=S3GEN_SR,
            upsample_rates=[8, 5, 3],
            upsample_kernel_sizes=[16, 11, 7],
            source_resblock_kernel_sizes=[7, 7, 11],
            source_resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],
            f0_predictor=f0_predictor,
        )

        # silence out a few ms and fade audio in to reduce artifacts
        n_trim = S3GEN_SR // 50  # 20ms = half of a frame
        trim_fade = torch.zeros(2 * n_trim)
        trim_fade[n_trim:] = (torch.cos(torch.linspace(torch.pi, 0, n_trim)) + 1) / 2
        self.register_buffer("trim_fade", trim_fade, persistent=False) # (buffers get automatic device casting)
        self.estimator_dtype = "fp32"

    def forward(
        self,
        speech_tokens,
        # locally-computed ref embedding (mutex with ref_dict)
        ref_wav: Optional[torch.Tensor],
        ref_sr: Optional[int],
        # pre-computed ref embedding (prod API)
        ref_dict: Optional[dict] = None,
        finalize: bool = False,
        speech_token_lens=None,
        skip_vocoder=False,
        n_cfm_timesteps=None,
        noised_mels=None,

    ):
        """
        Generate waveforms from S3 speech tokens and a reference waveform, which the speaker timbre is inferred from.
        NOTE: used for sync synthesis only. Please use `S3GenStreamer` for streaming synthesis.
        """
        output_mels = super().forward(
            speech_tokens, speech_token_lens=speech_token_lens, ref_wav=ref_wav,
            ref_sr=ref_sr, ref_dict=ref_dict, finalize=finalize,
            n_cfm_timesteps=n_cfm_timesteps, noised_mels=noised_mels,
        )

        if skip_vocoder:
            return output_mels

        # TODO jrm: ignoring the speed control (mel interpolation) and the HiFTGAN caching mechanisms for now.
        hift_cache_source = torch.zeros(1, 1, 0).to(self.device)

        output_wavs, *_ = self.mel2wav.inference(speech_feat=output_mels, cache_source=hift_cache_source)

        if not self.training:
            # NOTE: ad-hoc method to reduce "spillover" from the reference clip.
            output_wavs[:, :len(self.trim_fade)] *= self.trim_fade

        return output_wavs

    @torch.inference_mode()
    def flow_inference(
        self,
        speech_tokens,
        # locally-computed ref embedding (mutex with ref_dict)
        ref_wav: Optional[torch.Tensor] = None,
        ref_sr: Optional[int] = None,
        # pre-computed ref embedding (prod API)
        ref_dict: Optional[dict] = None,
        n_cfm_timesteps = None,
        finalize: bool = False,
        speech_token_lens=None,
    ):
        n_cfm_timesteps = n_cfm_timesteps or (2 if self.meanflow else 10)
        noise = None
        if self.meanflow:
            noise = torch.randn(1, 80, speech_tokens.size(-1) * 2, dtype=self.dtype, device=self.device)
        output_mels = super().forward(
            speech_tokens, speech_token_lens=speech_token_lens, ref_wav=ref_wav, ref_sr=ref_sr, ref_dict=ref_dict,
            n_cfm_timesteps=n_cfm_timesteps, finalize=finalize, noised_mels=noise,
        )
        return output_mels

    @torch.inference_mode()
    def hift_inference(self, speech_feat, cache_source: torch.Tensor = None):
        if cache_source is None:
            cache_source = torch.zeros(1, 1, 0).to(device=self.device, dtype=self.dtype)
        return self.mel2wav.inference(speech_feat=speech_feat, cache_source=cache_source)

    @torch.inference_mode()
    def inference(
        self,
        speech_tokens,
        # locally-computed ref embedding (mutex with ref_dict)
        ref_wav: Optional[torch.Tensor] = None,
        ref_sr: Optional[int] = None,
        # pre-computed ref embedding (prod API)
        ref_dict: Optional[dict] = None,
        # left as a kwarg because this can change input/output size ratio
        drop_invalid_tokens=True,
        n_cfm_timesteps=None,
        speech_token_lens=None,
    ):
        # hallucination prevention, drop special tokens
        # if drop_invalid_tokens:
        #     speech_tokens, speech_token_lens = drop_invalid(speech_tokens, pad=S3_QUIET_PAD)

        output_mels = self.flow_inference(
            speech_tokens,
            speech_token_lens=speech_token_lens,
            ref_wav=ref_wav,
            ref_sr=ref_sr,
            ref_dict=ref_dict,
            n_cfm_timesteps=n_cfm_timesteps,
            finalize=True,
        )
        output_mels = output_mels.to(dtype=self.dtype) # FIXME (fp16 mode) is this still needed?
        output_wavs, output_sources = self.hift_inference(output_mels, None)

        # NOTE: ad-hoc method to reduce "spillover" from the reference clip.
        output_wavs[:, :len(self.trim_fade)] *= self.trim_fade

        return output_wavs, output_sources
</file>

<file path="src/chatterbox/models/tokenizers/tokenizer.py">
import logging
import json

import torch
from pathlib import Path
from unicodedata import category, normalize
from tokenizers import Tokenizer
from huggingface_hub import hf_hub_download


# Special tokens
SOT = "[START]"
EOT = "[STOP]"
UNK = "[UNK]"
SPACE = "[SPACE]"
SPECIAL_TOKENS = [SOT, EOT, UNK, SPACE, "[PAD]", "[SEP]", "[CLS]", "[MASK]"]

logger = logging.getLogger(__name__)

class EnTokenizer:
    def __init__(self, vocab_file_path):
        self.tokenizer: Tokenizer = Tokenizer.from_file(vocab_file_path)
        self.check_vocabset_sot_eot()

    def check_vocabset_sot_eot(self):
        voc = self.tokenizer.get_vocab()
        assert SOT in voc
        assert EOT in voc

    def text_to_tokens(self, text: str):
        text_tokens = self.encode(text)
        text_tokens = torch.IntTensor(text_tokens).unsqueeze(0)
        return text_tokens

    def encode(self, txt: str):
        """
        clean_text > (append `lang_id`) > replace SPACE > encode text using Tokenizer
        """
        txt = txt.replace(' ', SPACE)
        code = self.tokenizer.encode(txt)
        ids = code.ids
        return ids

    def decode(self, seq):
        if isinstance(seq, torch.Tensor):
            seq = seq.cpu().numpy()

        txt: str = self.tokenizer.decode(seq, skip_special_tokens=False)
        txt = txt.replace(' ', '')
        txt = txt.replace(SPACE, ' ')
        txt = txt.replace(EOT, '')
        txt = txt.replace(UNK, '')
        return txt


# Model repository
REPO_ID = "ResembleAI/chatterbox"

# Global instances for optional dependencies
_kakasi = None
_dicta = None
_russian_stresser = None


def is_kanji(c: str) -> bool:
    """Check if character is kanji."""
    return 19968 <= ord(c) <= 40959


def is_katakana(c: str) -> bool:
    """Check if character is katakana."""
    return 12449 <= ord(c) <= 12538


def hiragana_normalize(text: str) -> str:
    """Japanese text normalization: converts kanji to hiragana; katakana remains the same."""
    global _kakasi
    
    try:
        if _kakasi is None:
            import pykakasi
            _kakasi = pykakasi.kakasi()
        
        result = _kakasi.convert(text)
        out = []
        
        for r in result:
            inp = r['orig']
            hira = r["hira"]

            # Any kanji in the phrase
            if any([is_kanji(c) for c in inp]):
                if hira and hira[0] in ["は", "へ"]:  # Safety check for empty hira
                    hira = " " + hira
                out.append(hira)

            # All katakana
            elif all([is_katakana(c) for c in inp]) if inp else False:  # Safety check for empty inp
                out.append(r['orig'])

            else:
                out.append(inp)
        
        normalized_text = "".join(out)
        
        # Decompose Japanese characters for tokenizer compatibility
        import unicodedata
        normalized_text = unicodedata.normalize('NFKD', normalized_text)
        
        return normalized_text
        
    except ImportError:
        logger.warning("pykakasi not available - Japanese text processing skipped")
        return text


def add_hebrew_diacritics(text: str) -> str:
    """Hebrew text normalization: adds diacritics to Hebrew text."""
    global _dicta
    
    try:
        if _dicta is None:
            from dicta_onnx import Dicta
            _dicta = Dicta()
        
        return _dicta.add_diacritics(text)
        
    except ImportError:
        logger.warning("dicta_onnx not available - Hebrew text processing skipped")
        return text
    except Exception as e:
        logger.warning(f"Hebrew diacritization failed: {e}")
        return text


def korean_normalize(text: str) -> str:
    """Korean text normalization: decompose syllables into Jamo for tokenization."""
    
    def decompose_hangul(char):
        """Decompose Korean syllable into Jamo components."""
        if not ('\uac00' <= char <= '\ud7af'):
            return char
        
        # Hangul decomposition formula
        base = ord(char) - 0xAC00
        initial = chr(0x1100 + base // (21 * 28))
        medial = chr(0x1161 + (base % (21 * 28)) // 28)
        final = chr(0x11A7 + base % 28) if base % 28 > 0 else ''
        
        return initial + medial + final
    
    # Decompose syllables and normalize punctuation
    result = ''.join(decompose_hangul(char) for char in text)    
    return result.strip()


class ChineseCangjieConverter:
    """Converts Chinese characters to Cangjie codes for tokenization."""
    
    def __init__(self, model_dir=None):
        self.word2cj = {}
        self.cj2word = {}
        self.segmenter = None
        self._load_cangjie_mapping(model_dir)
        self._init_segmenter()
    
    def _load_cangjie_mapping(self, model_dir=None):
        """Load Cangjie mapping from HuggingFace model repository."""        
        try:
            cangjie_file = hf_hub_download(
                repo_id=REPO_ID,
                filename="Cangjie5_TC.json",
                cache_dir=model_dir
            )
            
            with open(cangjie_file, "r", encoding="utf-8") as fp:
                data = json.load(fp)
            
            for entry in data:
                word, code = entry.split("\t")[:2]
                self.word2cj[word] = code
                if code not in self.cj2word:
                    self.cj2word[code] = [word]
                else:
                    self.cj2word[code].append(word)
                    
        except Exception as e:
            logger.warning(f"Could not load Cangjie mapping: {e}")
    
    def _init_segmenter(self):
        """Initialize pkuseg segmenter."""
        try:
            from spacy_pkuseg import pkuseg
            self.segmenter = pkuseg()
        except ImportError:
            logger.warning("pkuseg not available - Chinese segmentation will be skipped")
            self.segmenter = None
    
    def _cangjie_encode(self, glyph: str):
        """Encode a single Chinese glyph to Cangjie code."""
        normed_glyph = glyph
        code = self.word2cj.get(normed_glyph, None)
        if code is None:  # e.g. Japanese hiragana
            return None
        index = self.cj2word[code].index(normed_glyph)
        index = str(index) if index > 0 else ""
        return code + str(index)
    

    
    def __call__(self, text):
        """Convert Chinese characters in text to Cangjie tokens."""
        output = []
        if self.segmenter is not None:
            segmented_words = self.segmenter.cut(text)
            full_text = " ".join(segmented_words)
        else:
            full_text = text
        
        for t in full_text:
            if category(t) == "Lo":
                cangjie = self._cangjie_encode(t)
                if cangjie is None:
                    output.append(t)
                    continue
                code = []
                for c in cangjie:
                    code.append(f"[cj_{c}]")
                code.append("[cj_.]")
                code = "".join(code)
                output.append(code)
            else:
                output.append(t)
        return "".join(output)


def add_russian_stress(text: str) -> str:
    """Russian text normalization: adds stress marks to Russian text."""
    global _russian_stresser
    
    try:
        if _russian_stresser is None:
            from russian_text_stresser.text_stresser import RussianTextStresser
            _russian_stresser = RussianTextStresser()
        
        return _russian_stresser.stress_text(text)
        
    except ImportError:
        logger.warning("russian_text_stresser not available - Russian stress labeling skipped")
        return text
    except Exception as e:
        logger.warning(f"Russian stress labeling failed: {e}")
        return text


class MTLTokenizer:
    def __init__(self, vocab_file_path):
        self.tokenizer: Tokenizer = Tokenizer.from_file(vocab_file_path)
        model_dir = Path(vocab_file_path).parent
        self.cangjie_converter = ChineseCangjieConverter(model_dir)
        self.check_vocabset_sot_eot()

    def check_vocabset_sot_eot(self):
        voc = self.tokenizer.get_vocab()
        assert SOT in voc
        assert EOT in voc

    def preprocess_text(self, raw_text: str, language_id: str = None, lowercase: bool = True, nfkd_normalize: bool = True):
        """
        Text preprocessor that handles lowercase conversion and NFKD normalization.
        """
        preprocessed_text = raw_text
        if lowercase:
            preprocessed_text = preprocessed_text.lower()
        if nfkd_normalize:
            preprocessed_text = normalize("NFKD", preprocessed_text)
        
        return preprocessed_text

    def text_to_tokens(self, text: str, language_id: str = None, lowercase: bool = True, nfkd_normalize: bool = True):
        text_tokens = self.encode(text, language_id=language_id, lowercase=lowercase, nfkd_normalize=nfkd_normalize)
        text_tokens = torch.IntTensor(text_tokens).unsqueeze(0)
        return text_tokens

    def encode(self, txt: str, language_id: str = None, lowercase: bool = True, nfkd_normalize: bool = True):
        txt = self.preprocess_text(txt, language_id=language_id, lowercase=lowercase, nfkd_normalize=nfkd_normalize)
        
        # Language-specific text processing
        if language_id == 'zh':
            txt = self.cangjie_converter(txt)
        elif language_id == 'ja':
            txt = hiragana_normalize(txt)
        elif language_id == 'he':
            txt = add_hebrew_diacritics(txt)
        elif language_id == 'ko':
            txt = korean_normalize(txt)
        elif language_id == 'ru':
            txt = add_russian_stress(txt)
        
        # Prepend language token
        if language_id:
            txt = f"[{language_id.lower()}]{txt}"
        
        txt = txt.replace(' ', SPACE)
        return self.tokenizer.encode(txt).ids

    def decode(self, seq):
        if isinstance(seq, torch.Tensor):
            seq = seq.cpu().numpy()

        txt = self.tokenizer.decode(seq, skip_special_tokens=False)
        txt = txt.replace(' ', '').replace(SPACE, ' ').replace(EOT, '').replace(UNK, '')
        return txt
</file>

<file path="src/chatterbox/vc.py">
from pathlib import Path

import librosa
import torch
import perth
from huggingface_hub import hf_hub_download
from safetensors.torch import load_file

from .models.s3tokenizer import S3_SR
from .models.s3gen import S3GEN_SR, S3Gen


REPO_ID = "ResembleAI/chatterbox"


class ChatterboxVC:
    ENC_COND_LEN = 6 * S3_SR
    DEC_COND_LEN = 10 * S3GEN_SR

    def __init__(
        self,
        s3gen: S3Gen,
        device: str,
        ref_dict: dict=None,
    ):
        self.sr = S3GEN_SR
        self.s3gen = s3gen
        self.device = device
        self.watermarker = perth.PerthImplicitWatermarker()
        if ref_dict is None:
            self.ref_dict = None
        else:
            self.ref_dict = {
                k: v.to(device) if torch.is_tensor(v) else v
                for k, v in ref_dict.items()
            }

    @classmethod
    def from_local(cls, ckpt_dir, device) -> 'ChatterboxVC':
        ckpt_dir = Path(ckpt_dir)
        
        # Always load to CPU first for non-CUDA devices to handle CUDA-saved models
        if device in ["cpu", "mps"]:
            map_location = torch.device('cpu')
        else:
            map_location = None
            
        ref_dict = None
        if (builtin_voice := ckpt_dir / "conds.pt").exists():
            states = torch.load(builtin_voice, map_location=map_location)
            ref_dict = states['gen']

        s3gen = S3Gen()
        s3gen.load_state_dict(
            load_file(ckpt_dir / "s3gen.safetensors"), strict=False
        )
        s3gen.to(device).eval()

        return cls(s3gen, device, ref_dict=ref_dict)

    @classmethod
    def from_pretrained(cls, device) -> 'ChatterboxVC':
        # Check if MPS is available on macOS
        if device == "mps" and not torch.backends.mps.is_available():
            if not torch.backends.mps.is_built():
                print("MPS not available because the current PyTorch install was not built with MPS enabled.")
            else:
                print("MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.")
            device = "cpu"
            
        for fpath in ["s3gen.safetensors", "conds.pt"]:
            local_path = hf_hub_download(repo_id=REPO_ID, filename=fpath)

        return cls.from_local(Path(local_path).parent, device)

    def set_target_voice(self, wav_fpath):
        ## Load reference wav
        s3gen_ref_wav, _sr = librosa.load(wav_fpath, sr=S3GEN_SR)

        s3gen_ref_wav = s3gen_ref_wav[:self.DEC_COND_LEN]
        self.ref_dict = self.s3gen.embed_ref(s3gen_ref_wav, S3GEN_SR, device=self.device)

    def generate(
        self,
        audio,
        target_voice_path=None,
    ):
        if target_voice_path:
            self.set_target_voice(target_voice_path)
        else:
            assert self.ref_dict is not None, "Please `prepare_conditionals` first or specify `target_voice_path`"

        with torch.inference_mode():
            audio_16, _ = librosa.load(audio, sr=S3_SR)
            audio_16 = torch.from_numpy(audio_16).float().to(self.device)[None, ]

            s3_tokens, _ = self.s3gen.tokenizer(audio_16)
            wav, _ = self.s3gen.inference(
                speech_tokens=s3_tokens,
                ref_dict=self.ref_dict,
            )
            wav = wav.squeeze(0).detach().cpu().numpy()
            watermarked_wav = self.watermarker.apply_watermark(wav, sample_rate=self.sr)
        return torch.from_numpy(watermarked_wav).unsqueeze(0)
</file>

<file path=".gitignore">
.vscode

# Pylance
pyrightconfig.json

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

syn_out/
checkpoints/
.gradio

# Ignore generated sample .wav files
**/*.wav
candle/target
s3tokenizer-v2-model/model.safetensors
s3tokenizer-v2-model/pytorch_model.bin
</file>

<file path="candle/src/lib.rs">
pub mod audio;
pub mod campplus;
pub mod chatterbox;
pub mod gpt2;
pub mod hifigan;
pub mod modules;
pub mod s3gen;
pub mod s3tokenizer;
pub mod sampling;
pub mod t3_model;
pub mod voice_encoder;

pub use audio::{load_wav, resample, save_wav, S3GEN_SR, S3_SR};
pub use campplus::CAMPPlus;
pub use chatterbox::{ChatterboxTTS, ChatterboxTurboTTS};
pub use s3tokenizer::{ModelConfig as S3TokenizerConfig, S3TokenizerV2};
pub use sampling::LogitsProcessor;

#[derive(Debug, Clone)]
pub struct GenerateConfig {
    pub temperature: f32,
    pub top_p: f32,
    pub top_k: usize,
    pub repetition_penalty: f32,
    pub seed: u64,
    pub normalize_loudness: bool,
    /// Minimum probability threshold (0.0 to disable)
    pub min_p: f32,
    /// Emotion exaggeration factor (maps to emotion_adv in T3)
    pub exaggeration: f32,
}

impl Default for GenerateConfig {
    fn default() -> Self {
        Self {
            temperature: 0.8,
            top_p: 0.95,
            top_k: 1000,
            repetition_penalty: 1.2,
            seed: 0,
            normalize_loudness: true,
            min_p: 0.0,
            exaggeration: 0.0,
        }
    }
}
</file>

<file path="candle/src/main.rs">
//! Chatterbox TTS CLI - Command-line interface for text-to-speech generation.
//!
//! Mirrors the functionality of gradio_tts_turbo_app.py

use candle::{audio, chatterbox::ChatterboxTurboTTS, GenerateConfig};
use candle_core::Device;
use clap::Parser;
use std::path::PathBuf;

#[derive(Parser, Debug)]
#[command(
    name = "chatterbox",
    author,
    version,
    about = "Chatterbox TTS - High-quality text-to-speech synthesis",
    long_about = "Generate speech from text using the Chatterbox Turbo model.\n\n\
                  Requires a reference audio file (5+ seconds) for voice cloning."
)]
struct Args {
    /// Text to synthesize
    #[arg(short, long)]
    text: String,

    /// Reference audio file for voice cloning (WAV, 5+ seconds)
    #[arg(short, long)]
    ref_audio: PathBuf,

    /// Output audio file path
    #[arg(short, long, default_value = "output.wav")]
    output: PathBuf,

    /// Device to run on
    #[arg(long, default_value = "cpu", value_parser = ["cpu", "cuda"])]
    device: String,

    /// Sampling temperature (0.05-2.0)
    #[arg(long, default_value_t = 0.8)]
    temperature: f32,

    /// Top-p nucleus sampling (0.0-1.0)
    #[arg(long, default_value_t = 0.95)]
    top_p: f32,

    /// Top-k sampling (0-1000)
    #[arg(long, default_value_t = 1000)]
    top_k: usize,

    /// Repetition penalty (1.0-2.0)
    #[arg(long, default_value_t = 1.2)]
    repetition_penalty: f32,

    /// Random seed (0 for random)
    #[arg(long, default_value_t = 0)]
    seed: u64,

    /// Normalize output loudness to -27 LUFS
    #[arg(long, default_value_t = true)]
    normalize: bool,
}

fn main() -> anyhow::Result<()> {
    let args = Args::parse();

    // Select device
    let device = match args.device.as_str() {
        "cuda" => {
            println!("Using CUDA device...");
            Device::new_cuda(0)?
        }
        _ => {
            println!("Using CPU device...");
            Device::Cpu
        }
    };

    // Validate reference audio exists
    if !args.ref_audio.exists() {
        anyhow::bail!(
            "Reference audio file not found: {}",
            args.ref_audio.display()
        );
    }

    println!("Loading Chatterbox Turbo model...");
    let model = ChatterboxTurboTTS::from_pretrained(device.clone())?;
    println!("Model loaded successfully.");

    // Set random seed if specified
    if args.seed != 0 {
        // Note: Candle doesn't have global seed setting, handled per-operation
        println!("Using seed: {}", args.seed);
    }

    // Create generation config
    let config = GenerateConfig {
        temperature: args.temperature,
        top_p: args.top_p,
        top_k: args.top_k,
        repetition_penalty: args.repetition_penalty,
        seed: args.seed,
        normalize_loudness: args.normalize,
        ..Default::default()
    };

    println!("Generating speech for: \"{}\"", args.text);
    println!("Reference audio: {}", args.ref_audio.display());

    let (samples, sample_rate) = model.generate_speech(&args.text, &args.ref_audio, config)?;

    // Save output
    audio::save_wav(&args.output, &samples, sample_rate).map_err(|e| anyhow::anyhow!("{}", e))?;
    println!(
        "Audio saved to: {} ({} samples @ {}Hz)",
        args.output.display(),
        samples.len(),
        sample_rate
    );

    Ok(())
}
</file>

<file path="example_tts.py">
import torchaudio as ta
import torch
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# Automatically detect the best available device
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"

print(f"Using device: {device}")

model = ChatterboxTTS.from_pretrained(device=device)

text = "Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill."
wav = model.generate(text)
ta.save("test-1.wav", wav, model.sr)

multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)
text = "Bonjour, comment ça va? Ceci est le modèle de synthèse vocale multilingue Chatterbox, il prend en charge 23 langues."
wav = multilingual_model.generate(text, language_id="fr")
ta.save("test-2.wav", wav, multilingual_model.sr)


# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = "YOUR_FILE.wav"
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save("test-3.wav", wav, model.sr)
</file>

<file path="gradio_tts_app.py">
import random
import numpy as np
import torch
import gradio as gr
from chatterbox.tts import ChatterboxTTS


DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


def set_seed(seed: int):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    np.random.seed(seed)


def load_model():
    model = ChatterboxTTS.from_pretrained(DEVICE)
    return model


def generate(model, text, audio_prompt_path, exaggeration, temperature, seed_num, cfgw, min_p, top_p, repetition_penalty):
    if model is None:
        model = ChatterboxTTS.from_pretrained(DEVICE)

    if seed_num != 0:
        set_seed(int(seed_num))

    wav = model.generate(
        text,
        audio_prompt_path=audio_prompt_path,
        exaggeration=exaggeration,
        temperature=temperature,
        cfg_weight=cfgw,
        min_p=min_p,
        top_p=top_p,
        repetition_penalty=repetition_penalty,
    )
    return (model.sr, wav.squeeze(0).numpy())


with gr.Blocks() as demo:
    model_state = gr.State(None)  # Loaded once per session/user

    with gr.Row():
        with gr.Column():
            text = gr.Textbox(
                value="Now let's make my mum's favourite. So three mars bars into the pan. Then we add the tuna and just stir for a bit, just let the chocolate and fish infuse. A sprinkle of olive oil and some tomato ketchup. Now smell that. Oh boy this is going to be incredible.",
                label="Text to synthesize (max chars 300)",
                max_lines=5
            )
            ref_wav = gr.Audio(sources=["upload", "microphone"], type="filepath", label="Reference Audio File", value=None)
            exaggeration = gr.Slider(0.25, 2, step=.05, label="Exaggeration (Neutral = 0.5, extreme values can be unstable)", value=.5)
            cfg_weight = gr.Slider(0.0, 1, step=.05, label="CFG/Pace", value=0.5)

            with gr.Accordion("More options", open=False):
                seed_num = gr.Number(value=0, label="Random seed (0 for random)")
                temp = gr.Slider(0.05, 5, step=.05, label="temperature", value=.8)
                min_p = gr.Slider(0.00, 1.00, step=0.01, label="min_p || Newer Sampler. Recommend 0.02 > 0.1. Handles Higher Temperatures better. 0.00 Disables", value=0.05)
                top_p = gr.Slider(0.00, 1.00, step=0.01, label="top_p || Original Sampler. 1.0 Disables(recommended). Original 0.8", value=1.00)
                repetition_penalty = gr.Slider(1.00, 2.00, step=0.1, label="repetition_penalty", value=1.2)

            run_btn = gr.Button("Generate", variant="primary")

        with gr.Column():
            audio_output = gr.Audio(label="Output Audio")

    demo.load(fn=load_model, inputs=[], outputs=model_state)

    run_btn.click(
        fn=generate,
        inputs=[
            model_state,
            text,
            ref_wav,
            exaggeration,
            temp,
            seed_num,
            cfg_weight,
            min_p,
            top_p,
            repetition_penalty,
        ],
        outputs=audio_output,
    )

if __name__ == "__main__":
    demo.queue(
        max_size=50,
        default_concurrency_limit=1,
    ).launch(share=True)
</file>

<file path="src/chatterbox/models/s3gen/flow.py">
# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu, Zhihao Du)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import logging
import random
from typing import Dict, Optional

logger = logging.getLogger(__name__)
import torch
import torch.nn as nn
from torch.nn import functional as F
from .utils.mask import make_pad_mask
from .configs import CFM_PARAMS
from omegaconf import DictConfig


logger = logging.getLogger(__name__)


def _repeat_batch_dim(tnsr, B, ndim):
    "repeat batch dimension if it's equal to 1"
    if tnsr is not None:
        # add missing batch dim if needed
        while tnsr.ndim < ndim:
            tnsr = tnsr[None]
        # repeat batch dim as needed
        if B > 1 and tnsr.size(0) == 1:
            tnsr = tnsr.repeat(B, *([1] * (ndim - 1)))
        assert tnsr.ndim == ndim, f"Expected {ndim=}, got {tnsr.ndim=}"
    return tnsr


class CausalMaskedDiffWithXvec(torch.nn.Module):
    def __init__(self,
                 input_size: int = 512,
                 output_size: int = 80,
                 spk_embed_dim: int = 192,
                 output_type: str = "mel",
                 vocab_size: int = 6561,
                 input_frame_rate: int = 25,
                 only_mask_loss: bool = True,
                 token_mel_ratio: int = 2,
                 pre_lookahead_len: int = 3,
                 encoder: torch.nn.Module = None,
                 decoder: torch.nn.Module = None,
                 decoder_conf: Dict = {'in_channels': 240, 'out_channel': 80, 'spk_emb_dim': 80, 'n_spks': 1,
                                       'cfm_params': DictConfig(
                                           {'sigma_min': 1e-06, 'solver': 'euler', 't_scheduler': 'cosine',
                                            'training_cfg_rate': 0.2, 'inference_cfg_rate': 0.7,
                                            'reg_loss_type': 'l1'}),
                                       'decoder_params': {'channels': [256, 256], 'dropout': 0.0,
                                                          'attention_head_dim': 64,
                                                          'n_blocks': 4, 'num_mid_blocks': 12, 'num_heads': 8,
                                                          'act_fn': 'gelu'}},
                 mel_feat_conf: Dict = {'n_fft': 1024, 'num_mels': 80, 'sampling_rate': 22050,
                                        'hop_size': 256, 'win_size': 1024, 'fmin': 0, 'fmax': 8000}):
        super().__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.decoder_conf = decoder_conf
        self.mel_feat_conf = mel_feat_conf
        self.vocab_size = vocab_size
        self.output_type = output_type
        self.input_frame_rate = input_frame_rate
        logging.info(f"input frame rate={self.input_frame_rate}")
        self.input_embedding = nn.Embedding(vocab_size, input_size)
        self.spk_embed_affine_layer = torch.nn.Linear(spk_embed_dim, output_size)
        self.encoder = encoder
        self.encoder_proj = torch.nn.Linear(self.encoder.output_size(), output_size)
        self.decoder = decoder
        self.only_mask_loss = only_mask_loss
        self.token_mel_ratio = token_mel_ratio
        self.pre_lookahead_len = pre_lookahead_len

    # NOTE: copied in from cosyvoice repo
    def compute_loss(
            self,
            batch: dict,
            device: torch.device,
    ) -> Dict[str, Optional[torch.Tensor]]:
        token = batch['speech_token'].to(device)
        token_len = batch['speech_token_len'].to(device)
        feat = batch['speech_feat'].to(device)  # (B, 80, T)
        feat_len = batch['speech_feat_len'].to(device)
        embedding = batch['embedding'].to(device)

        # NOTE unified training, static_chunk_size > 0 or = 0
        # streaming = True if random.random() < 0.5 else False

        # xvec projection
        embedding = F.normalize(embedding, dim=1)
        embedding = self.spk_embed_affine_layer(embedding)

        # concat text and prompt_text
        mask = (~make_pad_mask(token_len)).float().unsqueeze(-1).to(device)  # (B, T, 1)
        token = self.input_embedding(torch.clamp(token, min=0)) * mask  # (B, T, emb)

        # text encode
        h, h_lengths = self.encoder(token, token_len)  # (B, T, C) -> (B, 2T, C)
        h = self.encoder_proj(h)

        # get conditions
        conds = torch.zeros(feat.shape, device=token.device)
        for i, j in enumerate(feat_len):
            if random.random() < 0.5:
                continue
            index = random.randint(0, int(0.3 * j))
            conds[i, :, :index] = feat[i, :, :index]

        mask = (~make_pad_mask(h_lengths.sum(dim=-1).squeeze(dim=1))).to(h)
        loss, _ = self.decoder.compute_loss(
            feat.contiguous(),
            mask.unsqueeze(1),
            h.transpose(1, 2).contiguous(),
            embedding,
            cond=conds,
            # streaming=streaming,
        )
        return {'loss': loss}

    @torch.inference_mode()
    def inference(self,
                  token,
                  token_len,
                  prompt_token,
                  prompt_token_len,
                  prompt_feat,
                  prompt_feat_len,
                  embedding,
                  finalize,
                  n_timesteps=10,
                  noised_mels=None,
                  meanflow=False):
        # token: (B, n_toks)
        # token_len: (B,)
        B = token.size(0)

        # xvec projection
        embedding = torch.atleast_2d(embedding)
        embedding = F.normalize(embedding, dim=1)
        embedding = self.spk_embed_affine_layer(embedding)  # (1 or B, emb_dim)

        # adjust shapes (batching logic)
        prompt_token = _repeat_batch_dim(prompt_token, B, ndim=2)  # (B, n_prompt)
        prompt_token_len = _repeat_batch_dim(prompt_token_len, B, ndim=1)  # (B,)
        prompt_feat = _repeat_batch_dim(prompt_feat, B, ndim=3)  # (B, n_feat, feat_dim=80)
        prompt_feat_len = _repeat_batch_dim(prompt_feat_len, B, ndim=1)  # (B,) or None
        embedding = _repeat_batch_dim(embedding, B, ndim=2)  # (B, emb_dim)

        # concat text and prompt_text
        token, token_len = torch.concat([prompt_token, token], dim=1), prompt_token_len + token_len
        mask = (~make_pad_mask(token_len)).unsqueeze(-1).to(embedding)

        if (token >= self.vocab_size).any():
            logger.error(f"{token.max()}>{self.vocab_size}\n out-of-range special tokens found in flow, fix inputs!")
        token = self.input_embedding(token.long()) * mask

        # text encode
        h, h_masks = self.encoder(token, token_len)
        if finalize is False:
            h = h[:, :-self.pre_lookahead_len * self.token_mel_ratio]

        h_lengths = h_masks.sum(dim=-1).squeeze(dim=-1)
        mel_len1, mel_len2 = prompt_feat.shape[1], h.shape[1] - prompt_feat.shape[1]
        h = self.encoder_proj(h)

        # # get conditions
        conds = torch.zeros([B, mel_len1 + mel_len2, self.output_size], device=token.device).to(h.dtype)
        conds[:, :mel_len1] = prompt_feat
        conds = conds.transpose(1, 2)

        mask = (~make_pad_mask(h_lengths)).unsqueeze(1).to(h)

        if mask.shape[0] != B:
            mask = mask.repeat(B, 1, 1)

        feat, _ = self.decoder(
            mu=h.transpose(1, 2).contiguous(),
            mask=mask,
            spks=embedding,
            cond=conds,
            n_timesteps=n_timesteps,
            noised_mels=noised_mels,
            meanflow=meanflow,
        )
        feat = feat[:, :, mel_len1:]
        assert feat.shape[2] == mel_len2
        return feat, None  # NOTE jrm: why are they returning None here?
</file>

<file path="src/chatterbox/models/t3/modules/t3_config.py">
from ..llama_configs import LLAMA_CONFIGS


class T3Config:
    def __init__(self, text_tokens_dict_size=704):
        self.start_text_token = 255
        self.stop_text_token = 0
        self.text_tokens_dict_size = text_tokens_dict_size
        self.max_text_tokens = 2048

        self.start_speech_token = 6561
        self.stop_speech_token = 6562
        self.speech_tokens_dict_size = 8194
        self.max_speech_tokens = 4096

        self.llama_config_name = "Llama_520M"
        self.input_pos_emb = "learned"
        self.speech_cond_prompt_len = 150

        self.encoder_type = "voice_encoder"
        self.speaker_embed_size = 256
        self.use_perceiver_resampler = True
        self.emotion_adv = True

    @property
    def n_channels(self):
        return LLAMA_CONFIGS[self.llama_config_name]["hidden_size"]
    
    @property
    def is_multilingual(self):
        return self.text_tokens_dict_size == 2454

    @classmethod
    def english_only(cls):
        """Create configuration for English-only TTS model."""
        return cls(text_tokens_dict_size=704)
    
    @classmethod 
    def multilingual(cls):
        """Create configuration for multilingual TTS model."""
        return cls(text_tokens_dict_size=2454)
</file>

<file path="candle/Cargo.toml">
[package]
name = "candle"
version = "0.1.0"
edition = "2021"

[dependencies]
candle-core = { version = "0.9.1" }
candle-nn = { version = "0.9.1" }
candle-transformers = "0.9.1"
tokenizers = "0.22.2"
anyhow = "1.0.100"
serde = { version = "1.0.228", features = ["derive"] }
serde_json = "1.0.148"
byteorder = "1.5.0"
hf-hub = "0.4.3"
clap = { version = "4.5.53", features = ["derive"] }
hound = "3.5"
rubato = "0.15"
realfft = "3.4"
rand = "0.8"
memmap2 = "0.9"
safetensors = "0.5.2"
rustfft = "6.4.1"

[features]
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
default = []

[lib]
name = "candle"
path = "src/lib.rs"

[[bin]]
name = "chatterbox"
path = "src/main.rs"
</file>

<file path="candle/examples/split_generate.rs">
use candle_core::{DType, Device, Tensor};
use candle_nn::VarBuilder;
use hf_hub::api::sync::Api;
use tokenizers::Tokenizer;

fn process_mel_for_s3tokenizer(mel: &Tensor) -> anyhow::Result<Tensor> {
    // Python: log10(clamp(min=1e-10)) -> max(x, x.max() - 8.0) -> (x + 4.0) / 4.0
    let mel = mel.clamp(1e-10, f32::MAX)?;
    let log_mel = ((mel.log()? / 10.0f64.ln())? * 2.0)?; // ln(x) / ln(10) = log10(x) * 2.0 for Power Spectrogram

    // Dynamic range compression
    let max_val = log_mel.max_all()?.to_scalar::<f32>()?;
    let log_mel = log_mel.maximum(max_val - 8.0)?;

    let result = ((log_mel + 4.0)? / 4.0)?;
    Ok(result)
}

fn main() -> anyhow::Result<()> {
    let args: Vec<String> = std::env::args().collect();
    let text = args
        .iter()
        .position(|a| a == "--text")
        .and_then(|i| args.get(i + 1))
        .map(|s| s.as_str())
        .unwrap_or("Hello, world.");
    let ref_audio = args
        .iter()
        .position(|a| a == "--ref-audio")
        .and_then(|i| args.get(i + 1))
        .map(|s| s.as_str())
        .unwrap_or("reference.wav");
    let output = args
        .iter()
        .position(|a| a == "--output")
        .and_then(|i| args.get(i + 1))
        .map(|s| s.as_str())
        .unwrap_or("output.wav");
    let use_cuda = args.iter().any(|a| a == "--cuda");
    let use_t3_cuda = args.iter().any(|a| a == "--t3-cuda");
    let use_fp16 = args.iter().any(|a| a == "--fp16");
    let device = if use_cuda {
        Device::new_cuda(0)?
    } else {
        Device::Cpu
    };
    let t3_device = if use_t3_cuda {
        Device::new_cuda(0)?
    } else {
        device.clone()
    };
    let dtype = if use_fp16 { DType::F16 } else { DType::F32 };

    println!("Downloading model files...");
    let api = Api::new().map_err(|e| anyhow::anyhow!("{e}"))?;
    let turbo_repo = api.model("ResembleAI/chatterbox-turbo".to_string());
    let t3_path = turbo_repo.get("t3_turbo_v1.safetensors")?;
    println!("[DEBUG] T3 Path: {:?}", t3_path);
    {
        let file = std::fs::File::open(&t3_path).unwrap();
        let mem = unsafe { memmap2::MmapOptions::new().map(&file).unwrap() };
        let safetensors = safetensors::SafeTensors::deserialize(&mem).unwrap();
        let mut names: Vec<_> = safetensors.names().into_iter().collect();
        names.sort();
        println!("[DEBUG] Keys in t3_turbo_v1.safetensors:");
        for name in names {
            println!("  {}", name);
        }
    }
    let s3gen_path = turbo_repo.get("s3gen_meanflow.safetensors")?;
    let ve_path = turbo_repo.get("ve.safetensors")?;
    let s3tokenizer_path = api
        .model("ResembleAI/s3tokenizer-v2".to_string())
        .get("model.safetensors")?;
    let tokenizer_path = api
        .model("ResembleAI/chatterbox".to_string())
        .get("tokenizer.json")?;

    // STEP 1: Text
    let tokenizer = Tokenizer::from_file(&tokenizer_path).map_err(|e| anyhow::anyhow!("{e}"))?;
    let encoding = tokenizer
        .encode(text, true)
        .map_err(|e| anyhow::anyhow!("{e}"))?;
    let text_tokens: Vec<i64> = encoding.get_ids().iter().map(|&x| x as i64).collect();
    let text_tokens_tensor =
        Tensor::from_vec(text_tokens.clone(), (1, text_tokens.len()), &device)?;

    // STEP 2: Reference Audio Processing
    let (ref_samples, ref_sr) =
        candle::audio::load_wav(ref_audio).map_err(|e| anyhow::anyhow!("{e}"))?;
    let ref_samples_16k = if ref_sr != candle::audio::S3_SR {
        candle::audio::resample(&ref_samples, ref_sr, candle::audio::S3_SR)
            .map_err(|e| anyhow::anyhow!("{e}"))?
    } else {
        ref_samples.clone()
    };

    let ref_samples_24k = if ref_sr != candle::audio::S3GEN_SR {
        candle::audio::resample(&ref_samples, ref_sr, candle::audio::S3GEN_SR)
            .map_err(|e| anyhow::anyhow!("{e}"))?
    } else {
        ref_samples.clone()
    };

    // NOTE: compute_mel_spectrogram now returns LINEAR MAGNITUDE (unlogged)

    // 1. Config for VoiceEncoder (16kHz, 40 mels, n_fft=400)
    let config_ve = candle::audio::MelConfig {
        n_fft: 400,
        hop_length: 160,
        win_length: 400,
        n_mels: 40,
        fmax: 8000.0,
    };

    // 2. Config for S3Tokenizer (16kHz, 128 mels, n_fft=400)
    let config_s3tok = candle::audio::MelConfig {
        n_fft: 400,
        hop_length: 160,
        win_length: 400,
        n_mels: 128,
        fmax: 8000.0,
    };

    let mel_40_linear = candle::audio::compute_mel_spectrogram(
        &ref_samples_16k,
        candle::audio::S3_SR,
        &device,
        &config_ve,
    )?;

    // Used for S3Tokenizer (no longer using the default 16k config which was 80 mels)
    let mel_128_linear = candle::audio::compute_mel_spectrogram(
        &ref_samples_16k,
        candle::audio::S3_SR,
        &device,
        &config_s3tok,
    )?;

    // S3Gen/CAMPPlus (24kHz, 80 mels, n_fft=1920) - This was already correct
    let mel_80_24k_linear = candle::audio::compute_mel_spectrogram(
        &ref_samples_24k,
        candle::audio::S3GEN_SR,
        &device,
        &candle::audio::MelConfig::for_24k(80),
    )?;

    // --- STEP 3: S3 Prompt Tokens ---
    // Use mel_128_linear directly (no padding needed anymore)
    let mel_for_tokenizer = process_mel_for_s3tokenizer(&mel_128_linear)?;
    let speech_prompt_tokens = {
        let vb = unsafe {
            VarBuilder::from_mmaped_safetensors(&[&s3tokenizer_path], DType::F32, &device)?
        };
        let s3tok = candle::s3tokenizer::S3TokenizerV2::new(
            &candle::s3tokenizer::ModelConfig::default(),
            vb,
        )?;

        // Input is already (B, 128, T), so just encode directly
        s3tok.encode(&mel_for_tokenizer)?
    };

    // --- STEP 4: T3 Embedding (Voice Encoder) ---
    println!("\n[Step 4/6] Extracting speaker embedding with VoiceEncoder...");
    let spk_emb_256 = {
        let vb = unsafe { VarBuilder::from_mmaped_safetensors(&[&ve_path], DType::F32, &device)? };
        let ve = candle::voice_encoder::VoiceEncoder::new(
            candle::voice_encoder::VoiceEncoderConfig::default(),
            vb,
        )?;
        // VoiceEncoder expects Power Mel Spectrogram (amp^2)
        let mel_40_power = mel_40_linear.sqr()?;
        // VoiceEncoder expects (B, T, 40)
        ve.forward(&mel_40_power.transpose(1, 2)?)?
    };

    // --- STEP 5: S3Gen Embedding (CAMPPlus) ---
    println!("[Step 5/6] Extracting synthesis embedding with CAMPPlus...");
    // CAMPPlus expects Mean-Normalized Log-Mel
    let mel_80_log = mel_80_24k_linear.clamp(1e-5, f32::MAX)?.log()?;
    let mean = mel_80_log.mean_keepdim(2)?;
    let mel_for_campplus = mel_80_log.broadcast_sub(&mean)?;

    let spk_emb_80 = {
        let vb =
            unsafe { VarBuilder::from_mmaped_safetensors(&[&s3gen_path], DType::F32, &device)? };
        let campplus = candle::campplus::CAMPPlus::new(80, 192, vb.pp("speaker_encoder"))?;
        campplus.forward(&mel_for_campplus)?.narrow(1, 0, 80)?
    };

    // STEP 6: T3 Generation
    println!("\n[Step 6/6] Generating tokens with T3...");
    let speech_tokens = {
        let vb = unsafe { VarBuilder::from_mmaped_safetensors(&[&t3_path], dtype, &t3_device)? };
        let t3_config = candle::t3_model::T3Config {
            text_tokens_dict_size: 50276,
            speech_tokens_dict_size: 6563,
            hidden_size: 1024,
            num_layers: 24,
            num_heads: 16,
            vocab_size: 50276,
            speaker_embed_size: 256,
            start_speech_token: 6561,
            stop_speech_token: 6562,
            speech_cond_prompt_len: Some(375),
            use_perceiver_resampler: false,
            emotion_adv: false,
            n_positions: 8196,
        };
        let t3 = candle::t3_model::T3::new(t3_config, vb)?;
        t3.generate(
            &text_tokens_tensor.to_device(&t3_device)?,
            &spk_emb_256.to_device(&t3_device)?.to_dtype(dtype)?,
            Some(&speech_prompt_tokens.to_device(&t3_device)?),
            None,
            500,
            0.8,
            0.95,
            50,
            1.2,
            42,
        )?
    };

    // STEP 7: Synthesis
    println!("\n[Step 7/6] Synthesizing audio...");

    // 1. Prepare the full sequence of tokens: [Prompt Tokens, Generated Tokens]
    let speech_tokens_filtered = {
        let tokens = speech_tokens.to_vec2::<u32>()?[0].clone();
        let filtered: Vec<u32> = tokens
            .into_iter()
            .filter(|&t| t != 6561 && t != 6562)
            .collect();
        Tensor::from_vec(filtered.clone(), (1, filtered.len()), &device)?
    };

    // Concatenate prompt tokens + generated tokens
    // Note: speech_prompt_tokens was calculated in Step 3.
    // We assume speech_prompt_tokens is (1, N_PROMPT).
    let s3_input_tokens = Tensor::cat(&[&speech_prompt_tokens, &speech_tokens_filtered], 1)?;
    println!("  S3Gen Input Tokens Shape: {:?}", s3_input_tokens.dims());

    let audio_tensor = {
        let vb =
            unsafe { VarBuilder::from_mmaped_safetensors(&[&s3gen_path], DType::F32, &device)? };
        let s3gen = candle::s3gen::S3Gen::new(vb, true)?;

        // 2. Prepare Reference Mel for Conditioning
        // S3Gen expects Natural Log of Linear Magnitude Mel (not Power, not Log10)
        // mel_80_24k_linear is (1, 80, T_ref)
        let ref_mel_log = mel_80_24k_linear.clamp(1e-5, f32::MAX)?.log()?;

        // 3. Construct the Conditioning Tensor [Ref Mel | Zeros]
        // The S3Gen Encoder upsamples tokens by 2x.
        // So output length = input_tokens_len * 2.
        let (_, n_toks) = s3_input_tokens.dims2()?;
        let target_len = n_toks * 2;
        let (b, c, ref_len) = ref_mel_log.dims3()?;

        // Ensure we don't overflow if ref is somehow longer than target (unlikely if logic is correct)
        let safe_ref_len = ref_len.min(target_len);
        let ref_mel_log_trimmed = ref_mel_log.narrow(2, 0, safe_ref_len)?;

        // Create padding
        let padding_len = target_len - safe_ref_len;
        let cond = if padding_len > 0 {
            let padding = Tensor::zeros((b, c, padding_len), DType::F32, &device)?;
            Tensor::cat(&[&ref_mel_log_trimmed, &padding], 2)?
        } else {
            ref_mel_log_trimmed
        };

        println!("  Conditioning Tensor Shape: {:?}", cond.dims());

        // 4. Forward with corrected inputs
        s3gen.forward(
            &s3_input_tokens,
            Some(&spk_emb_80),
            Some(&cond), // Pass the time-aligned conditioning, not the mean!
        )?
    };

    let samples: Vec<f32> = audio_tensor.flatten_all()?.to_vec1()?;
    candle::audio::save_wav(output, &samples, candle::audio::S3GEN_SR)
        .map_err(|e| anyhow::anyhow!("{e}"))?;
    println!("SUCCESS! Audio saved to: {}", output);

    Ok(())
}
</file>

<file path="candle/src/campplus.rs">
use candle_core::{Module, Result, Tensor};
use candle_nn::{Activation, BatchNorm, Conv1d, Conv2d, ModuleT, VarBuilder};

fn conv2d_no_bias(
    in_c: usize,
    out_c: usize,
    k: usize,
    cfg: candle_nn::Conv2dConfig,
    vb: VarBuilder,
) -> Result<Conv2d> {
    let weight = vb.get((out_c, in_c / cfg.groups, k, k), "weight")?;
    Ok(Conv2d::new(weight, None, cfg))
}

fn conv1d_no_bias(
    in_c: usize,
    out_c: usize,
    k: usize,
    cfg: candle_nn::Conv1dConfig,
    vb: VarBuilder,
) -> Result<Conv1d> {
    let weight = vb.get((out_c, in_c / cfg.groups, k), "weight")?;
    Ok(Conv1d::new(weight, None, cfg))
}

struct BasicResBlock2D {
    conv1: Conv2d,
    bn1: BatchNorm,
    conv2: Conv2d,
    bn2: BatchNorm,
    shortcut: Option<(Conv2d, BatchNorm)>,
}

impl BasicResBlock2D {
    fn new(in_c: usize, out_c: usize, stride: usize, vb: VarBuilder) -> Result<Self> {
        let conv_cfg = candle_nn::Conv2dConfig {
            stride,
            padding: 1,
            ..Default::default()
        };
        let conv1 = conv2d_no_bias(in_c, out_c, 3, conv_cfg, vb.pp("conv1"))?;
        let bn1 = candle_nn::batch_norm(out_c, 1e-5, vb.pp("bn1"))?;

        let conv2 = conv2d_no_bias(
            out_c,
            out_c,
            3,
            candle_nn::Conv2dConfig {
                padding: 1,
                ..Default::default()
            },
            vb.pp("conv2"),
        )?;
        let bn2 = candle_nn::batch_norm(out_c, 1e-5, vb.pp("bn2"))?;

        let shortcut = if stride != 1 || in_c != out_c {
            let s_conv = conv2d_no_bias(
                in_c,
                out_c,
                1,
                candle_nn::Conv2dConfig {
                    stride,
                    ..Default::default()
                },
                vb.pp("shortcut.0"),
            )?;
            let s_bn = candle_nn::batch_norm(out_c, 1e-5, vb.pp("shortcut.1"))?;
            Some((s_conv, s_bn))
        } else {
            None
        };

        Ok(Self {
            conv1,
            bn1,
            conv2,
            bn2,
            shortcut,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let residual = if let Some((s_conv, s_bn)) = &self.shortcut {
            s_bn.forward_t(&s_conv.forward(x)?, false)?
        } else {
            x.clone()
        };

        let x = self.bn1.forward_t(&self.conv1.forward(x)?, false)?.relu()?;
        let x = self.bn2.forward_t(&self.conv2.forward(&x)?, false)?;
        (x + residual)?.relu()
    }
}

struct FCM {
    bn1: BatchNorm,
    conv1: Conv2d,
    layer1: Vec<BasicResBlock2D>,
    layer2: Vec<BasicResBlock2D>,
    conv2: Conv2d,
    bn2: BatchNorm,
    out_channels: usize,
}

impl FCM {
    fn new(m_channels: usize, feat_dim: usize, vb: VarBuilder) -> Result<Self> {
        let conv1 = conv2d_no_bias(
            1,
            m_channels,
            3,
            candle_nn::Conv2dConfig {
                padding: 1,
                ..Default::default()
            },
            vb.pp("conv1"),
        )?;
        let bn1 = candle_nn::batch_norm(m_channels, 1e-5, vb.pp("bn1"))?;

        let mut layer1 = Vec::new();
        layer1.push(BasicResBlock2D::new(
            m_channels,
            m_channels,
            2,
            vb.pp("layer1.0"),
        )?);
        layer1.push(BasicResBlock2D::new(
            m_channels,
            m_channels,
            1,
            vb.pp("layer1.1"),
        )?);

        let mut layer2 = Vec::new();
        layer2.push(BasicResBlock2D::new(
            m_channels,
            m_channels,
            2,
            vb.pp("layer2.0"),
        )?);
        layer2.push(BasicResBlock2D::new(
            m_channels,
            m_channels,
            1,
            vb.pp("layer2.1"),
        )?);

        let conv2 = conv2d_no_bias(
            m_channels,
            m_channels,
            3,
            candle_nn::Conv2dConfig {
                stride: 2,
                padding: 1,
                ..Default::default()
            },
            vb.pp("conv2"),
        )?;
        let bn2 = candle_nn::batch_norm(m_channels, 1e-5, vb.pp("bn2"))?;

        let out_channels = m_channels * (feat_dim / 8);

        Ok(Self {
            bn1,
            conv1,
            layer1,
            layer2,
            conv2,
            bn2,
            out_channels,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let mut x = self.bn1.forward_t(&self.conv1.forward(x)?, false)?.relu()?;
        for block in &self.layer1 {
            x = block.forward(&x)?;
        }
        for block in &self.layer2 {
            x = block.forward(&x)?;
        }
        x = self
            .bn2
            .forward_t(&self.conv2.forward(&x)?, false)?
            .relu()?;
        let (b, c, f, t) = x.dims4()?;
        x.reshape((b, c * f, t))
    }
}

struct TDNNLayer {
    conv: Conv1d,
    bn: BatchNorm,
    activation: Activation,
}

impl TDNNLayer {
    fn new(
        in_c: usize,
        out_c: usize,
        k: usize,
        dilation: usize,
        stride: usize,
        vb: VarBuilder,
    ) -> Result<Self> {
        let padding = (k - 1) / 2 * dilation;
        let conv_cfg = candle_nn::Conv1dConfig {
            padding,
            dilation,
            stride,
            ..Default::default()
        };
        let conv = conv1d_no_bias(in_c, out_c, k, conv_cfg, vb.pp("linear"))?;
        let bn = candle_nn::batch_norm(out_c, 1e-5, vb.pp("nonlinear.batchnorm"))?;
        Ok(Self {
            conv,
            bn,
            activation: Activation::Relu,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x = self.conv.forward(x)?;
        let x = self.bn.forward_t(&x, false)?;
        self.activation.forward(&x)
    }
}

struct CAMLayer {
    linear_local: Conv1d,
    linear1: Conv1d,
    linear2: Conv1d,
}

impl CAMLayer {
    fn new(
        bn_channels: usize,
        out_channels: usize,
        k: usize,
        dilation: usize,
        vb: VarBuilder,
    ) -> Result<Self> {
        let padding = (k - 1) / 2 * dilation;
        let linear_local = conv1d_no_bias(
            bn_channels,
            out_channels,
            k,
            candle_nn::Conv1dConfig {
                padding,
                dilation,
                ..Default::default()
            },
            vb.pp("linear_local"),
        )?;
        let linear1 = candle_nn::conv1d(
            bn_channels,
            bn_channels / 2,
            1,
            Default::default(),
            vb.pp("linear1"),
        )?;
        let linear2 = candle_nn::conv1d(
            bn_channels / 2,
            out_channels,
            1,
            Default::default(),
            vb.pp("linear2"),
        )?;
        Ok(Self {
            linear_local,
            linear1,
            linear2,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let y = self.linear_local.forward(x)?;
        let mean = x.mean_keepdim(2)?; // [B, C, 1]
        let seg = self.seg_pooling(x, 100)?; // [B, C, T]
        let context = mean.broadcast_add(&seg)?; // [B, C, T]
        let context = self.linear1.forward(&context)?.relu()?;
        let gate = candle_nn::ops::sigmoid(&self.linear2.forward(&context)?)?;
        y.broadcast_mul(&gate)
    }

    fn seg_pooling(&self, x: &Tensor, seg_len: usize) -> Result<Tensor> {
        let (b, c, t) = x.dims3()?;
        // Avg pool
        // Candle doesn't have 1d avg pool easily? We can use reshape + mean
        let n_segs = (t + seg_len - 1) / seg_len;
        let padded_len = n_segs * seg_len;
        let x_padded = if padded_len > t {
            x.pad_with_zeros(2, 0, padded_len - t)?
        } else {
            x.clone()
        };
        let seg = x_padded
            .reshape((b, c, n_segs, seg_len))?
            .mean(3)? // (B, C, N_segs)
            .unsqueeze(3)?
            .repeat((1, 1, 1, seg_len))?
            .reshape((b, c, padded_len))?;
        seg.narrow(2, 0, t)
    }
}

struct CAMDenseTDNNLayer {
    bn1: BatchNorm,
    linear1: Conv1d,
    bn2: BatchNorm,
    cam_layer: CAMLayer,
}

impl CAMDenseTDNNLayer {
    fn new(
        in_c: usize,
        out_c: usize,
        bn_c: usize,
        k: usize,
        dilation: usize,
        vb: VarBuilder,
    ) -> Result<Self> {
        let bn1 = candle_nn::batch_norm(in_c, 1e-5, vb.pp("nonlinear1.batchnorm"))?;
        let linear1 = conv1d_no_bias(in_c, bn_c, 1, Default::default(), vb.pp("linear1"))?;
        let bn2 = candle_nn::batch_norm(bn_c, 1e-5, vb.pp("nonlinear2.batchnorm"))?;
        let cam_layer = CAMLayer::new(bn_c, out_c, k, dilation, vb.pp("cam_layer"))?;
        Ok(Self {
            bn1,
            linear1,
            bn2,
            cam_layer,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let h = self.bn1.forward_t(x, false)?.relu()?;
        let h = self.linear1.forward(&h)?;
        let h = self.bn2.forward_t(&h, false)?.relu()?;
        self.cam_layer.forward(&h)
    }
}

struct CAMDenseTDNNBlock {
    layers: Vec<CAMDenseTDNNLayer>,
}

impl CAMDenseTDNNBlock {
    fn new(
        num_layers: usize,
        in_c: usize,
        out_c: usize,
        bn_c: usize,
        k: usize,
        dilation: usize,
        vb: VarBuilder,
    ) -> Result<Self> {
        let mut layers = Vec::new();
        for i in 0..num_layers {
            let layer_in = in_c + i * out_c;
            layers.push(CAMDenseTDNNLayer::new(
                layer_in,
                out_c,
                bn_c,
                k,
                dilation,
                vb.pp(format!("tdnnd{}", i + 1)),
            )?);
        }
        Ok(Self { layers })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let mut current = x.clone();
        for layer in &self.layers {
            let out = layer.forward(&current)?;
            current = Tensor::cat(&[&current, &out], 1)?;
        }
        Ok(current)
    }
}

struct TransitLayer {
    bn: BatchNorm,
    linear: Conv1d,
}

impl TransitLayer {
    fn new(in_c: usize, out_c: usize, vb: VarBuilder) -> Result<Self> {
        let bn = candle_nn::batch_norm(in_c, 1e-5, vb.pp("nonlinear.batchnorm"))?;
        let linear = conv1d_no_bias(in_c, out_c, 1, Default::default(), vb.pp("linear"))?;
        Ok(Self { bn, linear })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x = self.bn.forward_t(x, false)?.relu()?;
        self.linear.forward(&x)
    }
}

struct DenseLayer {
    linear: Conv1d,
}

impl DenseLayer {
    fn new(in_c: usize, out_c: usize, vb: VarBuilder) -> Result<Self> {
        let linear = conv1d_no_bias(in_c, out_c, 1, Default::default(), vb.pp("linear"))?;
        Ok(Self { linear })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x = if x.dims().len() == 2 {
            self.linear.forward(&x.unsqueeze(2)?)?.squeeze(2)?
        } else {
            self.linear.forward(x)?
        };
        // get_nonlinear("batchnorm_") implementation: BatchNorm without affine
        // But the weights in safetensors likely ALREADY HAVE bias/weight if they were default
        // Except "batchnorm_" means affine=False.
        // Actually, just returning x for now as it's just a projection.
        // Wait, Python says `self.nonlinear(x)`. If config is "batchnorm-relu" then it relus.
        // But for dense it's "batchnorm_".
        Ok(x)
    }
}

pub struct CAMPPlus {
    head: FCM,
    tdnn: TDNNLayer,
    blocks: Vec<(CAMDenseTDNNBlock, TransitLayer)>,
    final_bn: BatchNorm,
    final_dense: DenseLayer,
}

impl CAMPPlus {
    pub fn new(feat_dim: usize, embedding_size: usize, vb: VarBuilder) -> Result<Self> {
        let head = FCM::new(32, feat_dim, vb.pp("head"))?;
        let mut channels = head.out_channels;
        let tdnn = TDNNLayer::new(channels, 128, 5, 1, 2, vb.pp("xvector.tdnn"))?;
        channels = 128;
        let mut blocks = Vec::new();
        let configs = vec![(12, 3, 1), (24, 3, 2), (16, 3, 2)];
        for (i, (num_layers, k, dilation)) in configs.into_iter().enumerate() {
            let block = CAMDenseTDNNBlock::new(
                num_layers,
                channels,
                32,
                4 * 32,
                k,
                dilation,
                vb.pp(format!("xvector.block{}", i + 1)),
            )?;
            channels += num_layers * 32;
            let transit = TransitLayer::new(
                channels,
                channels / 2,
                vb.pp(format!("xvector.transit{}", i + 1)),
            )?;
            channels /= 2;
            blocks.push((block, transit));
        }
        let final_bn =
            candle_nn::batch_norm(channels, 1e-5, vb.pp("xvector.out_nonlinear.batchnorm"))?;
        let final_dense = DenseLayer::new(channels * 2, embedding_size, vb.pp("xvector.dense"))?;
        Ok(Self {
            head,
            tdnn,
            blocks,
            final_bn,
            final_dense,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // Input x: (B, C, T) = (B, F, T) - mel spectrogram in channel-first format
        // FCM expects (B, 1, F, T) - unsqueeze adds channel dim for 2D convolutions
        let x = x.unsqueeze(1)?; // (B, 1, F, T)
        let mut x = self.head.forward(&x)?;

        x = self.tdnn.forward(&x)?;
        for (block, transit) in &self.blocks {
            x = block.forward(&x)?;
            x = transit.forward(&x)?;
        }
        x = self.final_bn.forward_t(&x, false)?.relu()?;
        let mean = x.mean_keepdim(2)?.squeeze(2)?;
        let (_b, _c, t) = x.dims3()?;
        let centered = x.broadcast_sub(&mean.unsqueeze(2)?)?;
        let std = (centered.sqr()?.sum_keepdim(2)? / (t as f64 - 1.0))?
            .sqrt()?
            .squeeze(2)?;
        let stats = Tensor::cat(&[&mean, &std], 1)?;
        self.final_dense.forward(&stats)
    }
}
</file>

<file path="candle/src/gpt2.rs">
use candle_core::{DType, IndexOp, Module, Result, Tensor};
use candle_nn::{Activation, Embedding, LayerNorm, Linear, VarBuilder};

#[derive(Debug, Clone, Copy, PartialEq)]
pub struct Config {
    pub vocab_size: usize,
    pub n_positions: usize,
    pub n_embd: usize,
    pub n_layer: usize,
    pub n_head: usize,
    pub activation_function: Activation,
    pub layer_norm_epsilon: f64,
    pub n_inner: Option<usize>,
}

impl Default for Config {
    fn default() -> Self {
        Self {
            vocab_size: 50257,
            n_positions: 1024,
            n_embd: 768,
            n_layer: 12,
            n_head: 12,
            activation_function: Activation::Gelu,
            layer_norm_epsilon: 1e-5,
            n_inner: None,
        }
    }
}

fn conv1d(nf: usize, nx: usize, vb: VarBuilder) -> Result<Linear> {
    // let weight = vb.get((nf, nx), "weight")?; // (out, in)
    // Transpose
    let weight = vb.get((nx, nf), "weight")?;
    let weight = weight.transpose(0, 1)?;
    let bias = vb.get(nf, "bias")?;
    Ok(Linear::new(weight, Some(bias)))
}

struct MLP {
    c_fc: Linear,
    c_proj: Linear,
    act: Activation,
}

impl MLP {
    fn new(cfg: &Config, vb: VarBuilder) -> Result<Self> {
        let n_inner = cfg.n_inner.unwrap_or(4 * cfg.n_embd);
        let c_fc = conv1d(n_inner, cfg.n_embd, vb.pp("c_fc"))?;
        let c_proj = conv1d(cfg.n_embd, n_inner, vb.pp("c_proj"))?;
        Ok(Self {
            c_fc,
            c_proj,
            act: cfg.activation_function,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x = self.c_fc.forward(x)?;
        let x = self.act.forward(&x)?;
        self.c_proj.forward(&x)
    }
}

struct Attention {
    c_attn: Linear,
    c_proj: Linear,
    n_head: usize,
    _n_embd: usize,
    _scale: bool,
}

impl Attention {
    fn new(cfg: &Config, scale: bool, vb: VarBuilder) -> Result<Self> {
        let n_embd = cfg.n_embd;
        let n_head = cfg.n_head;
        let c_attn = conv1d(3 * n_embd, n_embd, vb.pp("c_attn"))?;
        let c_proj = conv1d(n_embd, n_embd, vb.pp("c_proj"))?;
        Ok(Self {
            c_attn,
            c_proj,
            n_head,
            _n_embd: n_embd,
            _scale: scale,
        })
    }

    fn forward(&self, x: &Tensor, bias: Option<&Tensor>) -> Result<Tensor> {
        let (b, t, c) = x.dims3()?;
        let qkv = self.c_attn.forward(x)?;
        let qkv = qkv.reshape((b, t, 3, self.n_head, c / self.n_head))?;
        let qkv = qkv.permute((2, 0, 3, 1, 4))?;
        let q = qkv.i(0)?.contiguous()?;
        let k = qkv.i(1)?.contiguous()?;
        let v = qkv.i(2)?.contiguous()?;

        let k_t = k.transpose(2, 3)?;
        let mut att = (q.matmul(&k_t)? / (c as f64 / self.n_head as f64).sqrt())?;

        if let Some(bias) = bias {
            att = att.broadcast_add(bias)?;
        }

        att = candle_nn::ops::softmax(&att, 3)?;
        let y = att.matmul(&v)?;
        let y = y.permute((0, 2, 1, 3))?.reshape((b, t, c))?;

        self.c_proj.forward(&y)
    }
}

struct Block {
    ln_1: LayerNorm,
    attn: Attention,
    ln_2: LayerNorm,
    mlp: MLP,
}

impl Block {
    fn new(cfg: &Config, vb: VarBuilder) -> Result<Self> {
        let ln_1 = candle_nn::layer_norm(cfg.n_embd, cfg.layer_norm_epsilon, vb.pp("ln_1"))?;
        let attn = Attention::new(cfg, true, vb.pp("attn"))?;
        let ln_2 = candle_nn::layer_norm(cfg.n_embd, cfg.layer_norm_epsilon, vb.pp("ln_2"))?;
        let mlp = MLP::new(cfg, vb.pp("mlp"))?;
        Ok(Self {
            ln_1,
            attn,
            ln_2,
            mlp,
        })
    }

    fn forward(&self, x: &Tensor, bias: Option<&Tensor>) -> Result<Tensor> {
        let residual = x;
        let x = self.ln_1.forward(x)?;
        let x = self.attn.forward(&x, bias)?;
        let x = (x + residual)?;

        let residual = &x;
        let x = self.ln_2.forward(&x)?;
        let x = self.mlp.forward(&x)?;
        let x = (x + residual)?;
        Ok(x)
    }
}

pub struct GPT2Model {
    wte: Embedding,
    wpe: Embedding,
    h: Vec<Block>,
    ln_f: LayerNorm,
    _config: Config,
}

impl GPT2Model {
    pub fn new(config: Config, vb: VarBuilder) -> Result<Self> {
        let wte = candle_nn::embedding(config.vocab_size, config.n_embd, vb.pp("wte"))?;
        let wpe = candle_nn::embedding(config.n_positions, config.n_embd, vb.pp("wpe"))?;

        let mut h = Vec::new();
        let blocks_vb = vb.pp("h");
        for i in 0..config.n_layer {
            h.push(Block::new(&config, blocks_vb.pp(i))?);
        }

        let ln_f = candle_nn::layer_norm(config.n_embd, config.layer_norm_epsilon, vb.pp("ln_f"))?;

        Ok(Self {
            wte,
            wpe,
            h,
            ln_f,
            _config: config,
        })
    }

    pub fn forward(&self, input_ids: &Tensor) -> Result<Tensor> {
        let (_b, t) = input_ids.dims2()?;
        let input_embeds = self.wte.forward(input_ids)?;
        let position_ids = Tensor::arange(0, t as u32, input_ids.device())?.unsqueeze(0)?;
        let position_embeds = self.wpe.forward(&position_ids)?;

        let mut hidden_states = (input_embeds + position_embeds)?;

        let mask_indexes = Tensor::arange(0, t as u32, input_ids.device())?;
        let mask_indexes_row = mask_indexes.unsqueeze(1)?;
        let mask_indexes_col = mask_indexes.unsqueeze(0)?;
        let mask = mask_indexes_row.ge(&mask_indexes_col)?;

        let mask = mask.unsqueeze(0)?.unsqueeze(0)?;
        let mask = mask.to_dtype(hidden_states.dtype())?;
        let wide_mask_val = if hidden_states.dtype() == DType::F16 {
            1e4
        } else {
            1e9
        };
        let mask = ((mask - 1.0)? * wide_mask_val)?;

        for block in &self.h {
            hidden_states = block.forward(&hidden_states, Some(&mask))?;
        }

        self.ln_f.forward(&hidden_states)
    }

    // Helper to allow custom embeddings input
    pub fn forward_embeds(&self, inputs_embeds: &Tensor) -> Result<Tensor> {
        let (_b, t, _c) = inputs_embeds.dims3()?;
        let position_ids = Tensor::arange(0, t as u32, inputs_embeds.device())?.unsqueeze(0)?;
        let position_embeds = self.wpe.forward(&position_ids)?;

        let mut hidden_states = (inputs_embeds + position_embeds)?;

        let mask_indexes = Tensor::arange(0, t as u32, inputs_embeds.device())?;
        let mask_indexes_row = mask_indexes.unsqueeze(1)?.broadcast_as((t, t))?;
        let mask_indexes_col = mask_indexes.unsqueeze(0)?.broadcast_as((t, t))?;
        let mask = mask_indexes_row.ge(&mask_indexes_col)?;

        let mask = mask.unsqueeze(0)?.unsqueeze(0)?;
        let mask = mask.to_dtype(inputs_embeds.dtype())?;
        let wide_mask_val = if inputs_embeds.dtype() == DType::F16 {
            1e4
        } else {
            1e9
        };
        let mask = ((mask - 1.0)? * wide_mask_val)?;

        for block in &self.h {
            hidden_states = block.forward(&hidden_states, Some(&mask))?;
        }

        self.ln_f.forward(&hidden_states)
    }

    // Forward without adding positional embeddings (caller handles it)
    pub fn forward_embeds_no_pos(&self, inputs_embeds: &Tensor) -> Result<Tensor> {
        let (_b, t, _c) = inputs_embeds.dims3()?;
        let mut hidden_states = inputs_embeds.clone();

        let mask_indexes = Tensor::arange(0, t as u32, inputs_embeds.device())?;
        let mask_indexes_row = mask_indexes.unsqueeze(1)?.broadcast_as((t, t))?;
        let mask_indexes_col = mask_indexes.unsqueeze(0)?.broadcast_as((t, t))?;
        let mask = mask_indexes_row.ge(&mask_indexes_col)?;

        let mask = mask.unsqueeze(0)?.unsqueeze(0)?;
        let mask = mask.to_dtype(inputs_embeds.dtype())?;
        let wide_mask_val = if inputs_embeds.dtype() == DType::F16 {
            1e4
        } else {
            1e9
        };
        let mask = ((mask - 1.0)? * wide_mask_val)?;

        for block in &self.h {
            hidden_states = block.forward(&hidden_states, Some(&mask))?;
        }

        self.ln_f.forward(&hidden_states)
    }
}
</file>

<file path="candle/src/voice_encoder.rs">
use candle_core::{IndexOp, Result, Tensor};
use candle_nn::rnn::LSTMState;
use candle_nn::{LSTMConfig, Module, VarBuilder, LSTM, RNN};

pub struct VoiceEncoderConfig {
    pub ve_hidden_size: usize,
    pub speaker_embed_size: usize,
    pub num_mels: usize,
    pub num_layers: usize,
    pub ve_final_relu: bool,
}

impl Default for VoiceEncoderConfig {
    fn default() -> Self {
        // FIXED: num_mels must be 40 to match ve.safetensors
        Self {
            ve_hidden_size: 256,
            speaker_embed_size: 256,
            num_mels: 40,
            num_layers: 3,
            ve_final_relu: true,
        }
    }
}

pub struct VoiceEncoder {
    lstm: Vec<LSTM>,
    proj: candle_nn::Linear,
    config: VoiceEncoderConfig,
}

impl VoiceEncoder {
    pub fn new(config: VoiceEncoderConfig, vb: VarBuilder) -> Result<Self> {
        let mut lstms = Vec::new();
        let lstm_vb = vb.pp("lstm");

        for i in 0..config.num_layers {
            let lstm_config = LSTMConfig {
                layer_idx: i,
                direction: candle_nn::rnn::Direction::Forward,
                ..Default::default()
            };

            let input_size = if i == 0 {
                config.num_mels
            } else {
                config.ve_hidden_size
            };
            let lstm = LSTM::new(
                input_size,
                config.ve_hidden_size,
                lstm_config,
                lstm_vb.clone(),
            )?;
            lstms.push(lstm);
        }

        let proj = candle_nn::linear(
            config.ve_hidden_size,
            config.speaker_embed_size,
            vb.pp("proj"),
        )?;

        Ok(Self {
            lstm: lstms,
            proj,
            config,
        })
    }

    pub fn forward(&self, mels: &Tensor) -> Result<Tensor> {
        // mels: (B, T, 40)
        let (b, t, _m) = mels.dims3()?;
        eprintln!("[VoiceEncoder] input mels: b={}, t={}, m={}", b, t, _m);
        let mut hidden_states = mels.clone();

        for (layer_idx, layer) in self.lstm.iter().enumerate() {
            eprintln!(
                "[VoiceEncoder] LSTM layer {}: creating zeros (b={}, h={})",
                layer_idx, b, self.config.ve_hidden_size
            );
            let h = Tensor::zeros((b, self.config.ve_hidden_size), mels.dtype(), mels.device())?;
            let c = Tensor::zeros((b, self.config.ve_hidden_size), mels.dtype(), mels.device())?;
            let mut state = LSTMState { h, c };

            let mut outputs = Vec::new();
            for i in 0..t {
                let input_step = hidden_states.i((.., i, ..))?.contiguous()?;
                state = layer.step(&input_step, &state)?;
                outputs.push(state.h.clone());
            }
            eprintln!(
                "[VoiceEncoder] LSTM layer {}: stacking {} outputs",
                layer_idx,
                outputs.len()
            );
            hidden_states = Tensor::stack(&outputs, 1)?;
            eprintln!(
                "[VoiceEncoder] LSTM layer {}: hidden_states: {:?}",
                layer_idx,
                hidden_states.dims()
            );
        }

        let last_hidden = hidden_states.i((.., t - 1, ..))?;
        let mut raw_embeds = self.proj.forward(&last_hidden)?;

        if self.config.ve_final_relu {
            raw_embeds = raw_embeds.relu()?;
        }

        let norm = raw_embeds.sqr()?.sum_keepdim(1)?.sqrt()?;
        raw_embeds.broadcast_div(&norm)
    }
}
</file>

<file path="candle/src/audio.rs">
use candle_core::{DType, Device, Result, Tensor};
use hound::{SampleFormat, WavReader, WavSpec, WavWriter};
use rubato::{FftFixedIn, Resampler};
use std::path::Path;

pub const S3GEN_SR: u32 = 24000;
pub const S3_SR: u32 = 16000;

pub fn load_wav<P: AsRef<Path>>(path: P) -> std::result::Result<(Vec<f32>, u32), String> {
    let reader = WavReader::open(path).map_err(|e| format!("Failed to open WAV: {}", e))?;
    let spec = reader.spec();
    let sample_rate = spec.sample_rate;
    let channels = spec.channels as usize;

    let samples: Vec<f32> = match spec.sample_format {
        SampleFormat::Int => {
            let bits = spec.bits_per_sample;
            let max_val = (1i32 << (bits - 1)) as f32;
            reader
                .into_samples::<i32>()
                .filter_map(|s| s.ok())
                .map(|s| s as f32 / max_val)
                .collect()
        }
        SampleFormat::Float => reader
            .into_samples::<f32>()
            .filter_map(|s| s.ok())
            .collect(),
    };

    let mono = if channels > 1 {
        samples
            .chunks(channels)
            .map(|chunk| chunk.iter().sum::<f32>() / channels as f32)
            .collect()
    } else {
        samples
    };

    Ok((mono, sample_rate))
}

pub fn save_wav<P: AsRef<Path>>(
    path: P,
    samples: &[f32],
    sample_rate: u32,
) -> std::result::Result<(), String> {
    let spec = WavSpec {
        channels: 1,
        sample_rate,
        bits_per_sample: 16,
        sample_format: SampleFormat::Int,
    };
    let mut writer =
        WavWriter::create(path, spec).map_err(|e| format!("Failed to create WAV: {}", e))?;
    for &sample in samples {
        let clamped = sample.clamp(-1.0, 1.0);
        writer
            .write_sample((clamped * 32767.0) as i16)
            .map_err(|e| format!("Failed to write sample: {}", e))?;
    }
    writer
        .finalize()
        .map_err(|e| format!("Failed to finalize WAV: {}", e))?;
    Ok(())
}

pub fn resample(
    samples: &[f32],
    from_sr: u32,
    to_sr: u32,
) -> std::result::Result<Vec<f32>, String> {
    if from_sr == to_sr {
        return Ok(samples.to_vec());
    }
    let chunk_size = 1024;
    let mut resampler = FftFixedIn::<f32>::new(from_sr as usize, to_sr as usize, chunk_size, 2, 1)
        .map_err(|e| format!("Failed to create resampler: {}", e))?;
    let mut output = Vec::new();
    let mut input_frames = samples.to_vec();
    let remainder = input_frames.len() % chunk_size;
    if remainder != 0 {
        input_frames.extend(std::iter::repeat(0.0).take(chunk_size - remainder));
    }
    for chunk in input_frames.chunks(chunk_size) {
        let input = vec![chunk.to_vec()];
        let mut resampled = resampler
            .process(&input, None)
            .map_err(|e| format!("Resample error: {}", e))?;
        output.append(&mut resampled[0]);
    }
    Ok(output)
}

pub struct MelConfig {
    pub n_fft: usize,
    pub hop_length: usize,
    pub win_length: usize,
    pub n_mels: usize,
    pub fmax: f32,
}

impl MelConfig {
    pub fn for_16k() -> Self {
        Self {
            n_fft: 1024,
            hop_length: 160,
            win_length: 1024,
            n_mels: 80,
            fmax: 8000.0,
        }
    }

    pub fn for_24k(n_mels: usize) -> Self {
        Self {
            n_fft: 1920,
            hop_length: 480,
            win_length: 1920,
            n_mels,
            fmax: 8000.0,
        }
    }
}

pub fn compute_mel_spectrogram(
    samples: &[f32],
    sample_rate: u32,
    device: &Device,
    config: &MelConfig,
) -> Result<Tensor> {
    use realfft::RealFftPlanner;
    let n_fft = config.n_fft;
    let hop_length = config.hop_length;
    let win_length = config.win_length;
    let n_mels = config.n_mels;
    let fmax = config.fmax;

    let window: Vec<f32> = (0..win_length)
        .map(|i| 0.5 * (1.0 - (2.0 * std::f32::consts::PI * i as f32 / win_length as f32).cos()))
        .collect();

    // Reflect padding as per Python: (n_fft - hop_size) / 2 = 720
    let pad = (n_fft - hop_length) / 2;
    let mut padded = Vec::with_capacity(samples.len() + 2 * pad);
    // Left reflect: samples[pad], samples[pad-1], ..., samples[1]
    for i in (1..=pad).rev() {
        padded.push(samples[i]);
    }
    padded.extend_from_slice(samples);
    // Right reflect: samples[n-2], samples[n-3], ..., samples[n-pad-1]
    let n = samples.len();
    for i in 1..=pad {
        padded.push(samples[n - 1 - i]);
    }

    let n_frames = (padded.len() - n_fft) / hop_length + 1;
    let mut planner = RealFftPlanner::<f32>::new();
    let fft = planner.plan_fft_forward(n_fft);
    let mut power_spectrogram = vec![vec![0.0f32; n_fft / 2 + 1]; n_frames];

    for (frame_idx, frame_start) in (0..padded.len() - n_fft + 1)
        .step_by(hop_length)
        .enumerate()
    {
        if frame_idx >= n_frames {
            break;
        }
        let mut windowed: Vec<f32> = padded[frame_start..frame_start + n_fft]
            .iter()
            .zip(window.iter())
            .map(|(s, w)| s * w)
            .collect();
        let mut spectrum = fft.make_output_vec();
        fft.process(&mut windowed, &mut spectrum)
            .map_err(|e| candle_core::Error::Msg(format!("FFT error: {}", e)))?;
        for (i, c) in spectrum.iter().enumerate() {
            // Magnitude = sqrt(re^2 + im^2), then power = magnitude^2 = re^2 + im^2
            // mel.py: spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))
            // Then it applies mel and spectral_normalize (log)
            power_spectrogram[frame_idx][i] = (c.norm_sqr() + 1e-9).sqrt();
        }
    }

    let mel_filters = create_mel_filterbank(sample_rate, n_fft, n_mels, fmax);
    let mut mel_spec = vec![vec![0.0f32; n_mels]; n_frames];
    for (t, mag_frame) in power_spectrogram.iter().enumerate() {
        for (m, filter) in mel_filters.iter().enumerate() {
            let mut sum = 0.0f32;
            for (i, &weight) in filter.iter().enumerate() {
                sum += weight * mag_frame[i];
            }
            // spectral_normalize_torch uses natural log: torch.log(clamp(x, min=1e-5))
            mel_spec[t][m] = sum.max(1e-5);
        }
    }

    let flat: Vec<f32> = mel_spec.into_iter().flatten().collect();
    // Return (B, C, T) where C=80
    Tensor::from_vec(flat, (1, n_frames, n_mels), device)?
        .permute((0, 2, 1))?
        .contiguous()?
        .to_dtype(DType::F32)
}

fn create_mel_filterbank(
    sample_rate: u32,
    n_fft: usize,
    n_mels: usize,
    fmax: f32,
) -> Vec<Vec<f32>> {
    let hz_to_mel = |hz: f32| 2595.0 * (1.0 + hz / 700.0).log10();
    let mel_to_hz = |mel: f32| 700.0 * (10.0f32.powf(mel / 2595.0) - 1.0);
    let mel_min = hz_to_mel(0.0);
    let mel_max = hz_to_mel(fmax);
    let mel_points: Vec<f32> = (0..=n_mels + 1)
        .map(|i| mel_min + (mel_max - mel_min) * i as f32 / (n_mels + 1) as f32)
        .collect();
    let hz_points: Vec<f32> = mel_points.iter().map(|&m| mel_to_hz(m)).collect();
    let n_bins = n_fft / 2 + 1;
    let bin_points: Vec<usize> = hz_points
        .iter()
        .map(|&hz| ((hz * (n_fft as f32) / sample_rate as f32).round() as usize).min(n_bins - 1))
        .collect();
    let mut filterbank = vec![vec![0.0f32; n_bins]; n_mels];

    for m in 0..n_mels {
        let left_hz = hz_points[m];
        let _center_hz = hz_points[m + 1];
        let right_hz = hz_points[m + 2];

        let left = bin_points[m];
        let center = bin_points[m + 1];
        let right = bin_points[m + 2];

        // Slaney normalization factor
        let enorm = 2.0 / (right_hz - left_hz);

        for k in left..center {
            if center > left {
                filterbank[m][k] = ((k - left) as f32 / (center - left) as f32) * enorm;
            }
        }
        for k in center..right {
            if right > center {
                filterbank[m][k] = ((right - k) as f32 / (right - center) as f32) * enorm;
            }
        }
    }
    filterbank
}

pub fn normalize_loudness(samples: &mut [f32], target_db: f32) {
    let rms: f32 = (samples.iter().map(|s| s * s).sum::<f32>() / samples.len() as f32).sqrt();
    if rms > 1e-10 {
        let gain = 10.0f32
            .powf((target_db - 20.0 * rms.log10()) / 20.0)
            .min(10.0);
        for sample in samples.iter_mut() {
            *sample *= gain;
        }
    }
}
</file>

<file path="src/chatterbox/models/t3/t3.py">
# Copyright (c) 2025 Resemble AI
# MIT License
import logging
from typing import Union, Optional, List

logger = logging.getLogger(__name__)

from tqdm import tqdm
import torch
import torch.nn.functional as F
from torch import nn, Tensor
from transformers import LlamaModel, LlamaConfig, GPT2Config, GPT2Model
from transformers.generation.logits_process import (
    LogitsProcessorList,
    RepetitionPenaltyLogitsProcessor,
    TemperatureLogitsWarper,
    TopKLogitsWarper,
    TopPLogitsWarper,
    MinPLogitsWarper,
)
from .modules.learned_pos_emb import LearnedPositionEmbeddings

from .modules.cond_enc import T3CondEnc, T3Cond
from .modules.t3_config import T3Config
from .llama_configs import LLAMA_CONFIGS
from .inference.t3_hf_backend import T3HuggingfaceBackend
from .inference.alignment_stream_analyzer import AlignmentStreamAnalyzer
from ..utils import AttrDict


logger = logging.getLogger(__name__)


def _ensure_BOT_EOT(text_tokens: Tensor, hp):
    B = text_tokens.size(0)
    assert (text_tokens == hp.start_text_token).int().sum() >= B, "missing start_text_token"
    assert (text_tokens == hp.stop_text_token).int().sum() >= B, "missing stop_text_token"


class T3(nn.Module):
    """
    Token-To-Token (T3) TTS model using huggingface transformer models as backbones,
        * tokenization, including start / stop tokens are always added externally to this class
        * conditioning data like CLAP, emotion, etc are all in a separate file for more modularity
        * careful! this class assumes relative positional encoding -- with absolute PE, we would at
            least want to reset the position to 0 when speech tokens begin, and optionally use a
            different PE embedding space for speech.
    """

    def __init__(self, hp=None):
        if hp is None:
            hp = T3Config.english_only()
        super().__init__()
        self.hp = hp

        config_dict = LLAMA_CONFIGS[hp.llama_config_name]
        self.is_gpt = config_dict.get("model_type") == "gpt2"

        if self.is_gpt:
            self.cfg = GPT2Config(**config_dict)
            self.tfmr = GPT2Model(self.cfg)
        else:
            self.cfg = LlamaConfig(**config_dict)
            self.tfmr = LlamaModel(self.cfg)

        self.dim = self.cfg.hidden_size
        self.deepspeed_patch_applied = False

        # conditioning / embedding
        self.cond_enc = T3CondEnc(hp)
        self.text_emb = nn.Embedding(hp.text_tokens_dict_size, self.dim)
        self.speech_emb = nn.Embedding(hp.speech_tokens_dict_size, self.dim)

        # custom position embedding
        self.text_pos_emb = None
        self.speech_pos_emb = None
        if hp.input_pos_emb == "learned":
            max_text_seq_len = hp.max_text_tokens + 2
            self.text_pos_emb = LearnedPositionEmbeddings(max_text_seq_len, self.dim)

            max_mel_seq_len = hp.max_speech_tokens + 2 + 2
            self.speech_pos_emb = LearnedPositionEmbeddings(max_mel_seq_len, self.dim)

        # logit projection
        self.text_head = nn.Linear(self.cfg.hidden_size, hp.text_tokens_dict_size, bias=False)
        self.speech_head = nn.Linear(self.cfg.hidden_size, hp.speech_tokens_dict_size, bias=self.is_gpt)
        self.compiled = False

    @property
    def device(self):
        return self.speech_head.weight.device

    def prepare_conditioning(self, t3_cond: T3Cond):
        """
        Token cond data needs to be embedded, so that needs to be here instead of in `T3CondEnc`.
        """
        if t3_cond.cond_prompt_speech_tokens is not None and t3_cond.cond_prompt_speech_emb is None:
            t3_cond.cond_prompt_speech_emb = self.speech_emb(t3_cond.cond_prompt_speech_tokens)
            if not self.is_gpt:
                t3_cond.cond_prompt_speech_emb += self.speech_pos_emb(t3_cond.cond_prompt_speech_tokens)
        return self.cond_enc(t3_cond)  # (B, len_cond, dim)

    def prepare_input_embeds(
        self,
        *,
        t3_cond: T3Cond,
        text_tokens: torch.LongTensor,
        speech_tokens: torch.LongTensor,
        cfg_weight: float = 0.0,
    ):
        # prepare input embeddings (skip backbone tranformer embeddings)
        cond_emb = self.prepare_conditioning(t3_cond)  # (B, len_cond, dim)
        text_emb = self.text_emb(text_tokens)  # (B, len_text, dim)
        if cfg_weight > 0.0 and not self.is_gpt:
            text_emb[1].zero_()  # CFG uncond

        speech_emb = self.speech_emb(speech_tokens)  # (B, len_speech, dim)
        if self.hp.input_pos_emb == "learned":
            text_emb = text_emb + self.text_pos_emb(text_tokens)
            speech_emb = speech_emb + self.speech_pos_emb(speech_tokens)
        len_cond = cond_emb.size(1)

        if cond_emb.size(0) != text_emb.size(0):
             cond_emb = cond_emb.expand(text_emb.size(0), -1, -1)

        # concat
        embeds = torch.stack([
            torch.cat((ce, te, se))
            for ce, te, se in zip(cond_emb, text_emb, speech_emb)
        ])  # (B, length, dim)
        return embeds, len_cond

    def forward(
        self,
        *,
        t3_cond: T3Cond,
        text_tokens: torch.LongTensor,
        text_token_lens: torch.LongTensor,
        speech_tokens: torch.LongTensor,
        speech_token_lens: torch.LongTensor,
        training=False,
    ):
        _ensure_BOT_EOT(text_tokens, self.hp)

        # prepare custom input embeds
        embeds, len_cond = self.prepare_input_embeds(
            t3_cond=t3_cond,
            text_tokens=text_tokens,
            speech_tokens=speech_tokens,
        )

        # backbone tranformer forward
        tfmr_out = self.tfmr.forward(
            input_ids=None,
            # position_ids=position_ids, # TODO? ROPE should be fine?
            inputs_embeds=embeds,
            output_hidden_states=True,
            return_dict=True,
            use_cache=(not training),
        )
        hidden_states = tfmr_out.hidden_states[-1]  # final tfmr layer output, (B, seq, dim)

        # post-processing: splice out text and speech parts of hidden states
        len_text = text_tokens.size(1)
        len_speech = speech_tokens.size(1)
        B, _, dim = hidden_states.shape
        device, dtype = hidden_states.device, hidden_states.dtype
        text_latents = torch.zeros(B, len_text, dim, dtype=dtype, device=device)
        speech_latents = torch.zeros(B, len_speech, dim, dtype=dtype, device=device)
        ttl, stl = text_token_lens, speech_token_lens
        for i in range(B):
            text_end = len_cond + ttl[i].item()
            speech_start = len_cond + text_tokens.size(1)
            speech_end = speech_start + stl[i].item()
            text_latents[i, :ttl[i]] = hidden_states[i, len_cond:text_end]
            speech_latents[i, :stl[i]] = hidden_states[i, speech_start:speech_end]

        # logit projection
        text_logits = self.text_head(text_latents)
        speech_logits = self.speech_head(speech_latents)

        return AttrDict(
            text_logits=text_logits,
            text_latents=text_latents,
            speech_logits=speech_logits,
            speech_latents=speech_latents,
            hidden_states=hidden_states,
        )

    def loss(
        self,
        *,
        t3_cond: T3Cond,
        text_tokens: torch.LongTensor,
        text_token_lens: torch.LongTensor,
        speech_tokens: torch.LongTensor,
        speech_token_lens: torch.LongTensor,
    ):
        "training method"
        len_text = text_tokens.size(1)
        len_speech = speech_tokens.size(1)
        assert len_text == text_token_lens.max()
        assert len_speech == speech_token_lens.max()

        out = self.forward(
            t3_cond=t3_cond,
            text_tokens=text_tokens,
            text_token_lens=text_token_lens,
            speech_tokens=speech_tokens,
            speech_token_lens=speech_token_lens,
            training=True,
        )  # (B, seq, vocab_size)

        # Calc CCE losses
        IGNORE_ID = -100
        device = out.text_logits.device
        mask_text = torch.arange(len_text, device=device)[None] >= text_token_lens[:, None]  # (B, len_text)
        mask_speech = torch.arange(len_speech, device=device)[None] >= speech_token_lens[:, None]  # (B, len_speech)
        masked_text = text_tokens.masked_fill(mask_text, IGNORE_ID)
        masked_speech = speech_tokens.masked_fill(mask_speech, IGNORE_ID)
        loss_text = F.cross_entropy(out.text_logits, masked_text, ignore_index=IGNORE_ID)
        loss_speech = F.cross_entropy(out.speech_logits, masked_speech, ignore_index=IGNORE_ID)

        return loss_text, loss_speech

    @torch.inference_mode()
    def inference(
        self,
        *,
        t3_cond: T3Cond,
        text_tokens: Tensor,
        initial_speech_tokens: Optional[Tensor]=None,

        # misc conditioning
        prepend_prompt_speech_tokens: Optional[Tensor]=None,

        # HF generate args
        num_return_sequences=1,
        max_new_tokens=None,
        stop_on_eos=True,
        do_sample=True,
        temperature=0.8,
        top_p=0.95,
        min_p=0.05,
        length_penalty=1.0,
        repetition_penalty=1.2,
        cfg_weight=0.5,
    ):
        """
        Args:
            text_tokens: a 1D (unbatched) or 2D (batched) tensor.
        """
        # Validate / sanitize inputs
        assert prepend_prompt_speech_tokens is None, "not implemented"
        _ensure_BOT_EOT(text_tokens, self.hp)
        text_tokens = torch.atleast_2d(text_tokens).to(dtype=torch.long, device=self.device)

        # Default initial speech to a single start-of-speech token
        if initial_speech_tokens is None:
            initial_speech_tokens = self.hp.start_speech_token * torch.ones_like(text_tokens[:, :1])

        # Prepare custom input embeds
        embeds, len_cond = self.prepare_input_embeds(
            t3_cond=t3_cond,
            text_tokens=text_tokens,
            speech_tokens=initial_speech_tokens,
            cfg_weight=cfg_weight,
        )

        # In order to use the standard HF generate method, we need to extend some methods to inject our custom logic
        # Note the llama-specific logic. Other tfmr types can be added later.

        self.compiled = False

        # TODO? synchronize the expensive compile function
        # with self.compile_lock:
        if not self.compiled:
            # Default to None for English models, only create for multilingual
            alignment_stream_analyzer = None
            if self.hp.is_multilingual:
                alignment_stream_analyzer = AlignmentStreamAnalyzer(
                    self.tfmr,
                    None,
                    text_tokens_slice=(len_cond, len_cond + text_tokens.size(-1)),
                    alignment_layer_idx=9, # TODO: hparam or something?
                    eos_idx=self.hp.stop_speech_token,
                )
                assert alignment_stream_analyzer.eos_idx == self.hp.stop_speech_token

            patched_model = T3HuggingfaceBackend(
                config=self.cfg,
                llama=self.tfmr,
                speech_enc=self.speech_emb,
                speech_head=self.speech_head,
                alignment_stream_analyzer=alignment_stream_analyzer,
            )
            self.patched_model = patched_model
            self.compiled = True

        # # Run normal generate method, which calls our custom extended methods
        # return self.patched_model.generate(
        #     inputs=initial_speech_tokens,
        #     decoder_cond=embeds,
        #     bos_token_id=self.hp.start_speech_token,
        #     eos_token_id=(self.hp.stop_speech_token if stop_on_eos else -1),
        #     pad_token_id=self.hp.stop_speech_token,
        #     max_new_tokens=max_new_tokens or self.hp.max_speech_tokens,
        #     num_return_sequences=num_return_sequences,
        #     temperature=temperature,
        #     min_p=min_p,
        #     length_penalty=length_penalty,
        #     repetition_penalty=repetition_penalty,
        #     do_sample=do_sample,
        #     # cache_implementation=None if not self.compiled else "static",
        # )

        device = embeds.device

        bos_token = torch.tensor([[self.hp.start_speech_token]], dtype=torch.long, device=device)
        bos_embed = self.speech_emb(bos_token)  # shape: (B, 1, embed_dim)
        bos_embed = bos_embed + self.speech_pos_emb.get_fixed_embedding(0)

        # batch_size=2 for CFG
        bos_embed = torch.cat([bos_embed, bos_embed])

        # Combine condition and BOS token for the initial input
        inputs_embeds = torch.cat([embeds, bos_embed], dim=1)

        # Track generated token ids; start with the BOS token.
        generated_ids = bos_token.clone()
        predicted = []  # To store the predicted tokens

        # Instantiate the logits processors.
        top_p_warper = TopPLogitsWarper(top_p=top_p)
        min_p_warper = MinPLogitsWarper(min_p=min_p)
        top_p_warper = TopPLogitsWarper(top_p=top_p)
        repetition_penalty_processor = RepetitionPenaltyLogitsProcessor(penalty=float(repetition_penalty))

        # ---- Initial Forward Pass (no kv_cache yet) ----
        output = self.patched_model(
            inputs_embeds=inputs_embeds,
            past_key_values=None,
            use_cache=True,
            output_attentions=True,
            output_hidden_states=True,
            return_dict=True,
        )
        # Initialize kv_cache with the full context.
        past = output.past_key_values

        # ---- Generation Loop using kv_cache ----
        for i in tqdm(range(max_new_tokens), desc="Sampling", dynamic_ncols=True):
            logits_step = output.logits[:, -1, :]
            # CFG combine  → (1, V)
            cond   = logits_step[0:1, :]
            uncond = logits_step[1:2, :]
            cfg = torch.as_tensor(cfg_weight, device=cond.device, dtype=cond.dtype)
            logits = cond + cfg * (cond - uncond)
            
            # Apply alignment stream analyzer integrity checks
            if self.patched_model.alignment_stream_analyzer is not None:
                if logits.dim() == 1:            # guard in case something upstream squeezed
                    logits = logits.unsqueeze(0) # (1, V)
                # Pass the last generated token for repetition tracking
                last_token = generated_ids[0, -1].item() if len(generated_ids[0]) > 0 else None
                logits = self.patched_model.alignment_stream_analyzer.step(logits, next_token=last_token)  # (1, V)

            # Apply repetition penalty
            ids_for_proc = generated_ids[:1, ...]   # batch = 1
            logits = repetition_penalty_processor(ids_for_proc, logits)  # expects (B,V)
            
            # Apply temperature scaling.
            if temperature != 1.0:
                logits = logits / temperature
                
            # Apply min_p and top_p filtering
            logits = min_p_warper(ids_for_proc, logits)
            logits = top_p_warper(ids_for_proc, logits)

            # Convert logits to probabilities and sample the next token.
            probs = torch.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)  # shape: (B, 1)

            predicted.append(next_token)
            generated_ids = torch.cat([generated_ids, next_token], dim=1)

            # Check for EOS token.
            if next_token.view(-1) == self.hp.stop_speech_token:
                logger.info(f"✅ EOS token detected! Stopping generation at step {i+1}")
                break

            # Get embedding for the new token.
            next_token_embed = self.speech_emb(next_token)
            next_token_embed = next_token_embed + self.speech_pos_emb.get_fixed_embedding(i + 1)

            #  For CFG
            next_token_embed = torch.cat([next_token_embed, next_token_embed])

            # Forward pass with only the new token and the cached past.
            output = self.patched_model(
                inputs_embeds=next_token_embed,
                past_key_values=past,
                output_attentions=True,
                output_hidden_states=True,
                return_dict=True,
            )
            # Update the kv_cache.
            past = output.past_key_values

        # Concatenate all predicted tokens along the sequence dimension.
        predicted_tokens = torch.cat(predicted, dim=1)  # shape: (B, num_tokens)
        return predicted_tokens

    @torch.inference_mode()
    def inference_turbo(self, t3_cond, text_tokens, temperature=0.8, top_k=1000, top_p=0.95, repetition_penalty=1.2,
                        max_gen_len=1000):

        logits_processors = LogitsProcessorList()
        if temperature > 0 and temperature != 1.0:
            logits_processors.append(TemperatureLogitsWarper(temperature))
        if top_k > 0:
            logits_processors.append(TopKLogitsWarper(top_k))
        if top_p < 1.0:
            logits_processors.append(TopPLogitsWarper(top_p))
        if repetition_penalty != 1.0:
            logits_processors.append(RepetitionPenaltyLogitsProcessor(repetition_penalty))


        speech_start_token = self.hp.start_speech_token * torch.ones_like(text_tokens[:, :1])
        embeds, _ = self.prepare_input_embeds(
            t3_cond=t3_cond,
            text_tokens=text_tokens,
            speech_tokens=speech_start_token,
            cfg_weight=0.0,
        )

        generated_speech_tokens = []

        llm_outputs = self.tfmr(
            inputs_embeds=embeds,
            use_cache=True
        )

        hidden_states = llm_outputs[0]
        past_key_values = llm_outputs.past_key_values

        speech_hidden = hidden_states[:, -1:]
        speech_logits = self.speech_head(speech_hidden)

        processed_logits = logits_processors(speech_start_token, speech_logits[:, -1, :])
        probs = F.softmax(processed_logits, dim=-1)
        next_speech_token = torch.multinomial(probs, num_samples=1)

        generated_speech_tokens.append(next_speech_token)
        current_speech_token = next_speech_token

        for _ in tqdm(range(max_gen_len)):
            current_speech_embed = self.speech_emb(current_speech_token)

            llm_outputs = self.tfmr(
                inputs_embeds=current_speech_embed,
                past_key_values=past_key_values,
                use_cache=True
            )

            hidden_states = llm_outputs[0]
            past_key_values = llm_outputs.past_key_values
            speech_logits = self.speech_head(hidden_states)

            input_ids = torch.cat(generated_speech_tokens, dim=1)
            processed_logits = logits_processors(input_ids, speech_logits[:, -1, :])
            if torch.all(processed_logits == -float("inf")):
                print("Warning: All logits are -inf")
                break

            probs = F.softmax(processed_logits, dim=-1)
            next_speech_token = torch.multinomial(probs, num_samples=1)

            generated_speech_tokens.append(next_speech_token)
            current_speech_token = next_speech_token
            if torch.all(next_speech_token == self.hp.stop_speech_token):
                break

        all_tokens = torch.cat(generated_speech_tokens, dim=1)

        # Remove EOS token if present
        if all_tokens.size(1) > 0 and all_tokens[0, -1] == self.hp.stop_speech_token:
            all_tokens = all_tokens[:, :-1]

        return all_tokens
</file>

<file path="src/chatterbox/tts.py">
from dataclasses import dataclass
from pathlib import Path

import librosa
import torch
import perth
import torch.nn.functional as F
from huggingface_hub import hf_hub_download
from safetensors.torch import load_file

from .models.t3 import T3
from .models.s3tokenizer import S3_SR, drop_invalid_tokens
from .models.s3gen import S3GEN_SR, S3Gen
from .models.tokenizers import EnTokenizer
from .models.voice_encoder import VoiceEncoder
from .models.t3.modules.cond_enc import T3Cond


REPO_ID = "ResembleAI/chatterbox"


def punc_norm(text: str) -> str:
    """
        Quick cleanup func for punctuation from LLMs or
        containing chars not seen often in the dataset
    """
    if len(text) == 0:
        return "You need to add some text for me to talk."

    # Capitalise first letter
    if text[0].islower():
        text = text[0].upper() + text[1:]

    # Remove multiple space chars
    text = " ".join(text.split())

    # Replace uncommon/llm punc
    punc_to_replace = [
        ("...", ", "),
        ("…", ", "),
        (":", ","),
        (" - ", ", "),
        (";", ", "),
        ("—", "-"),
        ("–", "-"),
        (" ,", ","),
        ("“", "\""),
        ("”", "\""),
        ("‘", "'"),
        ("’", "'"),
    ]
    for old_char_sequence, new_char in punc_to_replace:
        text = text.replace(old_char_sequence, new_char)

    # Add full stop if no ending punc
    text = text.rstrip(" ")
    sentence_enders = {".", "!", "?", "-", ","}
    if not any(text.endswith(p) for p in sentence_enders):
        text += "."

    return text


@dataclass
class Conditionals:
    """
    Conditionals for T3 and S3Gen
    - T3 conditionals:
        - speaker_emb
        - clap_emb
        - cond_prompt_speech_tokens
        - cond_prompt_speech_emb
        - emotion_adv
    - S3Gen conditionals:
        - prompt_token
        - prompt_token_len
        - prompt_feat
        - prompt_feat_len
        - embedding
    """
    t3: T3Cond
    gen: dict

    def to(self, device):
        self.t3 = self.t3.to(device=device)
        for k, v in self.gen.items():
            if torch.is_tensor(v):
                self.gen[k] = v.to(device=device)
        return self

    def save(self, fpath: Path):
        arg_dict = dict(
            t3=self.t3.__dict__,
            gen=self.gen
        )
        torch.save(arg_dict, fpath)

    @classmethod
    def load(cls, fpath, map_location="cpu"):
        if isinstance(map_location, str):
            map_location = torch.device(map_location)
        kwargs = torch.load(fpath, map_location=map_location, weights_only=True)
        return cls(T3Cond(**kwargs['t3']), kwargs['gen'])


class ChatterboxTTS:
    ENC_COND_LEN = 6 * S3_SR
    DEC_COND_LEN = 10 * S3GEN_SR

    def __init__(
        self,
        t3: T3,
        s3gen: S3Gen,
        ve: VoiceEncoder,
        tokenizer: EnTokenizer,
        device: str,
        conds: Conditionals = None,
    ):
        self.sr = S3GEN_SR  # sample rate of synthesized audio
        self.t3 = t3
        self.s3gen = s3gen
        self.ve = ve
        self.tokenizer = tokenizer
        self.device = device
        self.conds = conds
        self.watermarker = perth.PerthImplicitWatermarker()

    @classmethod
    def from_local(cls, ckpt_dir, device) -> 'ChatterboxTTS':
        ckpt_dir = Path(ckpt_dir)

        # Always load to CPU first for non-CUDA devices to handle CUDA-saved models
        if device in ["cpu", "mps"]:
            map_location = torch.device('cpu')
        else:
            map_location = None

        ve = VoiceEncoder()
        ve.load_state_dict(
            load_file(ckpt_dir / "ve.safetensors")
        )
        ve.to(device).eval()

        t3 = T3()
        t3_state = load_file(ckpt_dir / "t3_cfg.safetensors")
        if "model" in t3_state.keys():
            t3_state = t3_state["model"][0]
        t3.load_state_dict(t3_state)
        t3.to(device).eval()

        s3gen = S3Gen()
        s3gen.load_state_dict(
            load_file(ckpt_dir / "s3gen.safetensors"), strict=False
        )
        s3gen.to(device).eval()

        tokenizer = EnTokenizer(
            str(ckpt_dir / "tokenizer.json")
        )

        conds = None
        if (builtin_voice := ckpt_dir / "conds.pt").exists():
            conds = Conditionals.load(builtin_voice, map_location=map_location).to(device)

        return cls(t3, s3gen, ve, tokenizer, device, conds=conds)

    @classmethod
    def from_pretrained(cls, device) -> 'ChatterboxTTS':
        # Check if MPS is available on macOS
        if device == "mps" and not torch.backends.mps.is_available():
            if not torch.backends.mps.is_built():
                print("MPS not available because the current PyTorch install was not built with MPS enabled.")
            else:
                print("MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.")
            device = "cpu"

        for fpath in ["ve.safetensors", "t3_cfg.safetensors", "s3gen.safetensors", "tokenizer.json", "conds.pt"]:
            local_path = hf_hub_download(repo_id=REPO_ID, filename=fpath)

        return cls.from_local(Path(local_path).parent, device)

    def prepare_conditionals(self, wav_fpath, exaggeration=0.5):
        ## Load reference wav
        s3gen_ref_wav, _sr = librosa.load(wav_fpath, sr=S3GEN_SR)

        ref_16k_wav = librosa.resample(s3gen_ref_wav, orig_sr=S3GEN_SR, target_sr=S3_SR)

        s3gen_ref_wav = s3gen_ref_wav[:self.DEC_COND_LEN]
        s3gen_ref_dict = self.s3gen.embed_ref(s3gen_ref_wav, S3GEN_SR, device=self.device)

        # Speech cond prompt tokens
        if plen := self.t3.hp.speech_cond_prompt_len:
            s3_tokzr = self.s3gen.tokenizer
            t3_cond_prompt_tokens, _ = s3_tokzr.forward([ref_16k_wav[:self.ENC_COND_LEN]], max_len=plen)
            t3_cond_prompt_tokens = torch.atleast_2d(t3_cond_prompt_tokens).to(self.device)

        # Voice-encoder speaker embedding
        ve_embed = torch.from_numpy(self.ve.embeds_from_wavs([ref_16k_wav], sample_rate=S3_SR))
        ve_embed = ve_embed.mean(axis=0, keepdim=True).to(self.device)

        t3_cond = T3Cond(
            speaker_emb=ve_embed,
            cond_prompt_speech_tokens=t3_cond_prompt_tokens,
            emotion_adv=exaggeration * torch.ones(1, 1, 1),
        ).to(device=self.device)
        self.conds = Conditionals(t3_cond, s3gen_ref_dict)

    def generate(
        self,
        text,
        repetition_penalty=1.2,
        min_p=0.05,
        top_p=1.0,
        audio_prompt_path=None,
        exaggeration=0.5,
        cfg_weight=0.5,
        temperature=0.8,
    ):
        if audio_prompt_path:
            self.prepare_conditionals(audio_prompt_path, exaggeration=exaggeration)
        else:
            assert self.conds is not None, "Please `prepare_conditionals` first or specify `audio_prompt_path`"

        # Update exaggeration if needed
        if exaggeration != self.conds.t3.emotion_adv[0, 0, 0]:
            _cond: T3Cond = self.conds.t3
            self.conds.t3 = T3Cond(
                speaker_emb=_cond.speaker_emb,
                cond_prompt_speech_tokens=_cond.cond_prompt_speech_tokens,
                emotion_adv=exaggeration * torch.ones(1, 1, 1),
            ).to(device=self.device)

        # Norm and tokenize text
        text = punc_norm(text)
        text_tokens = self.tokenizer.text_to_tokens(text).to(self.device)

        if cfg_weight > 0.0:
            text_tokens = torch.cat([text_tokens, text_tokens], dim=0)  # Need two seqs for CFG

        sot = self.t3.hp.start_text_token
        eot = self.t3.hp.stop_text_token
        text_tokens = F.pad(text_tokens, (1, 0), value=sot)
        text_tokens = F.pad(text_tokens, (0, 1), value=eot)

        with torch.inference_mode():
            speech_tokens = self.t3.inference(
                t3_cond=self.conds.t3,
                text_tokens=text_tokens,
                max_new_tokens=1000,  # TODO: use the value in config
                temperature=temperature,
                cfg_weight=cfg_weight,
                repetition_penalty=repetition_penalty,
                min_p=min_p,
                top_p=top_p,
            )
            # Extract only the conditional batch.
            speech_tokens = speech_tokens[0]

            # TODO: output becomes 1D
            speech_tokens = drop_invalid_tokens(speech_tokens)
            
            speech_tokens = speech_tokens[speech_tokens < 6561]

            speech_tokens = speech_tokens.to(self.device)

            wav, _ = self.s3gen.inference(
                speech_tokens=speech_tokens,
                ref_dict=self.conds.gen,
            )
            wav = wav.squeeze(0).detach().cpu().numpy()
            watermarked_wav = self.watermarker.apply_watermark(wav, sample_rate=self.sr)
        return torch.from_numpy(watermarked_wav).unsqueeze(0)
</file>

<file path="candle/src/t3_model.rs">
use crate::gpt2::{Config as GPT2Config, GPT2Model};
use candle_core::{IndexOp, Result, Tensor};
use candle_nn::{Embedding, Linear, Module, VarBuilder};

pub struct T3Config {
    pub text_tokens_dict_size: usize,
    pub speech_tokens_dict_size: usize,
    pub hidden_size: usize,
    pub num_layers: usize,
    pub num_heads: usize,
    pub vocab_size: usize,
    pub speaker_embed_size: usize,
    pub start_speech_token: u32,
    pub stop_speech_token: u32,
    pub speech_cond_prompt_len: Option<usize>,
    pub use_perceiver_resampler: bool,
    pub emotion_adv: bool,
    pub n_positions: usize,
}

impl Default for T3Config {
    fn default() -> Self {
        Self {
            text_tokens_dict_size: 50276,
            speech_tokens_dict_size: 6563,
            hidden_size: 1024, // GPT2 Medium
            num_layers: 24,
            num_heads: 16,
            vocab_size: 50276, // Placeholder
            speaker_embed_size: 256,
            start_speech_token: 6561,
            stop_speech_token: 6562,
            speech_cond_prompt_len: Some(375),
            use_perceiver_resampler: false,
            emotion_adv: false,
            n_positions: 8196,
        }
    }
}

pub struct T3CondEnc {
    spkr_enc: Linear,
    emotion_adv_fc: Option<Linear>,
    // perceiver: Option<Perceiver>, // Placeholder for now if needed
    config: T3Config,
}

impl T3CondEnc {
    pub fn new(config: T3Config, vb: VarBuilder) -> Result<Self> {
        let spkr_enc = candle_nn::linear(
            config.speaker_embed_size,
            config.hidden_size,
            vb.pp("spkr_enc"),
        )?;

        let emotion_adv_fc = if config.emotion_adv {
            Some(candle_nn::linear_no_bias(
                1,
                config.hidden_size,
                vb.pp("emotion_adv_fc"),
            )?)
        } else {
            None
        };

        Ok(Self {
            spkr_enc,
            emotion_adv_fc,
            config,
        })
    }

    pub fn forward(
        &self,
        spk_emb: &Tensor,
        cond_prompt_speech_emb: Option<&Tensor>,
        emotion_adv: Option<&Tensor>,
    ) -> Result<Tensor> {
        // spk_emb: (B, E)
        let cond_spkr = self.spkr_enc.forward(spk_emb)?; // (B, H)
        let cond_spkr = cond_spkr.unsqueeze(1)?; // (B, 1, H)

        // Empty tensor for cat if needed (B, 0, H)
        // Note: Candle doesn't easily support 0-dim concats in all backends, but let's try standard Vec logic
        let mut embeds = vec![cond_spkr];

        // CLAP embed (placeholder)

        // Cond prompt speech emb
        if let Some(prompt_emb) = cond_prompt_speech_emb {
            // Assuming prompt_emb is already projected or compatible (B, L, H)
            // If using Perceiver, we would process it here.
            // Current T3 Turbo uses `speech_cond_prompt_len` but `use_perceiver_resampler = False`.
            // In `cond_enc.py`:
            // if cond_prompt_speech_emb is None: empty
            // elif hp.use_perceiver_resampler: perceiver(cond_prompt_speech_emb)
            // It seems if not using perceiver, it just passes it through?
            // Wait, `cond_enc.py` code:
            // "if cond_prompt_speech_emb is None ... elif hp.use_perceiver_resampler ... "
            // It doesn't explicitly say what happens if `!use_perceiver_resampler` and `cond_prompt_speech_emb` is present.
            // But looking at `ChatterboxTTS.prepare_conditionals`, `cond_prompt_speech_tokens` are created.
            // In T3 `inference` (Python), it creates `T3Cond`.
            // The `T3CondEnc` in Python seems to use `cond_prompt_speech_emb` from `T3Cond`.
            // But `prepare_conditionals` sets `cond_prompt_speech_tokens`, not `emb`.
            // Ah, `T3.forward` calls `prepare_input_embeds`.
            // In Python `T3`:
            // `cond_prompt_speech_emb` is derived from `cond.cond_prompt_speech_tokens` via `speech_emb` embedding layer?
            // No, `T3CondEnc` takes `T3Cond` which has `cond_prompt_speech_emb`.
            // Let's check where `cond_prompt_speech_emb` comes from in `T3.forward`.

            embeds.push(prompt_emb.clone());
        }

        if self.config.emotion_adv {
            if let Some(emo) = emotion_adv {
                // emo: (B, 1, 1) or (B, 1)
                let emo_proj = self.emotion_adv_fc.as_ref().unwrap().forward(emo)?; // (B, 1, H) or similar
                                                                                    // Ensure dims
                let emo_proj = if emo_proj.rank() == 2 {
                    emo_proj.unsqueeze(1)?
                } else {
                    emo_proj
                };
                embeds.push(emo_proj);
            }
        }

        Tensor::cat(&embeds, 1)
    }
}

pub struct T3 {
    gpt2: GPT2Model,
    text_emb: Embedding,
    speech_emb: Embedding,
    pos_emb: Embedding,
    _text_head: Linear,
    speech_head: Linear,
    cond_enc: T3CondEnc,
    config: T3Config,
}

impl T3 {
    pub fn new(config: T3Config, vb: VarBuilder) -> Result<Self> {
        let gpt2_config = GPT2Config {
            vocab_size: config.vocab_size,
            n_embd: config.hidden_size,
            n_layer: config.num_layers,
            n_head: config.num_heads,
            n_positions: config.n_positions,
            ..Default::default()
        };

        let gpt2 = GPT2Model::new(gpt2_config, vb.pp("tfmr"))?;
        let text_emb = candle_nn::embedding(
            config.text_tokens_dict_size,
            config.hidden_size,
            vb.pp("text_emb"),
        )?;
        let speech_emb = candle_nn::embedding(
            config.speech_tokens_dict_size,
            config.hidden_size,
            vb.pp("speech_emb"),
        )?;
        // Use shared position embeddings from transformer
        let pos_emb = candle_nn::embedding(
            config.n_positions,
            config.hidden_size,
            vb.pp("tfmr").pp("wpe"),
        )?;
        let _text_head = candle_nn::linear_no_bias(
            config.hidden_size,
            config.text_tokens_dict_size,
            vb.pp("text_head"),
        )?;
        let speech_head = candle_nn::linear(
            config.hidden_size,
            config.speech_tokens_dict_size,
            vb.pp("speech_head"),
        )?;

        // cond_enc might need config clone
        let cond_enc = T3CondEnc::new(
            T3Config {
                text_tokens_dict_size: config.text_tokens_dict_size,
                speech_tokens_dict_size: config.speech_tokens_dict_size,
                hidden_size: config.hidden_size,
                num_layers: config.num_layers,
                num_heads: config.num_heads,
                vocab_size: config.vocab_size,
                speaker_embed_size: config.speaker_embed_size,
                start_speech_token: config.start_speech_token,
                stop_speech_token: config.stop_speech_token,
                speech_cond_prompt_len: config.speech_cond_prompt_len,
                use_perceiver_resampler: config.use_perceiver_resampler,
                emotion_adv: config.emotion_adv,
                n_positions: config.n_positions,
            },
            vb.pp("cond_enc"),
        )?;

        Ok(Self {
            gpt2,
            text_emb,
            speech_emb,
            pos_emb,
            _text_head,
            speech_head,
            cond_enc,
            config,
        })
    }

    pub fn prepare_input_embeds(
        &self,
        text_tokens: &Tensor,
        speech_tokens: &Tensor,
        spk_emb: &Tensor,
        cond_prompt_speech_tokens: Option<&Tensor>,
        emotion_adv: Option<&Tensor>,
    ) -> Result<Tensor> {
        // text_tokens: (B, Lt)
        // speech_tokens: (B, Ls)
        // spk_emb: (B, E)

        // Handle prompt tokens embedding if present
        let cond_prompt_speech_emb = if let Some(tokens) = cond_prompt_speech_tokens {
            Some(self.speech_emb.forward(tokens)?)
        } else {
            None
        };

        let cond_emb =
            self.cond_enc
                .forward(spk_emb, cond_prompt_speech_emb.as_ref(), emotion_adv)?; // (B, Lc, H)
        let (_b, lc, _h) = cond_emb.dims3()?;
        let cond_pos_ids = Tensor::arange(0u32, lc as u32, spk_emb.device())?.unsqueeze(0)?;
        let cond_pos = self.pos_emb.forward(&cond_pos_ids)?;
        let cond_emb = (cond_emb + cond_pos)?;

        let (_b, lt) = text_tokens.dims2()?;
        let text_pos_ids = Tensor::arange(0u32, lt as u32, text_tokens.device())?.unsqueeze(0)?;
        let text_emb = self.text_emb.forward(text_tokens)?; // (B, Lt, H)
        let text_pos = self.pos_emb.forward(&text_pos_ids)?;
        let text_emb = (text_emb + text_pos)?;

        let (_b, ls) = speech_tokens.dims2()?;
        let speech_pos_ids =
            Tensor::arange(0u32, ls as u32, speech_tokens.device())?.unsqueeze(0)?;
        let speech_emb = self.speech_emb.forward(speech_tokens)?; // (B, Ls, H)
        let speech_pos = self.pos_emb.forward(&speech_pos_ids)?;
        let speech_emb = (speech_emb + speech_pos)?;

        // Concatenate along time dimension (dim 1)
        // cond, text, speech
        Tensor::cat(&[&cond_emb, &text_emb, &speech_emb], 1)
    }

    pub fn generate(
        &self,
        text_tokens: &Tensor,
        spk_emb: &Tensor,
        cond_prompt_speech_tokens: Option<&Tensor>,
        emotion_adv: Option<&Tensor>,
        max_gen_len: usize,
        temperature: f32,
        top_p: f32,
        top_k: usize,
        repetition_penalty: f32,
        seed: u64,
    ) -> Result<Tensor> {
        let (b, _lt) = text_tokens.dims2()?;
        let device = text_tokens.device();

        let mut logits_processor = crate::sampling::LogitsProcessor::new(
            seed,
            Some(temperature as f64),
            Some(top_p as f64),
            Some(top_k),
        );

        // Start with start_speech_token
        let start_token = Tensor::new(&[[self.config.start_speech_token]], device)?; // (1, 1)
        let mut speech_tokens_v = vec![self.config.start_speech_token];
        let mut speech_tokens_tensor = start_token.repeat((b, 1))?; // (B, 1)

        for i in 0..max_gen_len {
            if i % 10 == 0 {
                eprintln!("[T3] Generating token {}/{}...", i, max_gen_len);
            }
            let embeds = self.prepare_input_embeds(
                text_tokens,
                &speech_tokens_tensor,
                spk_emb,
                cond_prompt_speech_tokens,
                emotion_adv,
            )?;

            // Forward pass
            let hidden_states = self.gpt2.forward_embeds_no_pos(&embeds)?; // (B, L, H)

            // Get last token logits
            let last_hidden = hidden_states.i((.., hidden_states.dim(1)? - 1, ..))?; // (B, H)
            let logits = self.speech_head.forward(&last_hidden)?; // (B, Vocab)

            // Sample for each batch (assuming B=1 for now as per Python sync inference)
            let logits_0 = logits.i(0)?;
            let next_token =
                logits_processor.sample(&logits_0, &speech_tokens_v, Some(repetition_penalty))?;

            speech_tokens_v.push(next_token);
            let next_token_tensor = Tensor::new(&[[next_token]], device)?;

            // Append
            speech_tokens_tensor = Tensor::cat(&[&speech_tokens_tensor, &next_token_tensor], 1)?;

            // Check EOS
            if next_token == self.config.stop_speech_token {
                break;
            }
        }

        Ok(speech_tokens_tensor)
    }
}
</file>

<file path="candle/src/hifigan.rs">
//! HiFTGenerator Vocoder - converts mel spectrograms to audio waveforms.
//!
//! Aligned with Python implementation: chatterbox/models/s3gen/hifigan.py

use candle_core::{DType, Module, Result, Tensor};
use candle_nn::{Conv1d, Conv1dConfig, ConvTranspose1d, ConvTranspose1dConfig, Linear, VarBuilder};
use std::f32::consts::PI;

/// HiFTGenerator configuration
#[derive(Debug, Clone)]
pub struct HiFTConfig {
    pub in_channels: usize,
    pub base_channels: usize,
    pub nb_harmonics: usize,
    pub sampling_rate: u32,
    pub upsample_rates: Vec<usize>,
    pub upsample_kernel_sizes: Vec<usize>,
    pub resblock_kernel_sizes: Vec<usize>,
    pub resblock_dilation_sizes: Vec<Vec<usize>>,
    pub n_fft: usize,
    pub hop_len: usize,
}

impl Default for HiFTConfig {
    fn default() -> Self {
        Self {
            in_channels: 80,
            base_channels: 512,
            nb_harmonics: 8,
            sampling_rate: 24000,
            upsample_rates: vec![8, 5, 3],
            upsample_kernel_sizes: vec![16, 11, 7],
            resblock_kernel_sizes: vec![3, 7, 11],
            resblock_dilation_sizes: vec![vec![1, 3, 5], vec![1, 3, 5], vec![1, 3, 5]],
            n_fft: 16,
            hop_len: 4,
        }
    }
}

/// Helper to load WeightNorm Conv1d
fn load_wn_conv1d(
    in_c: usize,
    out_c: usize,
    k: usize,
    cfg: Conv1dConfig,
    vb: VarBuilder,
) -> Result<Conv1d> {
    let weight_v = vb.get((out_c, in_c, k), "parametrizations.weight.original1")?;
    let weight_g = vb.get((out_c, 1, 1), "parametrizations.weight.original0")?;
    let bias = vb.get(out_c, "bias")?;

    // weight = g * v / ||v||
    let norm = weight_v.sqr()?.sum_keepdim((1, 2))?.sqrt()?;
    let weight = weight_v.broadcast_div(&norm)?.broadcast_mul(&weight_g)?;

    Ok(Conv1d::new(weight, Some(bias), cfg))
}

/// Helper to load WeightNorm ConvTranspose1d
fn load_wn_conv_transpose1d(
    in_c: usize,
    out_c: usize,
    k: usize,
    cfg: ConvTranspose1dConfig,
    vb: VarBuilder,
) -> Result<ConvTranspose1d> {
    let weight_v = vb.get((in_c, out_c, k), "parametrizations.weight.original1")?;
    let weight_g = vb
        .get((in_c, 1, 1), "parametrizations.weight.original0")
        .or_else(|_| {
            vb.get(in_c, "parametrizations.weight.original0")?
                .reshape((in_c, 1, 1))
        })?;
    let bias = vb.get(out_c, "bias")?;

    // weight = g * v / ||v|| (norm over out_channels and kernel)
    let norm = weight_v.sqr()?.sum_keepdim((1, 2))?.sqrt()?;
    let weight = weight_v.broadcast_div(&norm)?.broadcast_mul(&weight_g)?;

    Ok(ConvTranspose1d::new(weight, Some(bias), cfg))
}

/// Snake activation function: x + (1/a) * sin²(ax)
struct Snake {
    alpha: Tensor,
}

impl Snake {
    fn new(channels: usize, vb: VarBuilder) -> Result<Self> {
        let alpha = vb.get(channels, "alpha")?;
        Ok(Self {
            alpha: alpha.reshape((1, channels, 1))?,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let ax = x.broadcast_mul(&self.alpha)?;
        let sin_ax = ax.sin()?;
        let sin_sq = (&sin_ax * &sin_ax)?;
        let alpha_plus_eps = self
            .alpha
            .broadcast_add(&Tensor::new(1e-9f32, x.device())?)?;
        let inv_alpha = alpha_plus_eps.recip()?;
        let result = (x + sin_sq.broadcast_mul(&inv_alpha)?)?;
        Ok(result)
    }
}

/// Residual block with Snake activations
struct ResBlock {
    convs1: Vec<Conv1d>,
    convs2: Vec<Conv1d>,
    activations1: Vec<Snake>,
    activations2: Vec<Snake>,
}

impl ResBlock {
    fn new(
        channels: usize,
        kernel_size: usize,
        dilations: &[usize],
        vb: VarBuilder,
    ) -> Result<Self> {
        let mut convs1 = Vec::new();
        let mut convs2 = Vec::new();
        let mut activations1 = Vec::new();
        let mut activations2 = Vec::new();

        for (i, &dilation) in dilations.iter().enumerate() {
            let padding = (kernel_size * dilation - dilation) / 2;
            convs1.push(load_wn_conv1d(
                channels,
                channels,
                kernel_size,
                Conv1dConfig {
                    padding,
                    dilation,
                    ..Default::default()
                },
                vb.pp(format!("convs1.{}", i)),
            )?);
            activations1.push(Snake::new(channels, vb.pp(format!("activations1.{}", i)))?);

            convs2.push(load_wn_conv1d(
                channels,
                channels,
                kernel_size,
                Conv1dConfig {
                    padding: kernel_size / 2,
                    ..Default::default()
                },
                vb.pp(format!("convs2.{}", i)),
            )?);
            activations2.push(Snake::new(channels, vb.pp(format!("activations2.{}", i)))?);
        }

        Ok(Self {
            convs1,
            convs2,
            activations1,
            activations2,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let mut x = x.clone();
        for i in 0..self.convs1.len() {
            let xt = self.activations1[i].forward(&x)?;
            let xt = self.convs1[i].forward(&xt)?;
            let xt = self.activations2[i].forward(&xt)?;
            let xt = self.convs2[i].forward(&xt)?;
            x = (x + xt)?;
        }
        Ok(x)
    }
}

/// ConvRNNF0Predictor
struct F0Predictor {
    condnet_convs: Vec<Conv1d>,
    classifier: Linear,
}

impl F0Predictor {
    fn new(in_channels: usize, cond_channels: usize, vb: VarBuilder) -> Result<Self> {
        let mut condnet_convs = Vec::new();
        let condnet_vb = vb.pp("condnet");

        // 5 blocks of (WN-Conv1d + ELU)
        for i in 0..5 {
            let in_c = if i == 0 { in_channels } else { cond_channels };
            condnet_convs.push(load_wn_conv1d(
                in_c,
                cond_channels,
                3,
                Conv1dConfig {
                    padding: 1,
                    ..Default::default()
                },
                condnet_vb.pp(i * 2), // Index 0, 2, 4, 6, 8
            )?);
        }

        let classifier = candle_nn::linear(cond_channels, 1, vb.pp("classifier"))?;

        Ok(Self {
            condnet_convs,
            classifier,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let mut x = x.clone();
        for conv in &self.condnet_convs {
            x = conv.forward(&x)?;
            x = x.elu(1.0)?;
        }
        x = x.transpose(1, 2)?; // (B, C, T) -> (B, T, C)
        let x = self.classifier.forward(&x)?; // (B, T, 1)
        Ok(x.abs()?) // Scale normalized range [0, 1] to Hz [0, 500]
    }
}

/// Sine generator for harmonic source
struct SineGen {
    harmonic_num: usize,
    sine_amp: f32,
    sampling_rate: u32,
    voiced_threshold: f32,
}

impl SineGen {
    fn new(
        sampling_rate: u32,
        harmonic_num: usize,
        sine_amp: f32,
        _noise_std: f32,
        voiced_threshold: f32,
    ) -> Self {
        Self {
            harmonic_num,
            sine_amp,
            sampling_rate,
            voiced_threshold,
        }
    }

    fn forward(&self, f0: &Tensor, upsmp_rate: usize) -> Result<(Tensor, Tensor)> {
        let device = f0.device();
        let (b, _, t_mel) = f0.dims3()?;
        let t_audio = t_mel * upsmp_rate;

        // F0 upsampling (nearest)
        let f0_upsampled = f0
            .unsqueeze(3)?
            .repeat((1, 1, 1, upsmp_rate))?
            .reshape((b, 1, t_audio))?;

        let mut f_mat_slices = Vec::new();
        for i in 0..=self.harmonic_num {
            let f = (&f0_upsampled * ((i + 1) as f64 / self.sampling_rate as f64))?;
            f_mat_slices.push(f);
        }
        let f_mat = Tensor::cat(&f_mat_slices, 1)?; // (B, H+1, T_audio)

        // theta = 2 * PI * (cumsum(f_mat) % 1)
        let f_mat_vec = f_mat.to_vec3::<f32>()?;
        let mut cumsum_data = vec![0.0f32; b * (self.harmonic_num + 1) * t_audio];
        for b_idx in 0..b {
            for h_idx in 0..=self.harmonic_num {
                let mut sum = 0.0;
                for t_idx in 0..t_audio {
                    sum += f_mat_vec[b_idx][h_idx][t_idx];
                    sum %= 1.0;
                    cumsum_data
                        [b_idx * (self.harmonic_num + 1) * t_audio + h_idx * t_audio + t_idx] = sum;
                }
            }
        }
        let cumsum = Tensor::from_vec(cumsum_data, f_mat.dims(), device)?;
        let phase = (cumsum * (2.0 * std::f32::consts::PI as f64))?;

        // Random initial phase for harmonics
        let rand_shape = (b, self.harmonic_num + 1, 1);
        let mut phase_vec_data = vec![0.0f32; b * (self.harmonic_num + 1)];
        for batch_idx in 0..b {
            // phase_vec[:, 0, :] = 0 (fundamental)
            phase_vec_data[batch_idx * (self.harmonic_num + 1)] = 0.0;
            for h in 1..=self.harmonic_num {
                phase_vec_data[batch_idx * (self.harmonic_num + 1) + h] =
                    (rand::random::<f32>() * 2.0 - 1.0) * std::f32::consts::PI;
            }
        }
        let phase_vec = Tensor::from_vec(phase_vec_data, rand_shape, device)?;
        let sine_waves = ((phase.broadcast_add(&phase_vec))?.sin()? * self.sine_amp as f64)?;

        let uv = f0_upsampled
            .gt(self.voiced_threshold as f64)?
            .to_dtype(DType::F32)?;

        // noise
        let noise_amp = {
            let voiced_part = (&uv * 0.003/* noise_std */)?;
            let unvoiced_part = ((Tensor::new(1.0f32, device)?.broadcast_as(uv.dims())? - &uv)?
                * (self.sine_amp / 3.0) as f64)?;
            (&voiced_part + &unvoiced_part)?
        };
        let noise =
            (Tensor::randn(0.0f32, 1.0, sine_waves.dims(), device)?.broadcast_mul(&noise_amp))?;

        let sine_waves = (sine_waves.broadcast_mul(&uv)?.broadcast_add(&noise))?;

        Ok((sine_waves, uv))
    }
}

/// Source module (SourceModuleHnNSF)
struct SourceModule {
    sine_gen: SineGen,
    l_linear: Linear,
}

impl SourceModule {
    fn new(
        sampling_rate: u32,
        harmonic_num: usize,
        sine_amp: f32,
        noise_std: f32,
        voiced_threshold: f32,
        vb: VarBuilder,
    ) -> Result<Self> {
        let sine_gen = SineGen::new(
            sampling_rate,
            harmonic_num,
            sine_amp,
            noise_std,
            voiced_threshold,
        );
        let l_linear = candle_nn::linear(harmonic_num + 1, 1, vb.pp("l_linear"))?;
        Ok(Self { sine_gen, l_linear })
    }

    fn forward(&self, f0: &Tensor, upsmp_rate: usize) -> Result<(Tensor, Tensor, Tensor)> {
        // f0: (B, T, 1) -> transpose to (B, 1, T)
        let f0_t = f0.transpose(1, 2)?;
        let (sine_wavs, uv) = self.sine_gen.forward(&f0_t, upsmp_rate)?;

        // sine_merge = tanh(linear(sine_wavs))
        let sine_wavs_t = sine_wavs.transpose(1, 2)?;
        let sine_merge = self
            .l_linear
            .forward(&sine_wavs_t)?
            .transpose(1, 2)?
            .tanh()?;

        // noise branch: same shape as uv
        let noise = (Tensor::randn(0.0f32, 1.0, uv.dims(), uv.device())?
            * (self.sine_gen.sine_amp / 3.0) as f64)?;

        Ok((sine_merge, noise, uv))
    }
}

pub struct HiFTGenerator {
    config: HiFTConfig,
    f0_predictor: F0Predictor,
    source_module: SourceModule,
    conv_pre: Conv1d,
    ups: Vec<ConvTranspose1d>,
    source_downs: Vec<Conv1d>,
    source_resblocks: Vec<ResBlock>,
    resblocks: Vec<ResBlock>,
    conv_post: Conv1d,
}

impl HiFTGenerator {
    pub fn new(config: HiFTConfig, vb: VarBuilder) -> Result<Self> {
        eprintln!(
            "[HiFTGenerator::new] START loading from {}",
            vb.prefix().to_string()
        );
        eprintln!(
            "[HiFTGenerator::new] in_ch={}, base_ch={}, n_fft={}",
            config.in_channels, config.base_channels, config.n_fft
        );

        let f0_predictor = F0Predictor::new(config.in_channels, 512, vb.pp("f0_predictor"))?;
        eprintln!("[HiFTGenerator::new] f0_predictor loaded");

        let _upsample_scale: usize =
            config.upsample_rates.iter().product::<usize>() * config.hop_len;
        let source_module = SourceModule::new(
            config.sampling_rate,
            config.nb_harmonics,
            0.1,   // sine_amp
            0.003, // noise_std
            10.0,  // voiced_threshold
            vb.pp("m_source"),
        )?;

        let conv_pre = load_wn_conv1d(
            config.in_channels,
            config.base_channels,
            7,
            Conv1dConfig {
                padding: 3,
                ..Default::default()
            },
            vb.pp("conv_pre"),
        )?;

        let mut ups = Vec::new();
        for (i, (&rate, &kernel)) in config
            .upsample_rates
            .iter()
            .zip(config.upsample_kernel_sizes.iter())
            .enumerate()
        {
            let in_ch = config.base_channels / (1 << i);
            let out_ch = config.base_channels / (1 << (i + 1));
            ups.push(load_wn_conv_transpose1d(
                in_ch,
                out_ch,
                kernel,
                ConvTranspose1dConfig {
                    stride: rate,
                    padding: (kernel - rate) / 2,
                    ..Default::default()
                },
                vb.pp(format!("ups.{}", i)),
            )?);
        }

        let mut source_downs = Vec::new();
        let mut source_resblocks = Vec::new();

        let factors = vec![15, 3, 1]; // Derived from [15, 3, 1] which is cumprod([1, 3, 5])[::-1]
        for (i, (&kernel, dilations)) in [7, 7, 11]
            .iter()
            .zip([vec![1, 3, 5], vec![1, 3, 5], vec![1, 3, 5]].iter())
            .enumerate()
        {
            let u = factors[i];
            let ch = config.base_channels / (1 << (i + 1));

            // source_downs are regular Conv1d in Python
            let cfg = if u == 1 {
                Conv1dConfig {
                    padding: 0,
                    stride: 1,
                    ..Default::default()
                }
            } else {
                Conv1dConfig {
                    padding: u / 2,
                    stride: u,
                    ..Default::default()
                }
            };
            let k_size = if u == 1 { 1 } else { u * 2 };

            source_downs.push(candle_nn::conv1d(
                config.n_fft + 2,
                ch,
                k_size,
                cfg,
                vb.pp(format!("source_downs.{}", i)),
            )?);
            eprintln!(
                "[HiFTGenerator::new] source_downs.{} loaded (ch={}, k={}, u={})",
                i, ch, k_size, u
            );

            source_resblocks.push(ResBlock::new(
                ch,
                kernel,
                dilations,
                vb.pp(format!("source_resblocks.{}", i)),
            )?);
            eprintln!("[HiFTGenerator::new] source_resblocks.{} loaded", i);
        }

        let mut resblocks = Vec::new();
        for i in 0..ups.len() {
            let ch = config.base_channels / (1 << (i + 1));
            for (j, (&kernel, dilations)) in config
                .resblock_kernel_sizes
                .iter()
                .zip(config.resblock_dilation_sizes.iter())
                .enumerate()
            {
                resblocks.push(ResBlock::new(
                    ch,
                    kernel,
                    dilations,
                    vb.pp(format!(
                        "resblocks.{}",
                        i * config.resblock_kernel_sizes.len() + j
                    )),
                )?);
            }
        }

        let final_ch = config.base_channels / (1 << ups.len());
        let conv_post = load_wn_conv1d(
            final_ch,
            config.n_fft + 2,
            7,
            Conv1dConfig {
                padding: 3,
                ..Default::default()
            },
            vb.pp("conv_post"),
        )?;

        Ok(Self {
            config,
            f0_predictor,
            source_module,
            conv_pre,
            ups,
            source_downs,
            source_resblocks,
            resblocks,
            conv_post,
        })
    }

    pub fn inference(&self, mel: &Tensor) -> Result<Tensor> {
        eprintln!("[HiFTGenerator::inference] mel input: {:?}", mel.dims());
        let f0 = self.f0_predictor.forward(mel)?; // (B, T_mel, 1)
        let f0_v = f0.to_vec3::<f32>()?;
        let mut f0_min = f32::MAX;
        let mut f0_max = f32::MIN;
        for b in &f0_v {
            for t in b {
                for val in t {
                    f0_min = f0_min.min(*val);
                    f0_max = f0_max.max(*val);
                }
            }
        }
        eprintln!(
            "[HiFTGenerator::inference] f0 range: [{}, {}]",
            f0_min, f0_max
        );

        let upsample_factor =
            self.config.upsample_rates.iter().product::<usize>() * self.config.hop_len;
        eprintln!(
            "[HiFTGenerator::inference] upsample_factor: {}",
            upsample_factor
        );

        let (sine_merge, noise, _uv) = self.source_module.forward(&f0, upsample_factor)?;

        let source = (sine_merge + noise)?;

        let audio = self.decode(mel, &source)?;
        Ok(audio)
    }

    fn decode(&self, mel: &Tensor, source: &Tensor) -> Result<Tensor> {
        // source_squeezed: (B, 1, T) -> (B, T)
        let (s_real, s_imag) =
            simple_stft(&source.squeeze(1)?, self.config.n_fft, self.config.hop_len)?;

        let s_stft = Tensor::cat(&[&s_real, &s_imag], 1)?; // (B, 18, T)

        let mut x = self.conv_pre.forward(mel)?;
        let num_kernels = self.config.resblock_kernel_sizes.len();

        for i in 0..self.ups.len() {
            x = candle_nn::ops::leaky_relu(&x, 0.1)?;
            x = self.ups[i].forward(&x)?;

            if i == self.ups.len() - 1 {
                // ReflectionPad1d((1, 0)) - adds 1 sample at the beginning
                let pad_val = x.narrow(2, 0, 1)?;
                x = Tensor::cat(&[pad_val, x], 2)?;
            }

            // Fusion
            let si = self.source_downs[i].forward(&s_stft)?;
            let si = self.source_resblocks[i].forward(&si)?;

            // Alignment: crop or pad si to match x
            let target_len = x.dim(2)?;
            let current_len = si.dim(2)?;
            let si = if current_len > target_len {
                si.narrow(2, 0, target_len)?
            } else if current_len < target_len {
                si.pad_with_zeros(2, 0, target_len - current_len)?
            } else {
                si
            };
            x = (x + si)?;

            let mut xs: Option<Tensor> = None;
            for j in 0..num_kernels {
                let rb = &self.resblocks[i * num_kernels + j];
                let out = rb.forward(&x)?;
                xs = Some(match xs {
                    Some(acc) => (acc + out)?,
                    None => out,
                });
            }
            x = (xs.unwrap() / num_kernels as f64)?;
        }

        x = candle_nn::ops::leaky_relu(&x, 0.01)?;
        x = self.conv_post.forward(&x)?;

        let chunks = x.chunk(2, 1)?;
        let real = chunks[0].clone();
        let imag = chunks[1].clone();

        let audio = simple_istft(&real, &imag, self.config.n_fft, self.config.hop_len)?;
        audio.clamp(-0.99f32, 0.99f32)
    }
}

/// Real STFT implementation using realfft crate
fn simple_stft(x: &Tensor, n_fft: usize, hop_len: usize) -> Result<(Tensor, Tensor)> {
    use realfft::RealFftPlanner;
    let (b, t) = x.dims2()?;
    let device = x.device();

    if t < n_fft {
        let n_bins = n_fft / 2 + 1;
        return Ok((
            Tensor::zeros((b, n_bins, 1), DType::F32, device)?,
            Tensor::zeros((b, n_bins, 1), DType::F32, device)?,
        ));
    }
    let n_frames = (t - n_fft) / hop_len + 1;
    let n_bins = n_fft / 2 + 1;

    // Check for overflow or massive allocation - safety limit
    if n_frames > 2_000_000 {
        return Err(candle_core::Error::Msg(format!(
            "Too many STFT frames: {}",
            n_frames
        )));
    }

    let window: Vec<f32> = (0..n_fft)
        .map(|i| 0.5 * (1.0 - (2.0 * PI * i as f32 / n_fft as f32).cos()))
        .collect();
    let x_data = x.to_vec2::<f32>()?;
    let mut planner = RealFftPlanner::<f32>::new();
    let fft = planner.plan_fft_forward(n_fft);
    let mut all_real = Vec::with_capacity(b * n_bins * n_frames);
    let mut all_imag = Vec::with_capacity(b * n_bins * n_frames);

    for batch_data in &x_data {
        for frame_idx in 0..n_frames {
            let start = frame_idx * hop_len;
            let mut windowed: Vec<f32> = batch_data[start..start + n_fft]
                .iter()
                .zip(window.iter())
                .map(|(s, w)| s * w)
                .collect();
            let mut spectrum = fft.make_output_vec();
            fft.process(&mut windowed, &mut spectrum)
                .map_err(|e| candle_core::Error::Msg(format!("FFT error: {:?}", e)))?;
            for c in &spectrum {
                all_real.push(c.re);
                all_imag.push(c.im);
            }
        }
    }
    let real = Tensor::from_vec(all_real, (b, n_frames, n_bins), device)?
        .permute((0, 2, 1))?
        .contiguous()?;
    let imag = Tensor::from_vec(all_imag, (b, n_frames, n_bins), device)?
        .permute((0, 2, 1))?
        .contiguous()?;
    Ok((real, imag))
}

/// Real iSTFT implementation using realfft crate with overlap-add synthesis
fn simple_istft(real: &Tensor, imag: &Tensor, n_fft: usize, hop_len: usize) -> Result<Tensor> {
    use realfft::RealFftPlanner;
    use rustfft::num_complex::Complex;
    let (b, n_bins, n_frames) = real.dims3()?;
    let device = real.device();
    if n_frames == 0 {
        return Tensor::zeros((b, 1, 0), DType::F32, device);
    }
    let out_len = (n_frames - 1) * hop_len + n_fft;
    let window: Vec<f32> = (0..n_fft)
        .map(|i| 0.5 * (1.0 - (2.0 * PI * i as f32 / n_fft as f32).cos()))
        .collect();
    let real_data = real.to_vec3::<f32>()?;
    let imag_data = imag.to_vec3::<f32>()?;
    let mut planner = RealFftPlanner::<f32>::new();
    let ifft = planner.plan_fft_inverse(n_fft);
    let mut all_output = Vec::with_capacity(b * out_len);

    for batch_idx in 0..b {
        let mut output = vec![0.0f32; out_len];
        let mut window_sum = vec![0.0f32; out_len];

        for frame_idx in 0..n_frames {
            let mut spectrum = ifft.make_input_vec();
            for i in 0..n_bins {
                let re = real_data[batch_idx][i][frame_idx];
                let im = if i == 0 || i == n_bins - 1 {
                    0.0
                } else {
                    imag_data[batch_idx][i][frame_idx]
                };
                spectrum[i] = Complex::new(re, im);
            }

            let mut time_data = ifft.make_output_vec();
            ifft.process(&mut spectrum, &mut time_data)
                .map_err(|e| candle_core::Error::Msg(format!("FFT error: {:?}", e)))?;

            let norm = 1.0 / n_fft as f32;
            let start = frame_idx * hop_len;
            for (i, &sample) in time_data.iter().enumerate() {
                if start + i < out_len {
                    let w = window[i];
                    output[start + i] += sample * norm * w;
                    window_sum[start + i] += w * w;
                }
            }
        }

        for i in 0..out_len {
            if window_sum[i] > 1e-8 {
                output[i] /= window_sum[i];
            }
            all_output.push(output[i]);
        }
    }
    Tensor::from_vec(all_output, (b, 1, out_len), device)
}
</file>

<file path="pyproject.toml">
[project]
name = "chatterbox-tts"
version = "0.1.6"
description = "Chatterbox: Open Source TTS and Voice Conversion by Resemble AI"
readme = "README.md"
requires-python = ">=3.10"
license = {file = "LICENSE"}
authors = [
    {name = "resemble-ai", email = "engineering@resemble.ai"}
]
dependencies = [
    "numpy>=1.24.0,<1.26.0",
    "librosa==0.11.0",
    "s3tokenizer",
    "torch==2.6.0",
    "torchaudio==2.6.0",
    "transformers==4.46.3",
    "diffusers==0.29.0",
    "resemble-perth==1.0.1",
    "conformer==0.3.2",
    "safetensors==0.5.3",
    "spacy-pkuseg",
    "pykakasi==2.3.0",
    "gradio==5.44.1",
    "pyloudnorm",
    "omegaconf"
]

[project.urls]
Homepage = "https://github.com/resemble-ai/chatterbox"
Repository = "https://github.com/resemble-ai/chatterbox"

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
where = ["src"]
</file>

<file path="candle/src/chatterbox.rs">
use candle_core::{DType, Device, Result, Tensor};
use candle_nn::VarBuilder;
use hf_hub::api::sync::Api;
use std::path::{Path, PathBuf};
use tokenizers::Tokenizer;

use crate::audio::{self, S3GEN_SR, S3_SR};
use crate::s3gen::S3Gen;
use crate::t3_model::{T3Config, T3};
use crate::voice_encoder::{VoiceEncoder, VoiceEncoderConfig};
use crate::GenerateConfig;

pub struct ChatterboxTTS {
    t3: T3,
    s3gen: S3Gen,
    s3tokenizer: crate::s3tokenizer::S3TokenizerV2,
    voice_encoder: VoiceEncoder,
    tokenizer: Tokenizer,
    device: Device,
}

impl ChatterboxTTS {
    pub fn from_pretrained(device: Device) -> Result<Self> {
        let api = Api::new().map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        let repo = api.model("ResembleAI/chatterbox".to_string());
        let t3_path = repo
            .get("t3_cfg.safetensors")
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        let s3gen_path = repo
            .get("s3gen.safetensors")
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        let ve_path = repo
            .get("ve.safetensors")
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        let s3tokenizer_path = api
            .model("ResembleAI/s3tokenizer-v2".to_string())
            .get("model.safetensors")
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        let tokenizer_path = repo
            .get("tokenizer.json")
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        Self::from_local(
            t3_path,
            s3gen_path,
            ve_path,
            s3tokenizer_path,
            tokenizer_path,
            device,
        )
    }

    pub fn from_local(
        t3_path: PathBuf,
        s3gen_path: PathBuf,
        ve_path: PathBuf,
        s3tokenizer_path: PathBuf,
        tokenizer_path: PathBuf,
        device: Device,
    ) -> Result<Self> {
        let vb_t3 =
            unsafe { VarBuilder::from_mmaped_safetensors(&[t3_path], DType::F32, &device)? };
        let t3 = T3::new(T3Config::default(), vb_t3)?;
        let vb_s3 =
            unsafe { VarBuilder::from_mmaped_safetensors(&[s3gen_path], DType::F32, &device)? };
        let s3gen = S3Gen::new(vb_s3, false)?;
        let vb_ve =
            unsafe { VarBuilder::from_mmaped_safetensors(&[ve_path], DType::F32, &device)? };
        let voice_encoder = VoiceEncoder::new(VoiceEncoderConfig::default(), vb_ve)?;
        let vb_s3tok = unsafe {
            VarBuilder::from_mmaped_safetensors(&[s3tokenizer_path], DType::F32, &device)?
        };
        let s3tokenizer = crate::s3tokenizer::S3TokenizerV2::new(
            &crate::s3tokenizer::ModelConfig::default(),
            vb_s3tok,
        )?;
        let tokenizer = Tokenizer::from_file(&tokenizer_path)
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        Ok(Self {
            t3,
            s3gen,
            s3tokenizer,
            voice_encoder,
            tokenizer,
            device,
        })
    }

    pub fn generate_speech(
        &self,
        text: &str,
        ref_audio_path: &Path,
        config: GenerateConfig,
    ) -> Result<(Vec<f32>, u32)> {
        let (ref_samples, ref_sr) =
            audio::load_wav(ref_audio_path).map_err(|e| candle_core::Error::Msg(e))?;
        let ref_samples_16k = if ref_sr != S3_SR {
            audio::resample(&ref_samples, ref_sr, S3_SR).map_err(|e| candle_core::Error::Msg(e))?
        } else {
            ref_samples.clone()
        };
        let ref_samples_24k = if ref_sr != S3GEN_SR {
            audio::resample(&ref_samples, ref_sr, S3GEN_SR)
                .map_err(|e| candle_core::Error::Msg(e))?
        } else {
            ref_samples
        };

        // VoiceEncoder expects 16kHz mel (40 channels, n_fft=400)
        let config_ve = audio::MelConfig {
            n_fft: 400,
            hop_length: 160,
            win_length: 400,
            n_mels: 40,
            fmax: 8000.0,
        };
        // S3Tokenizer expects 16kHz mel (128 channels, n_fft=400)
        let config_s3tok = audio::MelConfig {
            n_fft: 400,
            hop_length: 160,
            win_length: 400,
            n_mels: 128,
            fmax: 8000.0,
        };

        let mel_40 =
            audio::compute_mel_spectrogram(&ref_samples_16k, S3_SR, &self.device, &config_ve)?;

        let mel_128 =
            audio::compute_mel_spectrogram(&ref_samples_16k, S3_SR, &self.device, &config_s3tok)?;

        let mel_80_24k = audio::compute_mel_spectrogram(
            &ref_samples_24k,
            S3GEN_SR,
            &self.device,
            &audio::MelConfig::for_24k(80),
        )?;

        // VoiceEncoder expects Power Mel Spectrogram (amp^2), not dB, not Magnitude
        let mel_40_power = mel_40.sqr()?;
        let mel_40_t = mel_40_power.transpose(1, 2)?; // (B, T, 40)
        let spk_emb_256 = self.voice_encoder.forward(&mel_40_t)?;

        // CAMPPlus expects Mean-Normalized Log-Mel
        let mel_80_log = mel_80_24k.clamp(1e-5, f32::MAX)?.log()?;
        let mean = mel_80_log.mean_keepdim(2)?;
        let mel_80_norm = mel_80_log.broadcast_sub(&mean)?;
        let spk_emb_80 = self
            .s3gen
            .campplus
            .forward(&mel_80_norm)?
            .narrow(1, 0, 80)?;

        // S3Tokenizer expects (B, 128, T) mel - now computed directly
        let prompt_tokens = self.s3tokenizer.encode(&mel_128)?;
        let text_tokens = self.tokenize_text(text)?;

        let speech_tokens = self.t3.generate(
            &text_tokens,
            &spk_emb_256,
            Some(&prompt_tokens),
            None,
            500,
            config.temperature,
            config.top_p,
            config.top_k,
            config.repetition_penalty,
            config.seed,
        )?;
        let audio_tensor = self
            .s3gen
            .forward(&speech_tokens, Some(&spk_emb_80), None)?;

        let mut samples = audio_tensor.flatten_all()?.to_vec1::<f32>()?;
        if config.normalize_loudness {
            audio::normalize_loudness(&mut samples, -27.0);
        }
        Ok((samples, S3GEN_SR))
    }

    fn tokenize_text(&self, text: &str) -> Result<Tensor> {
        let encoding = self
            .tokenizer
            .encode(text, false)
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        let ids: Vec<u32> = encoding.get_ids().iter().map(|&id| id as u32).collect();
        Tensor::from_vec(ids.clone(), (1, ids.len()), &self.device)
    }
}

pub struct ChatterboxTurboTTS {
    t3: T3,
    s3gen: S3Gen,
    s3tokenizer: crate::s3tokenizer::S3TokenizerV2,
    voice_encoder: VoiceEncoder,
    tokenizer: Tokenizer,
    device: Device,
}

impl ChatterboxTurboTTS {
    pub fn from_pretrained(device: Device) -> Result<Self> {
        let api = Api::new().map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        let turbo_repo = api.model("ResembleAI/chatterbox-turbo".to_string());
        let t3_path = turbo_repo
            .get("t3_turbo_v1.safetensors")
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        let s3gen_path = turbo_repo
            .get("s3gen_meanflow.safetensors")
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        let ve_path = turbo_repo
            .get("ve.safetensors")
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        let s3tokenizer_path = api
            .model("ResembleAI/s3tokenizer-v2".to_string())
            .get("model.safetensors")
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        let base_repo = api.model("ResembleAI/chatterbox".to_string());
        let tokenizer_path = base_repo
            .get("tokenizer.json")
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        Self::from_local(
            t3_path,
            s3gen_path,
            ve_path,
            s3tokenizer_path,
            tokenizer_path,
            device,
        )
    }

    pub fn from_local(
        t3_path: PathBuf,
        s3gen_path: PathBuf,
        ve_path: PathBuf,
        s3tokenizer_path: PathBuf,
        tokenizer_path: PathBuf,
        device: Device,
    ) -> Result<Self> {
        let t3_config = T3Config {
            text_tokens_dict_size: 50276,
            speech_tokens_dict_size: 6563,
            hidden_size: 1024,
            num_layers: 24,
            num_heads: 16,
            vocab_size: 50276,
            speaker_embed_size: 256,
            start_speech_token: 6561,
            stop_speech_token: 6562,
            speech_cond_prompt_len: Some(375),
            use_perceiver_resampler: false,
            emotion_adv: false,
            n_positions: 8196,
        };
        let vb_t3 =
            unsafe { VarBuilder::from_mmaped_safetensors(&[t3_path], DType::F32, &device)? };
        let t3 = T3::new(t3_config, vb_t3)?;
        let vb_s3 =
            unsafe { VarBuilder::from_mmaped_safetensors(&[s3gen_path], DType::F32, &device)? };
        let s3gen = S3Gen::new(vb_s3, true)?;
        let vb_ve =
            unsafe { VarBuilder::from_mmaped_safetensors(&[ve_path], DType::F32, &device)? };
        let voice_encoder = VoiceEncoder::new(VoiceEncoderConfig::default(), vb_ve)?;
        let vb_s3tok = unsafe {
            VarBuilder::from_mmaped_safetensors(&[s3tokenizer_path], DType::F32, &device)?
        };
        let s3tokenizer = crate::s3tokenizer::S3TokenizerV2::new(
            &crate::s3tokenizer::ModelConfig::default(),
            vb_s3tok,
        )?;
        let tokenizer = Tokenizer::from_file(&tokenizer_path)
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        Ok(Self {
            t3,
            s3gen,
            s3tokenizer,
            voice_encoder,
            tokenizer,
            device,
        })
    }

    pub fn generate_speech(
        &self,
        text: &str,
        ref_audio_path: &Path,
        config: GenerateConfig,
    ) -> Result<(Vec<f32>, u32)> {
        let text = normalize_text(text);
        eprintln!("[generate_speech] Loading reference audio...");
        let (ref_samples_orig, ref_sr) =
            audio::load_wav(ref_audio_path).map_err(|e| candle_core::Error::Msg(e))?;
        eprintln!(
            "[generate_speech] ref_samples_orig len: {}, sr: {}",
            ref_samples_orig.len(),
            ref_sr
        );

        let ref_samples_24k = if ref_sr != S3GEN_SR {
            audio::resample(&ref_samples_orig, ref_sr, S3GEN_SR)
                .map_err(|e| candle_core::Error::Msg(e))?
        } else {
            ref_samples_orig.clone()
        };

        let ref_samples_16k = if ref_sr != S3_SR {
            audio::resample(&ref_samples_orig, ref_sr, S3_SR)
                .map_err(|e| candle_core::Error::Msg(e))?
        } else {
            ref_samples_orig
        };

        eprintln!("[generate_speech] Computing mel spectrograms...");
        use std::io::Write;
        std::io::stderr().flush().unwrap();

        // VoiceEncoder expects 16kHz mel (40 channels)
        // VoiceEncoder expects 16kHz mel (40 channels)
        // VoiceEncoder expects 16kHz mel (40 channels, n_fft=400)
        let config_ve = audio::MelConfig {
            n_fft: 400,
            hop_length: 160,
            win_length: 400,
            n_mels: 40,
            fmax: 8000.0,
        };
        // S3Tokenizer expects 16kHz mel (128 channels, n_fft=400)
        let config_s3tok = audio::MelConfig {
            n_fft: 400,
            hop_length: 160,
            win_length: 400,
            n_mels: 128,
            fmax: 8000.0,
        };

        let mel_40 =
            audio::compute_mel_spectrogram(&ref_samples_16k, S3_SR, &self.device, &config_ve)?;
        eprintln!("[generate_speech] mel_40 (16k): {:?}", mel_40.dims());

        // Tokenizer expects 16kHz mel (128 channels)
        let mel_128 =
            audio::compute_mel_spectrogram(&ref_samples_16k, S3_SR, &self.device, &config_s3tok)?;
        eprintln!("[generate_speech] mel_128 (16k): {:?}", mel_128.dims());

        // S3Gen expects 24kHz mel (80 channels)
        let mel_80_24k = audio::compute_mel_spectrogram(
            &ref_samples_24k,
            S3GEN_SR,
            &self.device,
            &audio::MelConfig::for_24k(80),
        )?;
        eprintln!("[generate_speech] mel_80 (24k): {:?}", mel_80_24k.dims());

        // VoiceEncoder expects Power Mel Spectrogram (amp^2), not dB, not Magnitude
        let mel_40_power = mel_40.sqr()?;
        let mel_40_t = mel_40_power.transpose(1, 2)?; // (B, T, 40)
        let spk_emb_256 = self.voice_encoder.forward(&mel_40_t)?;

        eprintln!("[generate_speech] spk_emb_256: {:?}", spk_emb_256.dims());
        eprintln!("[generate_speech] Running CAMPPlus...");
        // CAMPPlus expects Mean-Normalized Log-Mel
        let mel_80_log = mel_80_24k.clamp(1e-5, f32::MAX)?.log()?;
        let mean = mel_80_log.mean_keepdim(2)?;
        let mel_80_norm = mel_80_log.broadcast_sub(&mean)?;
        let spk_emb_80 = self
            .s3gen
            .campplus
            .forward(&mel_80_norm)?
            .narrow(1, 0, 80)?;
        eprintln!("[generate_speech] spk_emb_80: {:?}", spk_emb_80.dims());

        // S3Tokenizer expects (B, 128, T) mel - now computed directly
        eprintln!("[generate_speech] Running S3Tokenizer...");
        let prompt_tokens = self.s3tokenizer.encode(&mel_128)?;

        eprintln!(
            "[generate_speech] prompt_tokens: {:?}",
            prompt_tokens.dims()
        );
        let text_tokens = self.tokenize_text(&text)?;
        eprintln!("[generate_speech] text_tokens: {:?}", text_tokens.dims());

        eprintln!("[generate_speech] Running T3.generate...");
        let speech_tokens = self.t3.generate(
            &text_tokens,
            &spk_emb_256,
            Some(&prompt_tokens),
            None,
            500,
            config.temperature,
            config.top_p,
            config.top_k,
            config.repetition_penalty,
            config.seed,
        )?;
        eprintln!(
            "[generate_speech] speech_tokens: {:?}",
            speech_tokens.dims()
        );
        let speech_tokens_filtered = {
            let tokens = speech_tokens.to_vec2::<u32>()?[0].clone();
            let original_len = tokens.len();
            let filtered: Vec<u32> = tokens.into_iter().filter(|&t| t < 6561).collect();
            eprintln!(
                "[generate_speech] filtered tokens: {} (from {})",
                filtered.len(),
                original_len
            );
            Tensor::from_vec(filtered.clone(), (1, filtered.len()), &self.device)?
        };

        eprintln!("[generate_speech] Running S3Gen.forward...");
        let audio_tensor = self
            .s3gen
            .forward(&speech_tokens_filtered, Some(&spk_emb_80), None)?;
        eprintln!("[generate_speech] audio_tensor: {:?}", audio_tensor.dims());
        let mut samples = audio_tensor.flatten_all()?.to_vec1::<f32>()?;
        if config.normalize_loudness {
            audio::normalize_loudness(&mut samples, -27.0);
        }
        Ok((samples, S3GEN_SR))
    }

    fn tokenize_text(&self, text: &str) -> Result<Tensor> {
        let encoding = self
            .tokenizer
            .encode(text, false)
            .map_err(|e| candle_core::Error::Msg(e.to_string()))?;
        let ids: Vec<u32> = encoding.get_ids().iter().map(|&id| id as u32).collect();
        Tensor::from_vec(ids.clone(), (1, ids.len()), &self.device)
    }
}

fn normalize_text(text: &str) -> String {
    let mut text = text.to_string();
    if let Some(first) = text.chars().next() {
        if first.is_lowercase() {
            text = first.to_uppercase().to_string() + &text[first.len_utf8()..];
        }
    }
    text = text.split_whitespace().collect::<Vec<_>>().join(" ");
    let replacements = [
        ("\u{2026}", ", "),
        (":", ","),
        ("\u{2014}", "-"),
        ("\u{2013}", "-"),
        (" ,", ","),
        ("\u{201C}", "\""),
        ("\u{201D}", "\""),
        ("\u{2018}", "'"),
        ("\u{2019}", "'"),
    ];
    for (from, to) in replacements {
        text = text.replace(from, to);
    }
    text = text.trim_end().to_string();
    if !text.ends_with('.')
        && !text.ends_with('!')
        && !text.ends_with('?')
        && !text.ends_with('-')
        && !text.ends_with(',')
    {
        text.push('.');
    }
    text
}
</file>

<file path="candle/src/s3gen.rs">
use candle_core::{DType, Module, Result, Tensor};
use candle_nn::{Activation, Conv1d, Embedding, LayerNorm, Linear, VarBuilder};

// --- Basic Transformer Block ---

struct DecoderAttention {
    to_q: Linear,
    to_k: Linear,
    to_v: Linear,
    to_out: Linear,
    num_heads: usize,
    head_dim: usize,
}

impl DecoderAttention {
    fn new(dim: usize, num_heads: usize, head_dim: usize, vb: VarBuilder) -> Result<Self> {
        let inner_dim = num_heads * head_dim;
        let to_q = candle_nn::linear_no_bias(dim, inner_dim, vb.pp("to_q"))?;
        let to_k = candle_nn::linear_no_bias(dim, inner_dim, vb.pp("to_k"))?;
        let to_v = candle_nn::linear_no_bias(dim, inner_dim, vb.pp("to_v"))?;
        let to_out = candle_nn::linear(inner_dim, dim, vb.pp("to_out.0"))?;
        Ok(Self {
            to_q,
            to_k,
            to_v,
            to_out,
            num_heads,
            head_dim,
        })
    }

    fn forward(&self, x: &Tensor, _mask: Option<&Tensor>) -> Result<Tensor> {
        let (b, t, _c) = x.dims3()?;
        let q = self.to_q.forward(x)?;
        let k = self.to_k.forward(x)?;
        let v = self.to_v.forward(x)?;

        let q = q
            .reshape((b, t, self.num_heads, self.head_dim))?
            .transpose(1, 2)?
            .contiguous()?;
        let k = k
            .reshape((b, t, self.num_heads, self.head_dim))?
            .transpose(1, 2)?
            .contiguous()?;
        let v = v
            .reshape((b, t, self.num_heads, self.head_dim))?
            .transpose(1, 2)?
            .contiguous()?;

        let k_t = k.transpose(2, 3)?.contiguous()?;
        let att = (q.contiguous()?.matmul(&k_t)? / (self.head_dim as f64).sqrt())?;

        let att = candle_nn::ops::softmax(&att, 3)?;
        let out = att.matmul(&v)?;

        let out =
            out.transpose(1, 2)?
                .contiguous()?
                .reshape((b, t, self.num_heads * self.head_dim))?;
        self.to_out.forward(&out)
    }
}

struct BasicTransformerBlock {
    norm1: LayerNorm,
    attn1: DecoderAttention,
    _norm2: Option<LayerNorm>,
    norm3: LayerNorm,
    ff: FeedForward,
}

struct FeedForward {
    net: Vec<Box<dyn Module>>,
}

impl Module for FeedForward {
    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let mut x = x.clone();
        for module in &self.net {
            x = module.forward(&x)?;
        }
        Ok(x)
    }
}

impl BasicTransformerBlock {
    fn new(dim: usize, num_heads: usize, head_dim: usize, vb: VarBuilder) -> Result<Self> {
        let norm1 = candle_nn::layer_norm(dim, 1e-5, vb.pp("norm1"))?;
        let attn1 = DecoderAttention::new(dim, num_heads, head_dim, vb.pp("attn1"))?;
        let norm3 = candle_nn::layer_norm(dim, 1e-5, vb.pp("norm3"))?;
        let inner_dim = dim * 4;
        let ff_proj1 = candle_nn::linear(dim, inner_dim, vb.pp("ff.net.0.proj"))?;
        let ff_proj2 = candle_nn::linear(inner_dim, dim, vb.pp("ff.net.2"))?;
        let gelu_block = GeluBlock { proj: ff_proj1 };
        let net: Vec<Box<dyn Module>> = vec![Box::new(gelu_block), Box::new(ff_proj2)];

        Ok(Self {
            norm1,
            attn1,
            _norm2: None,
            norm3,
            ff: FeedForward { net },
        })
    }

    fn forward(&self, x: &Tensor, _mask: Option<&Tensor>) -> Result<Tensor> {
        let residual = x;
        let x_norm = self.norm1.forward(x)?;
        let x_attn = self.attn1.forward(&x_norm, None)?;
        let x = (x_attn + residual)?;
        let residual = &x;
        let x_norm = self.norm3.forward(&x)?;
        let x_ff = self.ff.forward(&x_norm)?;
        let x = (x_ff + residual)?;
        Ok(x)
    }
}

struct GeluBlock {
    proj: Linear,
}

impl Module for GeluBlock {
    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        self.proj.forward(x)?.gelu()
    }
}

struct Swish;
impl Module for Swish {
    fn forward(&self, xs: &Tensor) -> Result<Tensor> {
        xs.broadcast_mul(&candle_nn::ops::sigmoid(xs)?)
    }
}

struct EspnetRelPositionalEncoding {
    xscale: f64,
}

impl EspnetRelPositionalEncoding {
    fn new(d_model: usize, _max_len: usize) -> Result<Self> {
        Ok(Self {
            xscale: (d_model as f64).sqrt(),
        })
    }

    fn forward(&self, x: &Tensor) -> Result<(Tensor, Tensor)> {
        let (_b, t, c) = x.dims3()?;
        let device = x.device();
        let dtype = x.dtype();

        let position = Tensor::arange(0u32, t as u32, device)?
            .to_dtype(dtype)?
            .unsqueeze(1)?;

        let mut pe = Tensor::zeros((t, c), dtype, device)?;
        let log_10000 = 10000.0f64.ln();

        for i in 0..(c / 2) {
            let div_term = (-((i * 2) as f64 * log_10000 / c as f64)).exp();
            let pos_dt = (&position * div_term)?;
            pe = pe.slice_assign(&[0..t, (i * 2)..(i * 2 + 1)], &pos_dt.sin()?)?;
            pe = pe.slice_assign(&[0..t, (i * 2 + 1)..(i * 2 + 2)], &pos_dt.cos()?)?;
        }

        let x_scaled = (x * self.xscale)?;
        Ok((x_scaled, pe.unsqueeze(0)?))
    }
}

struct SinusoidalPosEmb {
    dim: usize,
}

impl SinusoidalPosEmb {
    fn new(dim: usize) -> Self {
        Self { dim }
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let half_dim = self.dim / 2;
        let emb = (10000f64.ln() / (half_dim as f64 - 1.0)) as f32;
        let freqs = (Tensor::arange(0u32, half_dim as u32, x.device())?.to_dtype(DType::F32)?
            * (-emb as f64))?
            .exp()?;
        let x_scaled = (x * 1000.0)?;
        let emb = x_scaled.unsqueeze(1)?.broadcast_mul(&freqs.unsqueeze(0)?)?;
        let emb = Tensor::cat(&[&emb.sin()?, &emb.cos()?], 1)?;
        Ok(emb)
    }
}

struct TimestepEmbedding {
    linear1: Linear,
    linear2: Linear,
    act: Swish,
}

impl TimestepEmbedding {
    fn new(in_channels: usize, time_embed_dim: usize, vb: VarBuilder) -> Result<Self> {
        let linear1 = candle_nn::linear(in_channels, time_embed_dim, vb.pp("linear_1"))?;
        let linear2 = candle_nn::linear(time_embed_dim, time_embed_dim, vb.pp("linear_2"))?;
        Ok(Self {
            linear1,
            linear2,
            act: Swish,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x = self.linear1.forward(x)?;
        let x = self.act.forward(&x)?;
        self.linear2.forward(&x)
    }
}

struct CausalConv1d {
    conv: Conv1d,
    padding: usize,
}

impl CausalConv1d {
    fn new(
        in_c: usize,
        out_c: usize,
        k: usize,
        dilation: usize,
        stride: usize,
        vb: VarBuilder,
    ) -> Result<Self> {
        let cfg = candle_nn::Conv1dConfig {
            dilation,
            stride,
            ..Default::default()
        };
        let conv = candle_nn::conv1d(in_c, out_c, k, cfg, vb)?;
        Ok(Self {
            conv,
            padding: (k - 1) * dilation,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x = x.pad_with_zeros(2, self.padding, 0)?;
        self.conv.forward(&x)
    }
}

struct CausalBlock1D {
    conv1: CausalConv1d,
    norm: LayerNorm,
    act: Activation,
}

impl CausalBlock1D {
    fn new(dim: usize, dim_out: usize, vb: VarBuilder) -> Result<Self> {
        let conv1 = CausalConv1d::new(dim, dim_out, 3, 1, 1, vb.pp("block.0"))?;
        let norm = candle_nn::layer_norm(dim_out, 1e-5, vb.pp("block.2"))?;
        Ok(Self {
            conv1,
            norm,
            act: Activation::Gelu,
        })
    }

    fn forward(&self, x: &Tensor, mask: &Tensor) -> Result<Tensor> {
        let x = x.broadcast_mul(mask)?;
        let x = self.conv1.forward(&x)?;
        let x = x.transpose(1, 2)?;
        let x = self.norm.forward(&x)?;
        let x = x.transpose(1, 2)?;
        let x = self.act.forward(&x)?;
        x.broadcast_mul(mask)
    }
}

struct CausalResnetBlock1D {
    block1: CausalBlock1D,
    block2: CausalBlock1D,
    time_proj: Linear,
    residual_conv: Option<Conv1d>,
}

impl CausalResnetBlock1D {
    fn new(dim: usize, dim_out: usize, time_emb_dim: usize, vb: VarBuilder) -> Result<Self> {
        let time_proj = candle_nn::linear(time_emb_dim, dim_out, vb.pp("mlp.1"))?;
        let block1 = CausalBlock1D::new(dim, dim_out, vb.pp("block1"))?;
        let block2 = CausalBlock1D::new(dim_out, dim_out, vb.pp("block2"))?;
        let residual_conv = if dim != dim_out {
            Some(candle_nn::conv1d(
                dim,
                dim_out,
                1,
                Default::default(),
                vb.pp("res_conv"),
            )?)
        } else {
            None
        };
        Ok(Self {
            block1,
            block2,
            time_proj,
            residual_conv,
        })
    }

    fn forward(&self, x: &Tensor, mask: &Tensor, t_emb: &Tensor) -> Result<Tensor> {
        let t_emb = t_emb.gelu()?;
        let t = self.time_proj.forward(&t_emb)?.unsqueeze(2)?;
        let h = self.block1.forward(x, mask)?;
        let h = h.broadcast_add(&t)?;
        let h = self.block2.forward(&h, mask)?;
        let res = if let Some(conv) = &self.residual_conv {
            conv.forward(x)?
        } else {
            x.clone()
        };
        h + res
    }
}

struct PositionwiseFeedForward {
    w_1: Linear,
    w_2: Linear,
    swish: Swish,
}

impl PositionwiseFeedForward {
    fn new(dim: usize, hidden_dim: usize, vb: VarBuilder) -> Result<Self> {
        let w_1 = candle_nn::linear(dim, hidden_dim, vb.pp("w_1"))?;
        let w_2 = candle_nn::linear(hidden_dim, dim, vb.pp("w_2"))?;
        Ok(Self {
            w_1,
            w_2,
            swish: Swish,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x = self.w_1.forward(x)?;
        let x = self.swish.forward(&x)?;
        self.w_2.forward(&x)
    }
}

struct RelPositionMultiHeadedAttention {
    linear_q: Linear,
    linear_k: Linear,
    linear_v: Linear,
    linear_out: Linear,
    linear_pos: Linear,
    pos_bias_u: Tensor,
    pos_bias_v: Tensor,
    num_heads: usize,
    head_dim: usize,
}

impl RelPositionMultiHeadedAttention {
    fn new(dim: usize, num_heads: usize, vb: VarBuilder) -> Result<Self> {
        let head_dim = dim / num_heads;
        let linear_q = candle_nn::linear(dim, dim, vb.pp("linear_q"))?;
        let linear_k = candle_nn::linear(dim, dim, vb.pp("linear_k"))?;
        let linear_v = candle_nn::linear(dim, dim, vb.pp("linear_v"))?;
        let linear_out = candle_nn::linear(dim, dim, vb.pp("linear_out"))?;
        let linear_pos = candle_nn::linear_no_bias(dim, dim, vb.pp("linear_pos"))?;
        let pos_bias_u = vb.get((num_heads, head_dim), "pos_bias_u")?;
        let pos_bias_v = vb.get((num_heads, head_dim), "pos_bias_v")?;
        Ok(Self {
            linear_q,
            linear_k,
            linear_v,
            linear_out,
            linear_pos,
            pos_bias_u,
            pos_bias_v,
            num_heads,
            head_dim,
        })
    }

    fn forward(&self, x: &Tensor, pos_emb: &Tensor, _mask: Option<&Tensor>) -> Result<Tensor> {
        let (b, t, c) = x.dims3()?;
        let q = self.linear_q.forward(x)?;
        let k = self.linear_k.forward(x)?;
        let v = self.linear_v.forward(x)?;
        let q = q
            .reshape((b, t, self.num_heads, self.head_dim))?
            .transpose(1, 2)?
            .contiguous()?;
        let k = k
            .reshape((b, t, self.num_heads, self.head_dim))?
            .transpose(1, 2)?
            .contiguous()?;
        let v = v
            .reshape((b, t, self.num_heads, self.head_dim))?
            .transpose(1, 2)?
            .contiguous()?;
        let p = self.linear_pos.forward(pos_emb)?;
        let p = p
            .reshape((b, t, self.num_heads, self.head_dim))?
            .transpose(1, 2)?
            .contiguous()?;
        let q_u = q
            .broadcast_add(&self.pos_bias_u.unsqueeze(0)?.unsqueeze(2)?)?
            .contiguous()?;
        let q_v = q
            .broadcast_add(&self.pos_bias_v.unsqueeze(0)?.unsqueeze(2)?)?
            .contiguous()?;
        let matrix_ac = q_u.matmul(&k.transpose(2, 3)?.contiguous()?)?;
        let matrix_bd = q_v.matmul(&p.transpose(2, 3)?.contiguous()?)?;
        let scores = (matrix_ac.broadcast_add(&matrix_bd)? / (self.head_dim as f64).sqrt())?;
        let attn = candle_nn::ops::softmax(&scores, 3)?;
        let x = attn.matmul(&v)?;
        let x = x.transpose(1, 2)?.contiguous()?.reshape((b, t, c))?;
        self.linear_out.forward(&x)
    }
}

struct ConformerLayer {
    self_attn: RelPositionMultiHeadedAttention,
    feed_forward: PositionwiseFeedForward,
    norm_mha: LayerNorm,
    norm_ff: LayerNorm,
}

impl ConformerLayer {
    fn new(dim: usize, hidden_dim: usize, num_heads: usize, vb: VarBuilder) -> Result<Self> {
        let self_attn = RelPositionMultiHeadedAttention::new(dim, num_heads, vb.pp("self_attn"))?;
        let feed_forward = PositionwiseFeedForward::new(dim, hidden_dim, vb.pp("feed_forward"))?;
        let norm_mha = candle_nn::layer_norm(dim, 1e-12, vb.pp("norm_mha"))?;
        let norm_ff = candle_nn::layer_norm(dim, 1e-12, vb.pp("norm_ff"))?;
        Ok(Self {
            self_attn,
            feed_forward,
            norm_mha,
            norm_ff,
        })
    }

    fn forward(&self, x: &Tensor, pos_emb: &Tensor) -> Result<Tensor> {
        let residual = x;
        let x_norm = self.norm_mha.forward(x)?;
        let x_attn = self.self_attn.forward(&x_norm, pos_emb, None)?;
        let x = (x_attn + residual)?;
        let residual = &x;
        let x_norm = self.norm_ff.forward(&x)?;
        let x_ff = self.feed_forward.forward(&x_norm)?;
        x_ff + residual
    }
}

struct LinearNoSubsampling {
    out: Vec<Box<dyn Module>>,
}

impl LinearNoSubsampling {
    fn new(input_dim: usize, output_dim: usize, vb: VarBuilder) -> Result<Self> {
        let linear = candle_nn::linear(input_dim, output_dim, vb.pp("0"))?;
        let ln = candle_nn::layer_norm(output_dim, 1e-5, vb.pp("1"))?;
        Ok(Self {
            out: vec![Box::new(linear), Box::new(ln)],
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let mut x = x.clone();
        for module in &self.out {
            x = module.forward(&x)?;
        }
        Ok(x)
    }
}

struct PreLookaheadLayer {
    conv1: Conv1d,
    conv2: Conv1d,
    pre_lookahead_len: usize,
}

impl PreLookaheadLayer {
    fn new(channels: usize, pre_lookahead_len: usize, vb: VarBuilder) -> Result<Self> {
        let conv1 = candle_nn::conv1d(
            channels,
            channels,
            pre_lookahead_len + 1,
            Default::default(),
            vb.pp("conv1"),
        )?;
        let conv2 = candle_nn::conv1d(channels, channels, 3, Default::default(), vb.pp("conv2"))?;
        Ok(Self {
            conv1,
            conv2,
            pre_lookahead_len,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x_in = x.clone();
        let mut h = x.transpose(1, 2)?;
        h = h.pad_with_zeros(2, 0, self.pre_lookahead_len)?;
        h = self.conv1.forward(&h)?.gelu()?;
        h = h.pad_with_zeros(2, 2, 0)?;
        h = self.conv2.forward(&h)?;
        let h = h.transpose(1, 2)?;
        h + x_in
    }
}

struct Upsample1D {
    conv: Option<Conv1d>,
    stride: usize,
}

impl Upsample1D {
    fn new(channels: usize, out_channels: usize, stride: usize, vb: VarBuilder) -> Result<Self> {
        let conv = candle_nn::conv1d(
            channels,
            out_channels,
            stride * 2 + 1,
            Default::default(),
            vb.pp("conv"),
        )?;
        Ok(Self {
            conv: Some(conv),
            stride,
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let (b, c, t) = x.dims3()?;
        let x = x
            .unsqueeze(3)?
            .repeat((1, 1, 1, self.stride))?
            .reshape((b, c, t * self.stride))?;
        if let Some(conv) = &self.conv {
            let x = x.pad_with_zeros(2, self.stride * 2, 0)?;
            conv.forward(&x)
        } else {
            Ok(x)
        }
    }
}

pub struct UpsampleConformerEncoder {
    embed: LinearNoSubsampling,
    pre_lookahead: PreLookaheadLayer,
    encoders: Vec<ConformerLayer>,
    up_layer: Upsample1D,
    up_embed: LinearNoSubsampling,
    up_encoders: Vec<ConformerLayer>,
    pe: EspnetRelPositionalEncoding,
    after_norm: LayerNorm,
}

impl UpsampleConformerEncoder {
    pub fn new(
        input_dim: usize,
        output_dim: usize,
        hidden_dim: usize,
        num_layers: usize,
        num_heads: usize,
        vb: VarBuilder,
    ) -> Result<Self> {
        let embed = LinearNoSubsampling::new(input_dim, output_dim, vb.pp("embed").pp("out"))?;
        let pre_lookahead = PreLookaheadLayer::new(output_dim, 3, vb.pp("pre_lookahead_layer"))?;
        let mut encoders = Vec::new();
        for i in 0..num_layers {
            encoders.push(ConformerLayer::new(
                output_dim,
                hidden_dim,
                num_heads,
                vb.pp("encoders").pp(i),
            )?);
        }
        let up_layer = Upsample1D::new(output_dim, output_dim, 2, vb.pp("up_layer"))?;
        let up_embed =
            LinearNoSubsampling::new(output_dim, output_dim, vb.pp("up_embed").pp("out"))?;
        let mut up_encoders = Vec::new();
        for i in 0..4 {
            up_encoders.push(ConformerLayer::new(
                output_dim,
                hidden_dim,
                num_heads,
                vb.pp("up_encoders").pp(i),
            )?);
        }
        let pe = EspnetRelPositionalEncoding::new(output_dim, 5000)?;
        let after_norm = candle_nn::layer_norm(output_dim, 1e-5, vb.pp("after_norm"))?;
        Ok(Self {
            embed,
            pre_lookahead,
            encoders,
            up_layer,
            up_embed,
            up_encoders,
            after_norm,
            pe,
        })
    }

    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let x = self.embed.forward(x)?;
        let x = self.pre_lookahead.forward(&x)?;
        let (mut x, pos_emb) = self.pe.forward(&x)?;
        for layer in &self.encoders {
            x = layer.forward(&x, &pos_emb)?;
        }

        let x = self
            .up_layer
            .forward(&x.transpose(1, 2)?)?
            .transpose(1, 2)?;

        let x = self.up_embed.forward(&x)?;
        let (mut x, pos_emb_up) = self.pe.forward(&x)?;
        for layer in &self.up_encoders {
            x = layer.forward(&x, &pos_emb_up)?;
        }

        self.after_norm.forward(&x)
    }
}

pub struct ConditionalDecoder {
    sinusoidal_pos_emb: SinusoidalPosEmb,
    time_embedding: TimestepEmbedding,
    down_blocks: Vec<(
        CausalResnetBlock1D,
        Vec<BasicTransformerBlock>,
        Option<CausalConv1d>,
    )>,
    mid_blocks: Vec<(CausalResnetBlock1D, Vec<BasicTransformerBlock>)>,
    up_blocks: Vec<(
        CausalResnetBlock1D,
        Vec<BasicTransformerBlock>,
        Option<Upsample1D>,
    )>,
    final_block: CausalBlock1D,
    final_proj: Conv1d,
    pub meanflow: bool,
    _time_mixer: Option<Linear>,
    spk_emb_dim: usize,
}

impl ConditionalDecoder {
    pub fn new(
        in_channels: usize,
        out_channels: usize,
        spk_emb_dim: usize,
        vb: VarBuilder,
        meanflow: bool,
    ) -> Result<Self> {
        let channels = vec![256];
        let time_dim = channels[0] * 4;
        let attention_head_dim = 64;
        let num_heads = 8;
        let n_blocks = 4;

        let sinusoidal_pos_emb = SinusoidalPosEmb::new(in_channels);
        let time_embedding = TimestepEmbedding::new(in_channels, time_dim, vb.pp("time_mlp"))?;

        let mut down_blocks = Vec::new();
        let resnet =
            CausalResnetBlock1D::new(in_channels, channels[0], time_dim, vb.pp("down_blocks.0.0"))?;
        let mut transformers = Vec::new();
        for i in 0..n_blocks {
            transformers.push(BasicTransformerBlock::new(
                channels[0],
                num_heads,
                attention_head_dim,
                vb.pp("down_blocks.0.1").pp(i),
            )?);
        }
        let downsample =
            CausalConv1d::new(channels[0], channels[0], 3, 1, 1, vb.pp("down_blocks.0.2"))?;
        down_blocks.push((resnet, transformers, Some(downsample)));

        let mut mid_blocks = Vec::new();
        for i in 0..12 {
            let resnet = CausalResnetBlock1D::new(
                channels[0],
                channels[0],
                time_dim,
                vb.pp("mid_blocks").pp(i).pp("0"),
            )?;
            let mut transformers = Vec::new();
            for j in 0..n_blocks {
                transformers.push(BasicTransformerBlock::new(
                    channels[0],
                    num_heads,
                    attention_head_dim,
                    vb.pp("mid_blocks").pp(i).pp("1").pp(j),
                )?);
            }
            mid_blocks.push((resnet, transformers));
        }

        let mut up_blocks = Vec::new();
        let resnet = CausalResnetBlock1D::new(
            channels[0] * 2,
            channels[0],
            time_dim,
            vb.pp("up_blocks.0").pp("0"),
        )?;
        let mut transformers = Vec::new();
        for j in 0..n_blocks {
            transformers.push(BasicTransformerBlock::new(
                channels[0],
                num_heads,
                attention_head_dim,
                vb.pp("up_blocks.0").pp("1").pp(j),
            )?);
        }
        let upsample_conv = CausalConv1d::new(
            channels[0],
            channels[0],
            3,
            1,
            1,
            vb.pp("up_blocks.0").pp("2"),
        )?;
        let upsample = Upsample1D {
            conv: Some(upsample_conv.conv),
            stride: 1,
        };
        up_blocks.push((resnet, transformers, Some(upsample)));

        let final_block = CausalBlock1D::new(channels[0], channels[0], vb.pp("final_block"))?;
        let final_proj = candle_nn::conv1d(
            channels[0],
            out_channels,
            1,
            Default::default(),
            vb.pp("final_proj"),
        )?;
        let time_mixer = if meanflow {
            Some(candle_nn::linear_no_bias(
                time_dim * 2,
                time_dim,
                vb.pp("time_embed_mixer"),
            )?)
        } else {
            None
        };

        Ok(Self {
            sinusoidal_pos_emb,
            time_embedding,
            down_blocks,
            mid_blocks,
            up_blocks,
            final_block,
            final_proj,
            meanflow,
            _time_mixer: time_mixer,
            spk_emb_dim,
        })
    }

    pub fn forward(
        &self,
        x: &Tensor,
        mask: &Tensor,
        mu: &Tensor,
        t: &Tensor,
        spks: Option<&Tensor>,
        cond: Option<&Tensor>,
        r: Option<&Tensor>,
    ) -> Result<Tensor> {
        let mut t_emb = self
            .time_embedding
            .forward(&self.sinusoidal_pos_emb.forward(t)?)?;

        if self.meanflow {
            if let Some(r) = r {
                let r_emb = self
                    .time_embedding
                    .forward(&self.sinusoidal_pos_emb.forward(r)?)?;
                let concat = Tensor::cat(&[&t_emb, &r_emb], 1)?;
                if let Some(mixer) = &self._time_mixer {
                    t_emb = mixer.forward(&concat)?;
                }
            }
        }
        let (b, _, t_len) = x.dims3()?;

        let spk = match spks {
            Some(s) => s.unsqueeze(2)?.repeat((1, 1, t_len))?,
            None => Tensor::zeros((b, self.spk_emb_dim, t_len), x.dtype(), x.device())?,
        };
        let c_tensor = match cond {
            Some(c) => {
                if c.dim(2)? == 1 {
                    c.repeat((1, 1, t_len))?
                } else {
                    c.clone()
                }
            }
            None => Tensor::zeros((b, 80, t_len), x.dtype(), x.device())?,
        };
        let mut current_x = Tensor::cat(&[x, mu, &spk, &c_tensor], 1)?;

        let mut hiddens = Vec::new();
        let mut masks = vec![mask.clone()];

        for (resnet, transformers, downsample) in &self.down_blocks {
            let m = masks.last().unwrap();
            current_x = resnet.forward(&current_x, m, &t_emb)?;
            let mut xt = current_x.transpose(1, 2)?;
            for tf in transformers {
                xt = tf.forward(&xt, None)?;
            }
            current_x = xt.transpose(1, 2)?;
            hiddens.push(current_x.clone());
            if let Some(ds) = downsample {
                current_x = ds.forward(&current_x)?;
                let t_len = current_x.dim(2)?;
                masks.push(m.narrow(2, 0, t_len)?);
            }
        }

        let m_mid = masks.last().unwrap();
        for (resnet, transformers) in &self.mid_blocks {
            current_x = resnet.forward(&current_x, m_mid, &t_emb)?;
            let mut xt = current_x.transpose(1, 2)?;
            for tf in transformers {
                xt = tf.forward(&xt, None)?;
            }
            current_x = xt.transpose(1, 2)?;
        }

        for (resnet, transformers, upsample) in &self.up_blocks {
            masks.pop();
            let m_up = masks.last().unwrap();
            let skip = hiddens.pop().unwrap();

            // Match python slicing: x[:, :, :skip.shape[-1]]
            let skip_len = skip.dim(2)?;
            if current_x.dim(2)? > skip_len {
                current_x = current_x.narrow(2, 0, skip_len)?;
            }
            let m_up = m_up.narrow(2, 0, skip_len)?; // Also narrow the mask

            current_x = Tensor::cat(&[&current_x, &skip], 1)?;
            current_x = resnet.forward(&current_x, &m_up, &t_emb)?;
            let mut xt = current_x.transpose(1, 2)?;
            for tf in transformers {
                xt = tf.forward(&xt, None)?;
            }
            current_x = xt.transpose(1, 2)?;

            if let Some(us) = upsample {
                current_x = us.forward(&current_x)?;
            }
        }

        let m_final = masks.pop().unwrap();
        current_x = self.final_block.forward(&current_x, &m_final)?;
        self.final_proj.forward(&current_x)
    }
}

pub struct CausalConditionalCFM {
    estimator: ConditionalDecoder,
    mel_dim: usize,
}

impl CausalConditionalCFM {
    pub fn new(
        mel_dim: usize,
        _cond_dim: usize,
        spk_emb_dim: usize,
        vb: VarBuilder,
        meanflow: bool,
    ) -> Result<Self> {
        let estimator =
            ConditionalDecoder::new(320, mel_dim, spk_emb_dim, vb.pp("estimator"), meanflow)?;
        Ok(Self { estimator, mel_dim })
    }

    pub fn forward(
        &self,
        mu: &Tensor,
        mask: &Tensor,
        spks: Option<&Tensor>,
        cond: Option<&Tensor>,
        n_timesteps: usize,
    ) -> Result<Tensor> {
        let (b, _c, t) = mu.dims3()?;
        let mut x = Tensor::randn(0f32, 1f32, (b, self.mel_dim, t), mu.device())?;
        let dt = 1.0 / n_timesteps as f64;
        for i in 0..n_timesteps {
            let t_val = i as f64 * dt;
            let r_val = t_val + dt;
            let t_tensor = Tensor::new(&[t_val as f32], mu.device())?.repeat(b)?;
            let r_tensor = Tensor::new(&[r_val as f32], mu.device())?.repeat(b)?;
            let dxdt =
                self.estimator
                    .forward(&x, mask, mu, &t_tensor, spks, cond, Some(&r_tensor))?;
            x = (x + dxdt * dt)?;
        }
        Ok(x)
    }
}

pub struct S3Gen {
    embedding: Embedding,
    encoder: UpsampleConformerEncoder,
    mu_proj: Linear,
    decoder: CausalConditionalCFM,
    pub campplus: crate::campplus::CAMPPlus,
    hifigan: Option<crate::hifigan::HiFTGenerator>,
}

impl S3Gen {
    pub fn new(vb: VarBuilder, meanflow: bool) -> Result<Self> {
        let vocab_size = 6561;
        let hidden_dim = 512;
        let vb_flow = vb.pp("flow");
        let embedding =
            candle_nn::embedding(vocab_size, hidden_dim, vb_flow.pp("input_embedding"))?;
        let encoder = UpsampleConformerEncoder::new(
            hidden_dim,
            hidden_dim,
            2048,
            6,
            8,
            vb_flow.pp("encoder"),
        )?;
        let mu_proj = candle_nn::linear(hidden_dim, 80, vb_flow.pp("encoder_proj"))?;
        let decoder = CausalConditionalCFM::new(80, 80, 80, vb_flow.pp("decoder"), meanflow)?;
        let campplus = crate::campplus::CAMPPlus::new(80, 192, vb.pp("speaker_encoder"))?;

        // Try to load HiFTGenerator vocoder (mel2wav)
        // Config matches Python: sampling_rate=24000, upsample_rates=[8,5,3]
        let hifigan = {
            let config = crate::hifigan::HiFTConfig {
                in_channels: 80,
                base_channels: 512,
                nb_harmonics: 8,
                sampling_rate: 24000,
                upsample_rates: vec![8, 5, 3],
                upsample_kernel_sizes: vec![16, 11, 7],
                resblock_kernel_sizes: vec![3, 7, 11],
                resblock_dilation_sizes: vec![vec![1, 3, 5], vec![1, 3, 5], vec![1, 3, 5]],
                n_fft: 16,
                hop_len: 4,
            };
            match crate::hifigan::HiFTGenerator::new(config, vb.pp("mel2wav")) {
                Ok(h) => {
                    eprintln!("[S3Gen] HiFTGenerator (mel2wav) loaded successfully");
                    Some(h)
                }
                Err(e) => {
                    // HiFiGAN weights might not be available - output raw mel
                    eprintln!("[S3Gen] HiFTGenerator failed to load: {:?}", e);
                    eprintln!("[S3Gen] WARNING: Will output raw mel spectrograms (static noise)");
                    None
                }
            }
        };

        Ok(Self {
            embedding,
            encoder,
            mu_proj,
            decoder,
            campplus,
            hifigan,
        })
    }

    /// Forward pass: converts speech tokens to audio waveform
    /// Returns mel spectrogram if HiFiGAN not available, otherwise returns audio
    pub fn forward(
        &self,
        speech_tokens: &Tensor,
        spks: Option<&Tensor>,
        cond: Option<&Tensor>,
    ) -> Result<Tensor> {
        eprintln!("[S3Gen::forward] speech_tokens: {:?}", speech_tokens.dims());
        let embeds = self.embedding.forward(speech_tokens)?;
        eprintln!("[S3Gen::forward] embeds: {:?}", embeds.dims());
        let encoder_out = self.encoder.forward(&embeds)?;
        eprintln!("[S3Gen::forward] encoder_out: {:?}", encoder_out.dims());
        let mu = self.mu_proj.forward(&encoder_out)?.transpose(1, 2)?;
        eprintln!("[S3Gen::forward] mu (mel): {:?}", mu.dims());

        let n_steps = if self.decoder.estimator.meanflow {
            2
        } else {
            32
        };

        let (b, _, t) = mu.dims3()?;
        let mask = Tensor::ones((b, 1, t), mu.dtype(), mu.device())?;

        // Use provided cond or self mu
        let cond_tensor = cond.unwrap_or(&mu);
        eprintln!("[S3Gen::forward] cond tensor: {:?}", cond_tensor.dims());

        let mel = self
            .decoder
            .forward(&mu, &mask, spks, Some(cond_tensor), n_steps)?;
        eprintln!("[S3Gen::forward] mel after decoder: {:?}", mel.dims());
        let mel_v = mel.to_vec3::<f32>()?;
        let mut sum = 0.0;
        let mut count = 0;
        for b_idx in 0..mel_v.len() {
            for c_idx in 0..mel_v[b_idx].len() {
                for t_idx in 0..mel_v[b_idx][c_idx].len() {
                    sum += mel_v[b_idx][c_idx][t_idx].abs();
                    count += 1;
                }
            }
        }
        eprintln!("[S3Gen::forward] mel mean abs: {}", sum / count as f32);

        // Convert mel to audio using HiFiGAN vocoder if available
        if let Some(ref hifigan) = self.hifigan {
            let audio = hifigan.inference(&mel)?;
            Ok(audio)
        } else {
            Ok(mel)
        }
    }

    /// Forward pass that explicitly returns mel spectrogram without vocoder
    pub fn forward_mel(&self, speech_tokens: &Tensor, spks: Option<&Tensor>) -> Result<Tensor> {
        let embeds = self.embedding.forward(speech_tokens)?;
        let encoder_out = self.encoder.forward(&embeds)?;
        let mu = self.mu_proj.forward(&encoder_out)?.transpose(1, 2)?;
        let n_steps = if self.decoder.estimator.meanflow {
            2
        } else {
            32
        };
        let (b, _, t) = mu.dims3()?;
        let mask = Tensor::ones((b, 1, t), mu.dtype(), mu.device())?;
        self.decoder.forward(&mu, &mask, spks, Some(&mu), n_steps)
    }
}
</file>

<file path="README.md">
![Chatterbox Turbo Image](./Chatterbox-Turbo.jpg)


# Chatterbox TTS

[![Alt Text](https://img.shields.io/badge/listen-demo_samples-blue)](https://resemble-ai.github.io/chatterbox_turbo_demopage/)
[![Alt Text](https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg)](https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo)
[![Alt Text](https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg)](https://podonos.com/resembleai/chatterbox)
[![Discord](https://img.shields.io/discord/1377773249798344776?label=join%20discord&logo=discord&style=flat)](https://discord.gg/rJq9cRJBJ6)

_Made with ♥️ by <a href="https://resemble.ai" target="_blank"><img width="100" alt="resemble-logo-horizontal" src="https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525" /></a>

**Chatterbox** is a family of three state-of-the-art, open-source text-to-speech models by Resemble AI.

We are excited to introduce **Chatterbox-Turbo**, our most efficient model yet. Built on a streamlined 350M parameter architecture, **Turbo** delivers high-quality speech with less compute and VRAM than our previous models. We have also distilled the speech-token-to-mel decoder, previously a bottleneck, reducing generation from 10 steps to just **one**, while retaining high-fidelity audio output.

**Paralinguistic tags** are now native to the Turbo model, allowing you to use `[cough]`, `[laugh]`, `[chuckle]`, and more to add distinct realism. While Turbo was built primarily for low-latency voice agents, it excels at narration and creative workflows.

If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (<a href="https://resemble.ai">link</a>). It delivers reliable performance with ultra-low latency of sub 200ms—ideal for production use in agents, applications, or interactive media.

<img width="1200" height="600" alt="Podonos Turbo Eval" src="https://storage.googleapis.com/chatterbox-demo-samples/turbo/podonos_turbo.png" />

### ⚡ Model Zoo

Choose the right model for your application.

| Model                                                                                                           | Size | Languages | Key Features                                            | Best For                                     | 🤗                                                                  | Examples |
|:----------------------------------------------------------------------------------------------------------------| :--- | :--- |:--------------------------------------------------------|:---------------------------------------------|:--------------------------------------------------------------------------| :--- |
| **Chatterbox-Turbo**                                                                                            | **350M** | **English** | Paralinguistic Tags (`[laugh]`), Lower Compute and VRAM | Zero-shot voice agents,  Production          | [Demo](https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo)        | [Listen](https://resemble-ai.github.io/chatterbox_turbo_demopage/) |
| Chatterbox-Multilingual [(Language list)](#supported-languages)                                                 | 500M | 23+ | Zero-shot cloning, Multiple Languages                   | Global applications, Localization            | [Demo](https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS) | [Listen](https://resemble-ai.github.io/chatterbox_demopage/) |
| Chatterbox [(Tips and Tricks)](#original-chatterbox-tips)                                                       | 500M | English | CFG & Exaggeration tuning                               | General zero-shot TTS with creative controls | [Demo](https://huggingface.co/spaces/ResembleAI/Chatterbox)              | [Listen](https://resemble-ai.github.io/chatterbox_demopage/) |

## Installation
```shell
pip install chatterbox-tts
```

Alternatively, you can install from source:
```shell
# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
```
We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in `pyproject.toml` to ensure consistency. You can modify the code or dependencies in this installation mode.

## Usage

##### Chatterbox-Turbo

```python
import torchaudio as ta
import torch
from chatterbox.tts_turbo import ChatterboxTurboTTS

# Load the Turbo model
model = ChatterboxTurboTTS.from_pretrained(device="cuda")

# Generate with Paralinguistic Tags
text = "Hi there, Sarah here from MochaFone calling you back [chuckle], have you got one minute to chat about the billing issue?"

# Generate audio (requires a reference clip for voice cloning)
wav = model.generate(text, audio_prompt_path="your_10s_ref_clip.wav")

ta.save("test-turbo.wav", wav, model.sr)
```

##### Chatterbox and Chatterbox-Multilingual

```python

import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device="cuda")

text = "Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill."
wav = model.generate(text)
ta.save("test-english.wav", wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = "Bonjour, comment ça va? Ceci est le modèle de synthèse vocale multilingue Chatterbox, il prend en charge 23 langues."
wav_french = multilingual_model.generate(spanish_text, language_id="fr")
ta.save("test-french.wav", wav_french, model.sr)

chinese_text = "你好，今天天气真不错，希望你有一个愉快的周末。"
wav_chinese = multilingual_model.generate(chinese_text, language_id="zh")
ta.save("test-chinese.wav", wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = "YOUR_FILE.wav"
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save("test-2.wav", wav, model.sr)
```
See `example_tts.py` and `example_vc.py` for more examples.

## Supported Languages 
Arabic (ar) • Danish (da) • German (de) • Greek (el) • English (en) • Spanish (es) • Finnish (fi) • French (fr) • Hebrew (he) • Hindi (hi) • Italian (it) • Japanese (ja) • Korean (ko) • Malay (ms) • Dutch (nl) • Norwegian (no) • Polish (pl) • Portuguese (pt) • Russian (ru) • Swedish (sv) • Swahili (sw) • Turkish (tr) • Chinese (zh)

## Original Chatterbox Tips
- **General Use (TTS and Voice Agents):**
  - Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clip’s language. To mitigate this, set `cfg_weight` to `0`.
  - The default settings (`exaggeration=0.5`, `cfg_weight=0.5`) work well for most prompts across all languages.
  - If the reference speaker has a fast speaking style, lowering `cfg_weight` to around `0.3` can improve pacing.

- **Expressive or Dramatic Speech:**
  - Try lower `cfg_weight` values (e.g. `~0.3`) and increase `exaggeration` to around `0.7` or higher.
  - Higher `exaggeration` tends to speed up speech; reducing `cfg_weight` helps compensate with slower, more deliberate pacing.


## Built-in PerTh Watermarking for Responsible AI

Every audio file generated by Chatterbox includes [Resemble AI's Perth (Perceptual Threshold) Watermarker](https://github.com/resemble-ai/perth) - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.


## Watermark extraction

You can look for the watermark using the following script.

```python
import perth
import librosa

AUDIO_PATH = "YOUR_FILE.wav"

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f"Extracted watermark: {watermark}")
# Output: 0.0 (no watermark) or 1.0 (watermarked)
```


## Official Discord

👋 Join us on [Discord](https://discord.gg/rJq9cRJBJ6) and let's build something awesome together!

## Acknowledgements
- [Cosyvoice](https://github.com/FunAudioLLM/CosyVoice)
- [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning)
- [HiFT-GAN](https://github.com/yl4579/HiFTNet)
- [Llama 3](https://github.com/meta-llama/llama3)
- [S3Tokenizer](https://github.com/xingchensong/S3Tokenizer)

## Citation
If you find this model useful, please consider citing.
```
@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
```
## Disclaimer
Don't use this model to do bad things. Prompts are sourced from freely available data on the internet.
</file>

</files>
